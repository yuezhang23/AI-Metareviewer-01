id;text;label
zzOOqD6R1b;"REVIEW 
Summary:
The paper studies whether fine-tuning can elicit the hidden capabilities of LLMs, especially motivated by the setting of dangerous capabilities evaluations. 

To provide a specific experimental setup, the paper considers password-locked models, which responds with weak answers unless provided with a fixed password in the prompts. By giving strong demonstrations generated from a strong base model to the locked model, they show that SFT typically can recover most of the capabilities hidden in the locked model, achieving unlocking. 

The paper continues to study RL (with reward but not explicit demonstrations) and a toy setting where the password locking is achieved from scratch.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
In my view, the paper studies a well motivated concern in current safety research agenda on dangerous capability eval. The password-locking scenario is a novel instantiation that we can concretely study capability elicitation. I find that simply making this formal and executable is already a meaningful contribution.

The paper is nicely written and well structured. 

The two main lines of the experiments on SFT and RL are solid, and the results are novel.

Weaknesses:
For section 5, a minor concern I have is that the password-locking is achieved by SFT as well. This introduces this  weird ""causal structure"" in the experimental setup: specifically, the capability elicitation (in section 5) and the password-locking are both via SFT. I can imagine that we get the results in section 5, not (only) because SFT is good in capability elicitation, but because it's relatively weak in robustly password-locking the model. This might happen, for example, if password-locking a kind of an unnatural task, and a base LM trained on natural language corpus won't robustly lock itself via SFT. To be fair, I think this is not the most likely explanation of the results in section 5. But I am wondering if the author(s) has any take on the relative power of SFT. (Also, I think section 7 partially addresses this concern anyway, though I look forward to seeing more evidence against this weird hypothesis.)

The paper would generally be stronger if there is a straighforward baseline on capability elicitation via few-shot/many-shot prompting only (no messing with the model weights). Similarly, I'd love to see how RepE / steering vector perform on password-locked models; i.e., can you find a direction in the activation space to unlock the model? (I would guess you can.)

Limitations:
The paper addresses its limitations in section 8.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To determine whether models are safe to deploy, it's important for labs and regulators to be able to ""elicit"" model capabilities, including ones that are somehow ""hidden"" (whether by a malicious deployer, by some unforeseen property of the training data, or by an unaligned model that is able to plan). One way to evaluate for the presence of some capability is to fine-tune the model to perform the capability and check whether the model is able to do it. Does this work? The authors study this question by introducing and training ""password-locked"" models that are trained to only perform particular tasks well if a password is present in the prompt. The authors find that given sufficiently good demonstrations, fine-tuning and RL succeed at eliciting capabilities, but very poor demonstrations are insufficient.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The strongest contribution of this paper is the framing of the problem---motivating the problem of capability elicitation and showing some preliminary experiments in the space. The problem is pretty similar to backdoor detection but the new framing does provide different constraints which I find interesting: (1) the test model (in this case a password-locked model) doesn't necessarily have to be poisoned in a sneaky way like in backdoor attacks, instead the goal is to produce a test model in any way that somehow hides capabilities in a way that realistically imitates ""scheming"" or ""sandbagging""; (2) the attacker is not necessarily given access to the training data or high-quality data.
- Though I think it's unsurprising that fine-tuning successfully unlocks these password-locked models, I think the finding that fine-tuning on lower quality data doesn't work as well is an interesting contribution (a priori I could imagine that fine-tuning on lower quality data would still work to fully unlock the model if the password-locking were implemented internally as a simple conditional). The result in Fig 10 where domain generalization still works when each domain has a different password is also slightly surprising.
- Overall I like the experimental methodology (testing both supervised fine-tuning and RL, modulating the quality of the fine-tuning data, testing generalization across domains---all choices motivated by their problem framing).

Weaknesses:
The technical contributions here are somewhat limited. I'd say password-locking is a fairly simple way of modeling hidden capabilities and could be quite unlike the threat models that the authors are actually interested in (situationally-aware LLMs or AI labs deliberately hiding capabilities from auditors; in the case of situationally-aware models, presumably the model does some kind of complex internal reasoning to determine their next action, whereas these password-locked models seem likely to just learn some kind of simple conditional to output X or Y). Indeed, the authors acknowledge this briefly in the limitations section.  To some extent I think this is a result of the these threats not being understood well enough for us to know how to model them in a more realistic way than these password-locked models. Given the simplicity of password-locking I'd say that the success of the authors' elicitation methods are mostly unsurprising and also may not generalize to their real threat models.

Limitations:
The authors' limitations section is well written, and I can't think of any direct negative societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the problem of testing advanced LLMs for non-obvious capabilities in a toy setting, where a model has been fine-tuned to only perform well when presented with a fixed password, and to perform poorly otherwise. This is meant to simulate the difficulty of finding the correct prompt for eliciting a particular capability among many other suboptimal prompts. The authors then study various methods of fine-tuning to trigger the strong performance even without the right password, such as supervised fine-tuning on high/low-quality demonstrations as well as various forms of RL which do not require demonstrations at all. They find that performance on par with original with-password performance can be easily achieved, suggesting that prompt fragility and variability may not be difficult problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The problem is interesting, and the authors present a creative reduction to the toy problem of password-locked models
* A wide array of elicitation methods is considered, from many different settings of SFT as well as various demonstration-free methods using reinforcement learning to explore the more difficult where we do not have access to high quality demonstrations. Editorially, I do not think this is a very important setting, since it seems unlikely that we would ever be testing models for capabilities for which we have no idea what successful behaviors look like. And the dependence on RL approaches, which are in general much flakier than supervised learning, casts a lot of doubt on whether results in this setting would generalize in the real world. But the authors think this is important and study it carefully.
* Lots of analysis on a diverse set of task settings

Weaknesses:
* The authors explain the definition and process of password-locking models quite thoroughly, but their paper would be better served by a clearer and earlier motivation for studying password-locked models.
* In general, the authors lean heavily on references to prior work to explain various decisions in their experimental design, or different analyses of model performance. It would be helpful if these choices were just explained directly, and then separately credited prior work for motivating these decisions.
* There are various alternative approaches to eliciting stronger performance without the correct password, such as prompt optimization (e.g. with GCG or even gradient-free search) or few-shot prompting, which are not explored in this paper
* The nomenclature throughout the paper is a little confusing. The pi^weak_base notation was hard to keep track of, as well as the locked with password/locked without password/unlocked distinctions
* Given the close connection to neural backdoors/trojans, the authors should explain more clearly why or why not backdoor removal methods are not applicable here

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of eliciting hidden capabilities from language models, through fine-tuning. In order to plant 'locked' capabilities and then do controlled test of elicitation, the authors lock the capabilities using passwords they bake into the prompts at fine-tuning. Then, they try to elicit the capability without the password in a zero shot way, and then after fine-tuning with a few good samples, or using RL and existing judgment labels. They find that using a few good FT samples the capabilities are almost completely attainable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. I liked the idea of using passwords to control for the model behavior, and lock the capabilities.

Weaknesses:
1. I am not sure about the motivation of the paper. Is model locking something that people really do in practice?  Using a set of words in a prompt is pretty weak in practice, there might be stronger ways to lock a model, such as posthoc methods at decoding time. Also the findings and insights are not that surprising. Many safety alignment and jailbreaking papers show that alignment is 'shallow' and can be easily reversed [1,2]

2. Using fine-tuning and RL at decoding time is a pretty strong assumption, as having access to model parameters, training a model and also having access to high quality data is not that realistic.

[1] Patil, Vaidehi et al. “Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks.” ArXiv abs/2309.17410 (2023): n. pag.

[2] Yang, Xianjun, et al. ""Shadow alignment: The ease of subverting safely-aligned language models."" arXiv preprint arXiv:2310.02949 (2023).

Limitations:
The paper discusses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zxSWIdyW3A;"REVIEW 
Summary:
The authors present a Federated Hardware-Prompt learning (FedHP) framework to address the fact that compressive snapshot spectral imaging devices may not be easily tuneable against changes in the coded aperture, and that in fact the said access to coded apertures may not be possible due to privacy reasons. The authors solve this by hardware prompt learning, which essentially learns from observing diverse coded aperture samples of all clients, regularizing the input data space and achieving the goal of coping with heterogeneity sourcing from hardware. The results show on a specific dataset improvement across all 10 samples in terms of spectral reconstruction quality. The comparison is primarily in the sense of federated learning approaches.

Typo: figure 3  caption -> colorblue shouldn’t be there

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The presentation is somewhat accessible to a generally knowledgeable non-expert in federated learning, in that the purposes are clear.

Weaknesses:
The biggest weakness is arguably that the paper covers a somewhat very niche topic, which is the application of a federated learning scheme to compressive snapshot spectral imaging. To some extent one would expect the technique to abstract away from the specific case of CASSI, as the solution does not particularly pertain to CASSI.

In addition, due to limited data available in this setup and to very limited size datasets, it is difficult to ascertain the significance of the findings.

Limitations:
The addressed setup assumes that the problem the authors propose to tackle is meaningfully posed, I.e., that federated learning in the chosen formulation is practically meaningful. The reviewer is not sure whether this is a practically relevant problem considering that CASSI systems are arguably scientific instrumentation/experimental devices whose calibration is likely done per case anyways.

In addition the topic would appear to be more meaningful for publications that cover CASSI systems such as IEEE TGARRS or the like. It is hard for this reviewer to disentangle the margin of novelty of this paper in terms of federated learning approach vs. impact on the target application.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the challenges faced in snapshot compressive imaging (SCI) systems due to hardware shifts and the need for adaptability across multiple hardware configurations. By introducing a hardware-prompt network and leveraging federated learning, the framework enhances the adaptability and performance of SCI models across different hardware configurations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The manuscript is well-organized with a clear and logical structure that enhances the readability of the content.
2. The paper provides a detailed background on SCI and FL. The planned release of the Snapshot Spectral Heterogeneous Dataset (SSHD) will significantly aid future research.
3. Using different coded apertures for different clients closely mirrors real-world scenarios, adding significant practical relevance to the study.

Weaknesses:
1. The literature review on federated learning (FL) heterogeneity in the Introduction section lacks comprehensiveness. There are numerous recent papers addressing heterogeneity in FL that are not cited here. Additionally, the references included are somewhat outdated. Including more current and diverse references would strengthen the review and provide a more accurate context for the study.
2. the manuscript explains that the coded apertures for each client follow a specific distribution Pc, it does not provide further details about the exact nature or type of this distribution.
3. There are many ways to partition data to construct heterogeneous scenarios, such as practical and pathological methods. The approach of equally splitting the training dataset according to the number of clients is not very convincing. The authors should try different partitioning methods.
4. It is unclear which datasets were used to obtain the experimental results in Tables 1 and 2. The authors did not specify this, which creates confusion in the experimental analysis.

Limitations:
1. In the ""Discussion of the client number"" section, the number of clients increases very little, and the metrics slightly decline. However, the authors conclude that the performance is stable with the change in the number of clients. The small variation in the number of clients is unconvincing. A larger difference in the number of clients should be set to demonstrate this more effectively.
2. The authors mention the ""presence of data privacy"" in the contributions, but there is no further discussion or experimental comparison regarding data privacy in the subsequent sections. This makes it difficult to validate their contribution to data privacy protection.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Most existing reconstruction models in snapshot compressive imaging systems are trained using a single hardware configuration, making them highly susceptible to hardware variations. Previous approaches attempted to address this issue by centralizing data from multiple hardware configurations for training, but this proved difficult due to hardware heterogeneity across different platforms and privacy concerns. This paper proposes a Federated Hardware-Prompt Learning (FedHP) framework, which aligns data distributions across different hardware configurations by correcting the data distribution at the source, thereby enabling the trained model to adapt to multiple hardware configurations. The performance on existing datasets shows an improvement compared to previous popular training frameworks. Additionally, the authors have released their own created dataset and code.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.Previous work focused on the data itself, directly correcting various types of data through network models. In contrast, the authors of this paper focus on the root cause of the differences—hardware. They address the issue from the perspective of learning the differences in hardware.
2.The method proposed by the authors has achieved excellent performance compared to existing mainstream methods, and the average performance has also improved.

Weaknesses:
1.The number of clients used in the experiments is still relatively small. Although a simple comparison of the impact of different numbers of clients was made, there is not much difference in performance compared to other methods when the number of clients is larger.
2.Although good results were reported on simulated data, more results on real data should be included to evaluate the effectiveness of the proosed method.

Limitations:
The generalization to different hardware systems is crucial for deep learning based methods. The current form of this manuscript only reported on a small scale real dataset captured by several systems. A larger dataset captured by more systems is necessary to evaluate the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces FedHP, a reconstruction method for snapshot compressive imaging systems, which addresses the challenge of cross-hardware learning by proposing a federated learning approach. The key contribution lies in using a hardware-conditioned prompter to align data distributions across different hardware configurations, thereby enhancing the adaptability of pre-trained models without compromising data privacy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The writing of the paper is good, making it easy to read and follow with clear arguments.
2. The problem defined in the paper is novel with a clear motivation, providing good inspiration for solving the issue of inconsistent device configurations in snapshot compressive imaging.
3. The proposed method is clear and the conclusions are relatively convincing. Overall, it is an interesting work.

Weaknesses:
1. There are some typos in the writing. For example, the caption of Figure 3 and the bold parts in the second row of Table 1 and the eighth row of Table 2 are confusing.
2. The proposed FedHP method is relatively straightforward and lacks deeper insights. Moreover, it does not show a significant performance improvement compared to FedAvg.
3. The experiments are not comprehensive enough. Given that this work aims to address the snapshot compressive imaging (SCI) problem, I suggest adding experiments to test the applicability of other SCI systems, such as Coded Aperture Compressive Temporal Imaging (CACTI).
4. There is a lack of sufficient real-world experiments. It would be beneficial to set up multiple independent SCI systems to test the algorithm's performance. Including reconstruction results obtained from these real-world systems is recommended.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zw2K6LfFI9;"REVIEW 
Summary:
The paper proposes a framework that integrates large multimodal language models (MLLMs) and diffusion models to enable holistic language planning and vision planning for long-horizon robotic manipulation tasks with complex instructions. The authors jointly train the MLLM and diffusion model for language reasoning and visual imagination through latent image token generation. An explicit consistency loss aligns the reasoned instructions with the imagined subgoal images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel motivation for integrating of multiple modalities for providing better guidance.

2. Principled design of the framework components like the encoding-side alignment and the latent image token generation approach.

Weaknesses:
1. Weak experimental evaluation (see below questions).

Limitations:
Yes, the authors address the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles the problem of long-horizon task planning on pick-and-place tasks in the Ravens domain. Given a dataset of trajectories, it first learns the projection to align the vision and language encoder for a multimodal LLM. Then it finetunes both the multimodal LLM and a diffusion model to generate a step action in language, where the diffusion model is used to generate a conditioning subgoal image, which is proposed as an intermediate step that helps with the step action generation in language.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well-written and the figures are helpful for understanding the method.

Weaknesses:
- It is unclear, at least from the experiments in the paper, that the diffusion model is actually useful, especially when the output is still in language space. For example, it seems that the tasks studied in the paper can be easily tackled by a modern multimodal language model (likely even the open-sourced ones), by simply providing the the initial image and appropriate prompting. However, this is missing as an important baseline in the paper (and this does not require additional training data). Furthermore, to demonstrate the effectiveness of an image subgoal in addition to a language subgoal, the evaluation would have to be done on tasks that have subgoals that are difficult to describe in language but easy to describe in visual space, but all the evaluated tasks are the contrary.
- A related work “Video Language Planning” also seems to be missing from the paper, despite it might involve closed-sourced models. However, the idea seems quite relevant and it’s unclear if the paper provides additional insights for the community.

Limitations:
The limitations are described in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a holistic vision-language planning method for long-horizon robot manipulation, by learning a multi-modal large language model (MLLM). The MLLM generates interleaved language actions and keyframe images based on language goal and the initial image. Each pair of generated language and keyframe image is used as conditioning of a learned motion policy for robot manipulation.

Based on a pretrained MLLM model, the paper first learns a projector to align visual encoding to with language on image captioning tasks tailored to robot manipulation. Then it applies instruction tuning to fine-tune the MLLM, an output projector, and a diffusion model to generate interleaved language and images. Additional, the authors propose another training objective to align the generated language and images. All large models are fine-tuned with LoRA.

On simulated robot manipulatio benchmarks, the proposed method outperforms imitation learning, language planning, and vision planning methods. The paper also systematically evaluates capabilities of the MLLM along different axes, and justifies the benefits introduced by each loss design via ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the important challenge of robot long-horizon planning. The proposed method plans jointly in the language and image space, providing rich information for the low-level policy to condition on.
- The paper exploits the capabilities of MLLM to generate language and images for robot manipulation, used with a separate low-level policy. I think this is good practice as MLLM is not naturally suitable to generate robot motion.
- The experiments are comprehensive and provide useful information on understanding the capability of the trained MLLM.
- The paper is in general well-written and easy to follow.

Weaknesses:
- The explanation of low-level policy is missing from the main paper. This part is very important - the MLLM outputs language and images only, and it's not clear how these modalities are bridged with robot motion.
- The contribution of the alignment loss between generated image and language is not sufficiently justified in the experiment. It will be helpful if the authors can provide the task success rate when the loss is absent.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on robotic manipulation with complex instructions. It proposes PERIA, a framework that integrates MLLM and diffusion models to incorporate both language planning and visual planning for long-horizon language-instructed manipulation tasks. Specifically, PERIA first performs a lightweight multi-modal alignment to consolidate the multi-modal perception capabilities. Then, PERIA performs multi-modal instruction tuning, where it outputs both subgoal language descriptions and visual tokens, both of which are fed to a diffusion model to generate subgoal images. PERIA introduces an additional consistency loss between and generated subgoal image and language descriptions. Experimental results demonstrate that PERIA significantly outperforms competitive baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•	This work follows a natural and reasonable pipeline to tackle the manipulation tasks with complex language instructions. Combining language planning and visual generation for manipulation is a sound approach.

•	The alignment stage empowers the overall capabilities, as demonstrated in the experimental part.

•	PERIA achieves convincing experimental results compared with previous works. The authors also conduct extensive ablative study to mine more insights.

Weaknesses:
•	End-to-end learning for such a large system requires considerable cost. Such a comprehensive framework may lead to powerful performances but the resources may be a limitation. This paper does not present how much resources PERIA uses or related experiments to address such potential concerns.

•	One of my concerns is that the consistency objective, which forces the MLLM to output subgoal language descriptions, may suffer from accumulative error. This is because when the generated subgoal image is not the desired image but is a natural image that can be reached within one-step action, the MLLM would learn an incorrect subgoal description.

•	More literature references and related baselines should be incorporated.

•	The ablation in visual planning lacks an experiment where PERIA generates subgoal images with either subgoal descriptions or generated visual tokens, which should reveal more insights into what leads to the improvements in visual planning.

Limitations:
Yes, the authors address the limitations at the end of the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zv9gYC3xgF;"REVIEW 
Summary:
The paper studies the convergence of EM for learning mixtures of Gaussians.  Specifically, they consider a simplified setting where the Gaussians are in $d$-dimensions and all have covariance $I_d$.  They consider an overparameterized version of the problem where they parametrize the mixture they are trying to learn by a mixture of $n$ Gaussians with means $\mu_1, \dots , \mu_n$ and the ground truth distribution generating the data just consists of a single Gaussian $N(\mu^* , I_d)$.  The paper analyzes the dynamics of gradient EM for this problem.  The main result of the paper is proving that for this overparametrized variant, gradient EM converges to the true distribution at a rate of $1/\sqrt{t}$ with additional constants depending exponentially on the distance between the initialized means and the true mean, which they show is necessary.

There has been a long line of work on understanding the convergence of EM or gradient EM for learning mixtures of Gaussians.  Without overparametrization, provable convergence is known for mixtures of two Gaussians and it is also known that convergence fails in general for mixtures of three or more components.  For overparamterized settings, a previous work [Dwivedi et. al. 2018] shows that if we parametrize a mixture of two Gaussians and try to learn a ground truth distribution consisting of a single Gaussian, then EM converges at a $1/\sqrt{t}$ rate (as long as the mixing weights are set to be different).  This is in contrast to when we parametrize with only a single Gaussian and EM converges exponentially fast.  The results of the current paper can be seen as generalizing the results of [Dwivedi et. al. 2018] to more than two components.  The paper empirically validates their theoretical results with experiments on simple synthetic datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper makes progress on a well-studied problem of understanding convergence of EM for learning GMMs.  They give the first global convergence results for mixtures with more than two components.

The paper overcomes nontrivial technical barriers to extend previous results to more than two components.

Weaknesses:
The results of the paper only work when the ground truth is ""trivial"" i.e. a single Gaussian.

The results are qualitatively similar to previous work on overparametrized mixtures of two Gaussians.  The contributions of the paper are mostly technical and it is a bit difficult to find a nice conceptual takeaway \--- the previous work for two components already showed that overparametrization can lead to drastically slower convergence.  It would be much more exciting and novel, say, if we could prove something when the ground truth were not just a single Gaussian.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper talks about the gradient-EM algorithm for over-parameterized GMM. The paper mostly shows the GLOBAL convergence and its rate when using this model to learn a single Gaussian.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
I believe any non-convex global convergence optimization problem is valuable. It is an extension of Dwivedi et al. 2019.

Weaknesses:
1. The over-parametrized model may have severe overfitting problem. 
2. The based distribution is quite easy: a single normal, with known variance. In the paper, the covariance is fixed as the identity, which simplifies the problem in a deep way. Actually for symmetric 2-GMM, there are already faster algorithms to learn both mean and cov. 
3. I feel confused about the consistency and convergence in the paper. In Line 96, the convergence of KL divergence also contains the convergence of MLE, ie consistency. The convergence to the MLE is another loss function. Also in Remark 6, the convergence when sample size to infinity seems more easily ensured by WLLN.

Limitations:
Besides above, 
7. the citation format is not uniform.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focuses on the setting of a Gaussian Mixture Model with several summands and an input vector produced by one Gaussian distribution, where it employs the Expectation-Maximization rule to infer the model's parameters. Since the problem of having arbitrary number of summands has been unsolved, the paper provides an innovative scheme which includes the computation of the likelihood function and shows that the EM algorithm converges with sublinear complexity. 

The authors also show that there exist neighborhoods of slow convergence rates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is well written, the theorems, lemmata and algorithmic steps are described gradually.
- From a first overview of the literature, the result about global convergence seems novel. 
- Across section 4, there is intuition and remarks provided about the necessity of the steps.

Weaknesses:
- The experimental evaluation is used as a proof of concept and thus is limited. The authors could have (potentially) experimented with several datasets, with varying weights in the GMM, and try to benchmark their algorithm to compare the emergent convergence rates.

Limitations:
NA.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers fitting a single Gaussian with multiple-component Gaussian mixture models (GMM) through the Gradient EM algorithm. While the two balanced over-specified Gaussian setting has been widely studied in the previous work, generalizing it to multiple-component GMM requires significant algebraic efforts. The entirety of the paper is to show the $1/\sqrt{t}$ convergence rate of the population EM algorithm. In particular, the paper characterizes the explicit convergence rate of $1/\sqrt{T}$ with constants exponential in the number of components, the phenomenon that coincides with the exponential lower bound for the parameter estimation of general GMMs with no separation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
-	Extending some existing two-component results to general multiple-component GMM is non-trivial and significant. The paper nicely characterizes the convergence rate that captures some important properties of learning GMM that can be achieved by GMM. 

-	The paper is well-written, emphasizing important aspects of the results and well-contrasting their techniques to existing results. 

-	Proof sketch is nicely written to help readers understand their key results.

Weaknesses:
-	While the lower bound result (Theorem 7) is a nice addition to the literature, I believe that the gap between this lower bound and the upper bound is large, since the upper bound is exponentially slow in the number of components. 

-	One important result from two specified GMM is the $n^{-1/4}$ (n is the number of samples here) statistical rate after convergence. I would like to see $n^{-1/2k}$ style results in general k-component GMM settings. At least, the authors should have discussed this aspect of previous work and contrasted the implications to k-GMM settings. 

-	The experiment would have been nicer if the final statistical rates were compared.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zv4UISZzp5;"REVIEW 
Summary:
This paper proposes a method of generating prompts for evaluating large language models such that the prompts are dynamic and allow for showing meaningful performance gaps between different language models.The authors show that the generated data is more-challenging and discriminative than prior datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Work is very timely and addresses a major issue in how we can better evaluate LLMs which are continuously improving and saturating existing benchmarks.
- Good to see that the generated prompts are indeed harder than baseline datasets - this should indicate that the prompts are challenging enough to provide decent signal on a language model's capabilities.
- Experimented with many SOTA models and compared with several baseline datasets.

Weaknesses:
The main weakness of this work is that much of the pipeline relies prompting language models to modify seed data. This means that the performance of the language model plays a huge role in the quality of the resulting data. Given that the pipeline seems to have many different steps, each of these steps can introduce errors since LLMs are not fully reliable. It then becomes crucial to have a way of verifying that the generated questions are of high quality. There's also a concern that the ground truth answers might not be entirely accurate. The authors mention both of these issues as limitations.

Limitations:
The authors mention the most-important limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a prompt synthesis framework for evaluating LLMs to accurately reflect different Large Language Model abilities. The authors develop two models to measure LLMs’ question discriminative power and difficulty. This study presents “instruction gradient” and “response gradient” methods to exploit rule sets to generalize questions.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper focuses on the generation of a large number of queries and corresponding answers on general language and mathematical topics. They have released a set of over 3000 questions for LLM evaluation. Their proposed metrics (discrimination index and difficulty score) show significant improvement in the quality of the benchmark datasets.

Weaknesses:
Although the paper tries to solve a crucial research area in the scope of LLM evaluation, the study lacks in many different ways. The textual flow is difficult to follow. Many of the concepts introduced were not properly described or not cited with previous work’s references. These issues restricted the reviewability of this study.

Limitations:
While this proposed method is understood to work on general text questions fairly well, mathematical questions are the weakest part of this study.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel framework for evaluating Large Language Models LLMs) based on Item Discrimination ID theory, which generates adaptive, high- quality prompts to effectively differentiate model performance. Key contributions include a dynamic evaluation set that evolves with LLM advancements, a self- correct mechanism for prompt precision, and models to estimate prompt discrimination and difficulty. The authors validate their framework by testing it on five state-of-the-art models and release a dataset of over 3,000 prompts to aid further research, demonstrating enhanced challenge and discrimination over previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper proposes a novel prompt generation method to produce more challenging evaluation data.
The paper is well-structured and clearly written. The methodology and evaluation criteria are explained clearly, making the paper accessible to a broad audience.

Weaknesses:
The paper only used one LLM Hunyuan) to generalize data and did not verify whether the proposed method can generalize to other LLMs.
It is debatable whether using test data generated by an LLM to evaluate the performance of LLMs has practical value. The paper lacks validation of the effectiveness of the machine-generated test set, such as comparing its metrics with those of other human-annotated datasets.
The paper lacks an analysis of the diversity of the data used to produce the test set.

Limitations:
The authors have identified some limitations; however, there are additional ones that I have raised in the weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zuwpeRkJNH;"REVIEW 
Summary:
The paper addresses challenges in surgical video-language pretraining (VLP) due to the knowledge domain gap and scarcity of multi-modal data. It proposes a hierarchical knowledge augmentation approach and the Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) framework. This approach enhances data efficacy and tackles spatial-temporal challenges by combining language supervision with visual self-supervision. Extensive experiments demonstrate significant improvements in zero-shot transferring performance and the generalist visual representation for surgical scene understanding.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper presents a unique approach to surgical video-language pretraining by employing hierarchical knowledge augmentation using LLMs, significantly improving textual data quality and diversity. The PeskaVLP framework innovatively integrates visual and language supervision, addressing the spatial-temporal challenges in surgical scene understanding. The methodology is meticulously validated through extensive zero-shot and linear-probing evaluations on datasets such as Cholec80 and AutoLaparo, demonstrating substantial performance improvements. The clarity of the presentation, with well-organized sections and effective visual aids, facilitates comprehension. The significant contribution lies in enhancing surgical scene understanding and cross-modal retrieval, making it highly valuable for the NeurIPS community. The paper's originality in using hierarchical pretraining and the detailed discussion on model architectures and initialization underscore its quality and significance in advancing surgical data science.

Weaknesses:
Firstly, the dataset size is relatively small, with 1,007 videos for phase-level pretraining and 920 for video-level pretraining, which may limit the generalizability of the findings (as mentioned in the supplementary material). I know the difficulty in collecting medical data, but we must be sure that the presented approach can be generalized to different domains and hospitals. Furthermore, I doubt the methodology's potential to process ""noisy"" videos.    
Expanding the dataset and including more diverse surgical procedures would improve robustness. 

Secondly, while the paper mentions ASR errors in transcriptions, it does not provide a detailed methodology for handling them. Providing specific techniques for improving transcription accuracy would strengthen the study. 

Additionally, the practical implementation of the PeskaVLP framework in real-world surgical contexts is not thoroughly discussed. Detailing strategies for integration into clinical workflows and addressing potential technological barriers would be beneficial.

Limitations:
The authors have acknowledged the limitations related to dataset size and ASR errors but could elaborate on strategies to mitigate these issues. Specifically, they should discuss plans for expanding the dataset, incorporating more diverse samples, and improving transcription accuracy.

The positive societal impacts, such as enhancing surgical training and assistance, are well-discussed. However, the authors should address potential negative impacts, such as data privacy and ethical concerns. A detailed discussion on data security measures, user consent protocols, and ethical safeguards is needed.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach for enhancing surgical video analysis by incorporating procedural awareness. The authors propose a system that integrates knowledge of surgical procedures to improve the identification, segmentation, and annotation of surgical activities in video footage. This approach aims to address challenges such as the variability of surgical techniques and the complexity of visual data in operating rooms. The contributions of the paper include the development of a procedural model that can be aligned with video data, the creation of annotated datasets for training and evaluation, and the demonstration of improved performance over traditional video analysis methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.The integration of procedural knowledge into surgical video analysis is a highly original concept. This approach not only enhances the accuracy of video analysis but also opens new avenues for improving surgical training and documentation.

2.Introduces a novel hierarchical knowledge augmentation technique using large language models to refine surgical concepts. Employs a Dynamic Time Warping-based loss function for effective cross-modal procedural alignment. Demonstrates significant improvements in zero-shot transfer performance across multiple surgical datasets. Provides a robust general visual representation beneficial for various surgical scene understanding tasks.
Weaknesses:

3.The potential applications of this research in surgical training, intraoperative assistance, and postoperative review are significant. The approach addresses a critical need in medical video analysis, making it highly relevant and impactful.

Weaknesses:
Dataset Limitations: The annotated datasets used for training and evaluation are crucial for the model's success. Expanding the diversity and volume of these datasets would enhance the generalizability of the findings.

Limitations:
The paper does not adequately address potential limitations and negative societal impacts.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a Procedure-Encoded Surgical Knowledge-Augmented Video-Language Pretraining (PeskaVLP) method that enriches language supervision with LLM-refined surgical concepts. It further constructs hard negative samples by reversing the text orders at the phase and video levels and employs a Dynamic Time Warping (DTW) based loss to align multimodal procedures. Extensive experiments on multiple surgical procedures and comprehensive evaluations demonstrate the effectiveness of this framework.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well-written, with the background and motivation well-stated.
- Using LLM to augment surgical video text descriptions is a good idea to enhance the quality of surgical text narration. It establishes a good baseline and guideline for future works that aim to apply LLM in surgical narratives.
- A more comprehensive parent-child level cross-modal correspondence was designed using DTW than existing works.
- Demonstration of the proposed method can close the representation gap for different modality, and analysed both successful and complicated examples.

Weaknesses:
- By reading the enriched dataset by LLM in Appendix H, I am concerning that the variation and diversity of narration will be removed by the augmentation. Will that cause any problems?
- In my opinion, using LLM to refine the text description of surgical videos is the most important contribution of this paper. It would be interesting to see if other components are also effective enough without the knowledge augmentation.

Limitations:
The authors adequately addressed the limitations. Since the proposed method is tailored for surgical data and applications, it is strongly suggested that the authors include a discussion on the potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new framework called PeskaVLP for surgical video-language pretraining. A hierarchical knowledge augmentation approach is used for enriching text information. The pretraining is implemented with the proposed language supervision and visual self-supervision. A new training objective is proposed for surgical procedural understanding. Extensive experiments are conducted to demonstrate the effectiveness on the surgical phase recognition task and cross-modal retrieval task on multiple downstream dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper addresses the problem of VLP in the surgical scene. A hierarchical knowledge augmentation is proposed to tackle the problem of lack of textual information in the surgical field.
2. The paper is generally well-written and easy to follow.

Weaknesses:
1. The explanation of method details is not clear enough, and there is a lack of discussion on some experimental results
2. The proposed method is based on certain assumptions but lacks a comprehensive consideration of applicability.

Limitations:
Authors discussed it briefly in the appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zuwLGhgxtQ;"REVIEW 
Summary:
The paper investigates the complexity of sampling from heavy-tailed distributions and presents a distinction between obtaining high-accuracy and low-accuracy guarantees. It analyzes two types of proximal samplers: those based on Gaussian oracles and those based on stable oracles. The main findings are that Gaussian oracle-based samplers can only achieve low-accuracy guarantees when sampling from heavy-tailed distributions, while stable oracle-based samplers can achieve high-accuracy guarantees. Additionally, the paper establishes lower bounds for samplers using the stable oracle, indicating that the presented upper bounds are optimal and cannot be fundamentally improved.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well-motivated and interesting. 
2. Designed the algorithms and derived the upper bounds and lower bounds for different settings. 
3. The authors also provided insightful discussion.
4. The authors provided solid theoretical proof for the results.

Weaknesses:
There is no experiment to verify the theoretical findings.

Limitations:
There is no experiment.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of heavy-tailed sampling. First, the paper shows that while the gaussian proximal samplers are efficient for light-tailed targets, they are not accurate for heavy-tailed ones; the paper develops a lower bounds for the Gaussian proximal samplers, which reveals a fundamental challenge in heavy-tailed settings.

Then, the paper proceeds to develop a novel samplers based on restricted alpha-stable oracle; the insight is to replace the standard heat equation in gaussian oracle with a fractional heat flow. The paper proves that under suitable conditions the proposed sampler is efficient for heavy-tailed targets. Additionally, the paper proposes a practical implementation for a particular case of alpha=1.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novel theoretical analysis for the gaussian oracle sampler, which provides a new insight to developing sampling algorithms

- A novel methodology for heavy-tailed sampling

Weaknesses:
- The paper is purely theoretical and lacks experimental evaluation; it would be nice to at least have a toy illustration for the implementable algorithm 2+3 in the alpha=1 case.

- As the authors discussed in Sec5, the current paper does not present implementable algorithms for general alpha values in (0,2).

Limitations:
Most of the limitations have been touched upon in sec 5. Otherwise see the weakness comments.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focus on studying the complexity of heavy-tailed sampling and present a separation result in terms of obtaining high-accuracy versus low-accuracy guarantees. Their results are presented for proximal samplers that are based on Gaussian versus stable oracles. Authors show that proximal samplers based on the Gaussian oracle have a fundamental barrier in that they necessarily achieve only low-accuracy guarantees when sampling from a class of heavy-tailed targets. In contrast, proximal samplers based on the stable oracle exhibit high-accuracy guarantees, thereby overcoming the aforementioned limitation. They also prove lower bounds for samplers under the stable oracle and show that our upper bounds cannot be fundamentally improved.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Although I am not an expert in this field, I find this work quite interesting. The authors provide new material and support their statements with proofs.

Weaknesses:
The paper is not tested in any way on a numerical experiment. I am convinced that a paper presented at this type of conference should be both motivated by a real-world application and tested numerically, e.g., on a near-real-world formulation of the problem.

**After a rebuttal process**, the authors agreed with this weakness and promised to add the experiments to the final version of the paper.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide a lower bound for sampling from heavy tailed distributions under the Gaussian oracle of order $O(\textup{poly}(1/\varepsilon))$. They then propose an alternative proximal sampling algorithm using the $\alpha$-stable oracle that achieves a convergence rate of $O(\log(1/\varepsilon))$ for heavy-tailed distributions satisfying a fractional Poincare inequality. They then provide a practical implementation of the stable proximal sampler, and lower bounds on its convergence rate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This work presents a very nice combination of results showing a separation in the performance of stable and Gaussian proximal samplers. The combination of lower and upper bounds separating the two methods makes the work a particularly interesting contribution.

- The addition of a practical implementation of the stable proximal sampler is nice to have, demonstrating that it is viable in practice.

- The work is generally clearly presented and the authors are clear about their contributions.

- Overall, I consider this to be a very sound piece of theoretical work.

Weaknesses:
I have no major concerns about this paper. The presentation is somewhat dense in places, though this is mostly just a consequence of it being a very technical paper and not a flaw as such. If the authors want to make the claim that practicioners should use the stable proximal sampler in applied settings, then they may want to provide empirical evidence of its performance compared to the Gaussian proximal sampler. However, I understand that this is not the main purpose of this theoretical paper.

Limitations:
The authors provide an adequate discussion of the limitations of their methods in the final section, and I foresee no additional negative impacts of their work.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the complexity of sampling heavy-tailed distributions. It provides lower bounds on the complexity of Gaussian-based samplers for a class of heavy-tailed targets. Then, the paper constructs proximal samplers based on stable oracles, which improve the sampling complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper is well-written. The background of sampling and the research problems regarding sampling complexity are clearly introduced. The contributions of the lower bound on Gaussian-based samplers for heavy-tailed targets and the improved complexity using stable oracles are clearly presented.
* The paper is technically sound. The definitions and assumptions are discussed clearly, and the theoretical results are supported by proof sketches.

Weaknesses:
The contribution of the paper could be improved with empirical experiments to evaluate the sampling algorithms and their complexity.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ztwl4ubnXV;"REVIEW 
Summary:
The paper introduces ""AnonFair,"" a toolkit designed to enforce algorithmic fairness across various domains, including NLP, computer vision, and traditional tabular data. It is compatible with popular machine learning frameworks like sklearn, AutoGluon, and PyTorch. Unlike well-established fairness tools like FairLearn and AIF360, AnonFair extends to different types of data, including NLP and vision.

Other tools offer many methods but limited control over them, while AnonFair uses a single, highly customizable method that allows for per-group thresholding.

It specifically addresses the issue of overfitting by utilizing validation data, making it more reliable when traditional methods might fail.

Empirical evidence presented shows that AnonFair performs well, often matching or surpassing other methods in fairness benchmarks without being specifically optimized for complex or high-dimensional scenarios.

AnonFair seems to provide a robust and adaptable solution for implementing fairness in machine learning, in ways that other tools do not currently offer.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper does well in positioning AnonFair against competing tools by demonstrating its performance on standard fairness metrics and its versatility across a variety of use cases.
- AnonFair supports NLP and computer vision classification tasks, allowing broader applicability.
- The toolkit uses validation data to combat overfitting, ensuring that fairness measures remain robust across both training and unseen data.

- The toolkit not only competes well in terms of accuracy and fairness metrics but also offers significant advantages in computational efficiency.

Weaknesses:
- Some sections are overly detailed, such as the introduction, while others are missing necessary depth:
    - Section 3 could use a clearer structure, possibly with a diagram, to help readers understand how to interact with the toolkit.
    - The section on toolkit expressiveness needs more detailed examples and explanations of how the supported fairness measures are implemented. 
    - Results discussion is kept very brief and could benefit from specific numerical examples, like percentage improvements compared to other methods.m actual numbers, such as how much % improvement in comparison to method XY and such.

- The paper assumes readers are familiar with fairness terminology and metrics without adequate explanations or definitions for some acronyms (e.g., DEO in Table 3 and 4).
    - Subsection 4.3 lists supported fairness measures but fails to provide examples or brief explanations, making it less informative for those not familiar with these terms.

- Lack of consistency in terminology usage; for example, ""EOp"" in Figure 1 (top right) vs. ""EO"" in Section 5.2, “AnonFair” missing before ""Frontier"" in Figure 1 (left), and inconsistent references like ""See Figure"" vs. ""See fig..""

- A stronger call to action for community engagement, such as through open-source collaboration or empirical validation studies, could significantly enhance the broader impact and encourage more widespread adoption and refinement of AnonFair.

- The paper would benefit from a summary of explicit cases and recommendations advising users on the best scenarios for using the tool.

- Figure 2 is not referred to in the paper, or did I miss this part.

Limitations:
Some of the limitations are acknowledged, but could be expanded with more actionable insights. 
A call to action for community engagement, such as through open-source collaboration would also encourage broader impact and adoption of AnonFair against its competitors. 
It would be beneficial if the authors suggested potential improvements or future research directions for the suboptimal fairness metrics and data scarcity issues mentioned.
The broader impact section identifies ethical concerns well. However, detailing the intended applications and scenarios where AnonFair might be most effective, or where it could fail, would provide readers and users with clearer guidance on its practical use and limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper describes a new toolkit for algorithmic fairness, enabling the optimization of any fairness measure that is a function of the confusion matrix. Experiments on vision and NLP demonstrated the effectiveness of the proposed toolkit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
An easy-to-use toolkit for enforcing algorithmic fairness.

Weaknesses:
Presentation could be made more self-contained, e.g. a table listing the supported fairness metrics, as functions of the confusion matrix. This would help readers not familiar with the field.

It seems that only binary classification is supported. How can such metrics be extended to other tasks?

Some minimal code snippets for the interface could be shown as examples.

Limitations:
The authors adequately discussed the limitations of their toolkit.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new toolkit designed to enhance algorithmic fairness with greater expressiveness. Unlike existing toolkits, this one offers more customization options to optimize user-defined objectives and fairness constraints. Although the proposed toolkit currently includes only one method, it supports both computer vision and natural language processing (NLP) tasks. The authors compare the efficiency of this method, finding that the toolkit is relatively more efficient than Fairlearn. Comprehensive experiments were conducted on various datasets, and the results were compared with those from other popular toolkits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a versatile toolkit that supports both NLP and computer vision tasks, unlike existing toolkits which lack this capability.
- The proposed toolkit employs efficient optimization techniques that accelerate the evaluation process.

Weaknesses:
- The formulation presented in Subsection 4.2 of the paper is limited to a single-layer model, which restricts its applicability across different machine learning models. To enhance the flexibility of the method, I recommend adopting a more generic notation, particularly if we aim to incorporate pretrained language models.
- The abstract is quite unclear, especially the part that mentions ""9/9 and 10/10 of the group metrics of two popular review papers."" I suggest rephrasing the abstract for better clarity and comprehension.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes details of a fairness toolkit (""AnonFair""), which confers fairness to any given machine learning classifier by exploring a wide range of prediction thresholds for different groups (which are either provided upfront or inferred through an auxiliary classifier). The toolkit is designed to be quite expressive, as it can optimize several different metrics, e.g., false positives/negatives, true positives, etc. The toolkit can work across all classifiers (which can output class probabilities), including ones trained on vision and NLP tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper introduces and describes a toolkit that implements several fairness strategies and can support any fairness measure that can be expressed in terms of true positives, false positives, true negatives and false negatives. These techniques primarily rest upon adjusting the classification thresholds of different groups, and the paper also incorporates tricks to speed up their computations of precision and recall across different thresholds. The fairness techniques that this paper implements are (largely) classifier agnostic, and can be applied to a wide range of classifiers including NLP and vision classifiers (as this paper shows). Overall, I appreciate that expressivity and broad applicability of their toolkit.

Weaknesses:
While the toolkit might turn out to be useful for some practitioners, it is a relatively straightforward implementation of well-known (and simple) technique of adjusting prediction thresholds across groups. Exploring different thresholds can be computationally prohibitive, for which the authors use a standard trick to speed up their explorations (which I appreciate). The paper acknowledges and cites relevant papers/techniques that they implement.   Overall, the originality and novelty of their work is significantly limited, as the toolkit is an implementation of known and simple fairness techniques. Further, the underlying fairness techniques (not from the authors) are themselves applicable to most classifiers, so any implementation of the same could work for NLP and vision tasks—which is claimed to be one of the major contributions of this work.

Limitations:
I believe the paper adequately communicates their shortcomings and cites past references when using them. However, I think it might help to also acknowledge that the underlying fairness techniques broadly apply to a wide range of classifiers, and naturally extend to classifiers in computer vision and NLP domains. Reading parts of the paper felt like that there are significant challenges in adoption of fairness techniques to NLP and CV, and this paper overcomes them through novel solutions—which is not the case.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents AnonFair, a cutting-edge open-source toolkit designed to promote algorithmic fairness. Authors claim the following contributions:
(1) Comprehensive support for NLP and Computer Vision classification, as well as standard tabular problems.
(2) Enhanced robustness against overfitting challenges through the ability to enforce fairness on validation data.
(3) Versatility in optimizing any measure that is a function of True Positives, False Positives, False Negatives, and True Negatives, making it easily adaptable and more expressive than other toolkits.
(4) Seamless integration with popular ML toolkits such as sklearn, Autogluon, and pytorch.
(5) AnonFair supports 9/9 and 10/10 of the group metrics of two prominent review papers and is accessible online at no cost.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This toolkit progresses in algorithmic fairness and enhances multidisciplinary collaborations, it is design to integrate the intervention of policy-makers.

The paper includes a complete section of experiments and comparison with existing toolkits. 

AnonFair key contributions include support to popular and relevant NLP and Computer vision areas.

Weaknesses:
* Lack of clarity in some reported experiments, e.g. results tables are not cited in the text, metrics are not well-contextualized (e.g. larger or lower scores are better?)

* Lack of analysis, examples or human evaluation to better understand contributions and limitations of the method in each of the experiments.

Limitations:
Authors report some limitations, but further analysis on the experiments could raise more limitations that may be currently ignored.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zb8jLAh2VN;"REVIEW 
Summary:
This paper develops a switching RNN (SRNN) framework to model neural activity. It builds up on switching linear dynamical system models that are used in neuroscience to segment and extract underlying dynamics of observed neural activity. The different segments corresponding to unique dynamics often reflect distinct behavioral states. The crucial novelty of this work is that they allow the dynamics to be non-linear, unlike SLDS and rSLDS, making the model more expressive. They fit these models using VI using an inference network. Finally, they apply SRNN to synthetic data, as well as 3 distinct neural datasets and show that it outperforms SLDS and rSLDS on segmenting activity into behavioral modules where each module corresponds to distinct dynamics. They visualize these underlying dynamics, and also evaluate their fitted model on predicting future neural activity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. As we move towards large-scale neural datasets, it is crucial to scale model complexity in order to fully harness these datasets. This paper makes a step in that direction by allowing for non-linear dynamics, while also providing an appropriate fitting approach.
2. The experiment section is extensive, and I appreciate the application to multiple neural datasets. I particularly found the results on the decision-making dataset to be most impressive. 
3. The literature review is thorough, and the authors do a good job of situating their work in the context of other related studies.

Weaknesses:
1. The authors mention switching nonlinear dynamical systems (Dong et al. 2020), and discuss how their work differs from Dong et al. I think it is important to either provide an experimental comparison to SNLDS or a justification for why these existing models are insufficient to explain neural datasets, as the main novelty/motivation for SRNN and SNLDS is very much related (also noted by the authors in the paper). More on this in the  question section.

2. Behavioral segmentations are somewhat subjective in nature, and while I can see that in the experiments shown here they make sense, in a real world setup we may want to infer the number of such segmentations from the data. Here the authors set the number of discrete states to the # of true behavioral states, however this might not be known in practice. Furthermore, there might be distinct sets of dynamics within one behavioral state due to other reasons not totally explicit from behavior. From the current set of results, it is not clear if SRNN is capable of inferring the # of underlying states. I will elaborate more in the questions section on this as well. 

3. I also think the paper will benefit from some editing by the authors. The references are not formatted properly, commas are missing. The referencing to supplementary figures doesn't seem to be working, it links back to figures in the main text. I also think the authors can trim some of the background, such as the section on VI, in favor of explaining some of the experiments such as the Lorenz attractor setup in more detail.

4. While I appreciate the extensive experiments, I find it hard to reconcile some of the results. It seems like in some of the plots (Fig 3C/D, Fig 5C/D) prediction + reconstruction performance across all models is similar. However, the discrete states being inferred look hugely inaccurate for SLDS and rSLDS. I wonder if the authors have thoughts on why this happens.

Limitations:
The authors have addressed limitations in the last section of the paper, and I do not envision any societal impact of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a new class of probabilistic nonlinear state space models called switching RNNs. In essence, this extends the well-known switching linear dynamical system (SLDS) model to switch between nonlinear dynamics governed by a stochastic RNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The results shown in panels A of Figs 3, 4, and 5 are nice and convincing.

Weaknesses:
* Like many other deep learning based approaches, the model is not particularly interpretable. For example, panel F in Figs 3, 4, and 5 shows 2D flow fields for the different hidden states, but the RNN hidden state is 16-dimensional. Here the authors have used PCA to attempt to find a reasonable 2D flow field, but I know from experience that this has the potential to very poorly capture the true dynamics of the system. Intuitively, even small variance dimensions can matter a lot if the flow field changes rapidly along that dimension.

* There are many tunable parameters in this model (e.g. number of continuous and number of discrete states). It is unclear how to choose these on datasets without ground truth, or at least good educated guesses.

* Related to above, I worry a lot about the identifiability of this model. A nonlinear RNN without discrete switching can already model any flow field if given enough units. Thus a model with many continuous states (e.g. $P=128$) but zero discrete states may perform equally well to a model with few continuous states (e.g. $P=16$ or $P=8$) but a handful of discrete states. How would one then go about choosing between these models? Adding discussion or ideally some sort of mathematical analysis regarding the statistical identifiability of the model would be very helpful.

Limitations:
The discussion adequately acknowledges limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to model time series neural population activity using switching recurrent neural networks. The generative model includes discrete latent states

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method does appear to outperform related switching linear dynamical systems approaches in certain contexts.

Weaknesses:
High-level:
- The contribution beyond other switching nonlinear dynamical systems models is not clear. Such models include the cited Dong et al., 2020, as well as Karniol-Tambour et al., ICLR 2024. If there is a contribution beyond these works, the authors should compare against those existing related methods.
- The authors do not demonstrate an ability to automatically determine the appropriate number of discrete states. One approach to this might be ""co-smoothing"" (see Yu et al., Gaussian Process Factor Analysis, 2009).

Details:
- The mathematical details and notation are often unclear. For example, equation 2 does not appear to be a valid probability distribution, given the description that f(.) = tanh(.). Shouldn't this instead be a categorical distribution or similar? Relatedly, f is also used in equation 8, but from the context it appears to denote something entirely different.
- The authors should more clearly describe the cross-validation techniques for used for each dataset. The blanket statement in the intro to Section 4 (""On each dataset, we do N-fold cross-validation, where N equals to the number of conditions, sessions, or subjects in the dataset"") obscures how cross-validation was actually applied in each instance.

Limitations:
The authors address several limitations, including their need to manually set the number of discrete states, their need for good parameter initializations, and the heavy computational requirements for fitting their models.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes switching recurrent neural networks (SRNN), which allow the RNN weights to switch across time. The RNN weights switch based on a latent Markovian process of discrete states. The authors apply SRNN to a simulated dataset following the Lorenz attractor and three real-world neural recordings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clarity: The authors clearly explain the problem, related work, and methodology with well-written equations and easy-to-understand figures. 

- Extensive use of datasets: The paper applies SRNN to numerous real-world neural datasets, illustrating the effectiveness of SRNN in accurately segmenting different datasets in an unsupervised fashion.

Weaknesses:
- Lack of comparison with other methods:
The paper compares SRNN to (r)SLDS models. However, there exist many other models for unsupervised segmentation. For example, ARHMMs and their extensions are simple yet powerful and interpretable models for segmentation [1, 2]. The authors should cite and consider comparisons with multiple model classes.
In addition, the paper notes in line 103 that SRNNs have the most comparable structure to SNLDS, but the authors do not make comparisons. The authors should also cite and compare with [3], which has switching nonlinear dynamics.

[1] Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... & Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

[3] Karniol-Tambour, O., Zoltowski, D. M., Diamanti, E. M., Pinto, L., Tank, D. W., Brody, C. D., & Pillow, J. W. (2022). Modeling communication and switching nonlinear dynamics in multi-region neural activity. bioRxiv, 2022-09.

- Experiments:
The simulated experiment with the Lorenz attractor shows that SRNN does well when it has access to noiseless observations with known state dimensions. In order to have a more convincing simulated experiment, the authors could consider the following. First project the Lorenz attractor to a higher dimensional space and add additive Gaussian noise. Then fit SRNN (and other compared models) to the dataset to see if it can recover the Lorenz attractor and true latent state dimension (using some metric on held-out data). Another simulated experiment could be done with a dataset that simulates the NASCAR track [1,2].

[1] Linderman, S. W., Miller, A. C., Adams, R. P., Blei, D. M., Paninski, L., & Johnson, M. J. (2016). Recurrent switching linear dynamical systems. arXiv preprint arXiv:1610.08466.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

Limitations:
As the authors noted, some limitations of the model are that the model needs good initialization and that the model takes considerably more amount of time to train than simpler models such as SLDSs.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
KrHFICMPjm;"REVIEW 
Summary:
The paper introduce GUIDE, a RLHF framework for real-time RLHF with online and continous human feedback. GUIDE translates the human feedback to dense reward. Addtionally, GUIDE includes a parallel training model that learns a simulated human feedback. By involving 50 participants annotation, GUIDE solves three typical challenging sparse reward environments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is easy to read.
2. GUIDE firstly proposed novel continous human feedback and is also evaluated human annotators.
3. GUIDE demonstrated improvement compared to baselines in 3 environments, and analyzed cognitive tests and analyses.

Weaknesses:
1. My biggest concern comes from the practicality of GUIDE. From both theoretical and experimental perspectives, I find it hard to believe that such a simple continuous feedback model can be applied to real-world scenarios. For example, the paper states in line 38 that ""Current research has demonstrated success primarily in simple, low-dimensional tasks with limited solution spaces."" However, the experiments conducted in the paper also involve environments where baseline algorithms like DDPG or SAC can converge with a good reward function after only about **10 minutes of training**. Moreover, according to the experimental results, GUIDE, which incurs a high cost of human feedback, does not outperform manually designed simple rewards (such as the distance to the target, I think it is not hard to design it). Therefore, despite the fact that the environments used do have continuous actions and image inputs, I believe these environments are not suitable for validating RLHF algorithms because the reward functions are easy to design and the tasks themselves are simple.
2. The core argument of the paper is that continuous real-time feedback is extremely difficult to implement in practice. It requires annotators to constantly provide scalar rewards without pause, and such absolute value annotations are more susceptible to biases from different individuals. Pair-wise annotation is much easier than absolute value annotation and can be conducted asynchronously with the training process. If an AI agent needs to be trained for several days, the cost will be unacceptable.
3. Although the paper suggests using model predictions to synthesize feedback, such a simple supervised learning regression objective is unlikely to accurately model the complex reward distribution. My reasoning is that predicting the relative goodness of A and B is easier than predicting scalar reward values, but there will still be many prediction biases.
4. The definitions of various symbols in the paper are imprecise and confusing, for example:
- What is the meaning of A(s, a) in Equation 1? Also, A(s) = a in the line 123, is it the same?
- difference of Q(s, a) and q?
- How to get the r^hf?
These typos make it very difficult to understand the details of the paper.
5. There is a lack of discussion on recent related works in RLHF, such as:
- [1] White D, Wu M, Novoseller E, et al. Rating-based reinforcement learning[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(9): 10207-10215.  
- [2] Yuan Y, Hao J, Ma Y, et al. Uni-RLHF: Universal Platform and Benchmark Suite for Reinforcement Learning with Diverse Human Feedback[J]. ICLR2024.
- [3] Guan L, Verma M, Guo S S, et al. Widening the pipeline in human-guided reinforcement learning with explanation and context-aware data augmentation[J]. Advances in Neural Information Processing Systems, 2021, 34: 21885-21897.
- [4] Guan L, Valmeekam K, Kambhampati S. Relative behavioral attributes: Filling the gap between symbolic goal specification and reward learning from human preferences[J]. ICLR2023.

Limitations:
Yes, and yes.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new approach to reinforcement learning with human feedback in simple video games. The method relies on continuous human feedback that is provided by the human observer hovering their mouse over a window with a spectrum of positive and negative rewards. Unlike prior approaches, this method converts human feedback directly into a reinforcement learning reward with an added time delay. Moreover, the method includes a model that regresses states into observed human feedback, which allows for simulated human feedback. The effectiveness of the method is demonstrated in three simple games in a human study with 50 participants.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The authors propose a simple way to incorporate continuous human feedback as a reinforcement learning reward with a constant time delay. The environment reward and human feedback are simply added to form the final reward function.

2. It is demonstrated that human preference can be directly regressed from states and actions to provide simulated human feedback.

3. The authors perform an extensive human study showing the effectiveness of their method. They also correlate the subject’s performance in a cognitive test with their ability to guide an RL agent.

Weaknesses:
There are three major unstated assumptions:

1. The delay between an event appearing on the screen and the change in human feedback is constant (Question 1). The authors tune this constant for each environment. But, more complex environments might induce different delays as the human observer might need to think about what they saw.

2. People are able to provide constant feedback (Question 2). This might not be true for more complex environments where certain states might have ambiguous values.

3. The human feedback is Markov (Question 3). This might not be true in more complex games.

## Detailed comments:

* Equation 1 should have Q instead of A. Unless you want to define an advantage function A.
* Equation 2 should have an upper-case Q.
* The term $R_{t+k+1}$ in Equation 2 is not very clear.
* The meaning of “We follow recent advancements in neural architectures and hyperparameter designs” on line 125 is not clear.
* The rest of the paragraph on lines 125 - 127 is superfluous.

## Minor comments:

* Inconsistent spacing between text and citations.
* Calling this approach a “computational framework” might be a bit redundant given the context of the conference.

Limitations:
Limitations are partially addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new framework for human-in-the-loop reinforcement learning, where the human and provide real-time and continuous feedback, and an algorithm where the learning agents uses the human feedback to accelerate policy learning. The paper conducted a user study of 50 subjects to demonstrate the effectiveness of the proposed framework in accelerating policy learning and improving success rates over RL and human-in-the-loop RL baselines. Optionally, a human feedback simulator can also be trained to mimic human feedback after a certain amount of time, reducing the amount of human input.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper proposes human-in-the-loop RL where the human can provide real-time continuous feedback, which is a novel paradigm compared to mainstream existing work which focus on discrete feedback signals.
- This work conducted a user study of 50 subjects, which is the largest among relevant works. This is a great contribution in assessing the effectiveness of human-in-the-loop RL.
- The evaluation is done on three challenging tasks, and GUIDE outperform all the baselines by a large margin on the ""find treasure"" and ""high and seek"" tasks.
- The paper provides a detailed individual difference characterization by conducting a series of human cognitive tests. Analysis of the human cognitive test data provides meaningful insights. These data can also be very useful in future work.

Weaknesses:
- The baselines are generally quite weak. Based on the experiment results, it is unclear whether real-time continuous feedback is necessarily the best way for humans to guide the policy learning. There might be intermediate points on the spectrum of conventional discrete feedback and full continuous feedback that provides the best tradeoff between amount of human input and effectiveness of guiding policy learning.
- Whether the simulated human feedback is helpful is unclear. In both the ""bowling"" the ""find treasure"" task, the score does not increase much after switching to simulated feedback. It might be the case that the simulated human feedback only works for tasks where it is straightforward to model the reward.

Limitations:
The paper focused on the final success rate of policy learning, but did not provide sufficient data from the user's perspective. For example, the user study failed to include a survey regarding whether the real-time feedback system feels easier to use than the discrete feedback system.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new framework, GUIDE, for learning from continuous human feedback in complex decision making domains with continuous action spaces. By framing human feedback as a state-action value function, the framework proposes to learn this function and to combine it additively with the (generally sparse) reward coming from the environment. The feedback is collected in a continuous fashion by asking participants to move their mouse up or down to indicate higher or lower feedback values. In a user study, the paper finds that training agents with this type of feedback yields better performing agents than baselines. After assessing participants in a suite of cognitive tests, it finds that participants that score higher on the cognitive tests trained better agents.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces an interesting new way of collecting continuous human feedback, and shows significant improvement in two out of three tasks considered. The use of cognitive tests as part of the user study is interesting, and uncovers insightful correlation between subject performance and their cognitive test scores.

Weaknesses:
The assumptions regarding what human feedback represents do not seem consistent between section 3 and 4 (see Questions). Further, the treatment of the feedback collection is rather simple (added to environment reward function) and, especially if it does represent a signal regarding the future value of state-action pairs, heuristic. Relative to Tamer and Deep Tamer, which treated human feedback more consistently by using it directly as a proper state-action value function, this paper feels like a regression on that front. 

The implementation of the c-DeepTamer baseline raise a number of questions (see Questions), which shake my confidence in it as a baseline, or as a proper representative for how well Deep Tamer should perform here.

Limitations:
Limitations are discussed in the conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
FCsEvaMorw;"REVIEW 
Summary:
The paper addresses automatic red teaming of large language models through open-ended generation of jailbreaks. The key components of their methodology are 1) a categorization of different jailbreak categories to create a diverse archive of possible jailbreaks, 2) a strategy to evolve and mutate jailbreaks, 3) and a selection process to keep the jailbreaks with the highest quality.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper presents a considerable contribution towards practical automatic redteaming of large language models and address multiple weaknesses of prior work (such as attack diversity). The authors provide an exhaustive empirical evaluation of the proposed method with extensive hyperparameter descriptions.

Weaknesses:
W1: Despite the extensive lists of hyperparameters and explanations, the results are not realistically reproducible without the dataset of generated jailbreaks or the trained model. As adversarial robustness has shown to be brittle in the past, I strongly recommend any safety paper to make it as easy as possible to verify their defense. A considerable amount of powerful open-source large language models is available and there is no particular reason why releasing the trained robust model would provide any additional safety concern to the community. (Note that this is more of a personal concern and will not influence my score as I understand that it might have not been possible for the authors to release the model / code / data)

W2: The robustness evaluation of the the model trained with rainbow teaming data is insufficient. I would argue that safety assessments can never be “fixed” and need to be adaptive and designed for the model at hand. The authors performed a train test split to evaluate the robustness of the model, which is non-adaptive. At least any evaluation with one of the many adversarial attacks proposed for LLMs in the last year would have put the robustness into better perspective. Most defenses proposed in the robustness domain have later been shown to be ineffective and “offline” adversarial training (generating the attacks prior to training) does not yield robustness for image models against stronger attacks in my own experiments. Thus, I am a bit sceptic if rainbow teaming actually improves worst-case adversarial robustness.

W3: Evaluations are limited to the Llama series of models. Experiments on non-aligned models or models trained with less safety fine-tuning would have been interesting. E.g., ""How does rainbow-teaming compare to standard model alignment?"". (relatively minor concern)

I am very likely to raise my score if the authors provide more results regarding the worst-case adversarial robustness of models trained with rainbow teaming or provide a good reason why this is not necessary / out of scope.

Limitations:
The authors provide an extensive list of limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces RAINBOW TEAMING, an approach for generating diverse adversarial prompts to test and improve the robustness of LLMs) The method uses quality-diversity search to produce a wide range of effective adversarial prompts across different categories. The authors demonstrate its effectiveness on state-of-the-art models like Llama 2 and Llama 3.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
RAINBOW TEAMING offers a new perspective on adversarial prompt generation by framing it as a quality-diversity problem.

Weaknesses:
1. The study focuses primarily on Llama 2 and Llama 3 models, citing licensing constraints for not including other major models like GPT-4 or Claude. This focus limits the generalizability of the findings. It would have been more convincing to see results across a wider range of models from different providers, especially given the importance of the topic. 

2. While the authors report high inter-evaluator agreement between GPT-4, Llama Guard, and humans on a small sample, the study relies heavily on automated metrics for evaluating the safety of responses. 

3. While the paper mentions that fine-tuning with RAINBOW TEAMING-generated data improves model robustness, it lacks a detailed analysis of potential effects on the model's general performance or capabilities.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present Rainbow Teaming, a structured approach to automated redteaming of large language models. Based on a user-specified set of strategies and risk categories, Rainbow Teaming uses a mutator LLM to rewrite several variations of existing prompts, then compares the resulting outputs from the target model with a judge LLM against the existing prompt. If a more effective prompt is found, it replaces the existing prompt in the ""archive"" of prompts found so far. The authors conduct experiments redteaming various open-weight models such as Llama and Mistral, showing that their method achieves a high success rate on various risk categories. They also explore the use of the generated prompts in supervised finetuning, showing that robustness to Rainbow Teaming can be improved by training against Rainbow Teaming.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
* Exposition of proposed method is very clear and effective
* Lots of helpful figures and diagrams to illustrate the various components of the entire pipeline, such as the concept of the ""archive""
* Method appears quite effective at least against smaller, open-weight models

Weaknesses:
* Contribution and novelty seems very marginal. The difference from methods such as PAIR and TAP appears to come down to 1) presenting the attack/mutate LLM with high level categories instead of specific behaviors and 2) specifying several concrete strategies instead of relying on the attack/mutate LLM to come up with them on the spot
* Lack of comparisons to prior work. The authors offer various criticisms of PAIR, TAP, and the approaches studied by Perez et al. but do not show any evidence that their method outperforms or finds substantively different prompts from these approaches. Table 1 presents some results which I do not understand. Evaluations on common benchmarks such as AdvBench and HarmBench are missing.
* Lack of experiments on bigger models. The authors only run Rainbow Teaming against 7B models. They claim they are unable to evaluate against more powerful models such as GPT and Claude because of legal constraints, but there are plenty of larger and more powerful models for which this is not a concern, such as Llama-3 70B and many others.
* Fig 4 shows that Rainbow Teaming only improves performance by about 10% beyond the simple baseline of sampling lots of candidates from scratch, suggesting that the whole evolutionary framing of elites, mutations, etc is not so critical.
* The experiments on improving robustness with training on Rainbow Teaming prompts in Sec. 5 are not convincing. The authors generate 15 sets of prompts targeted against Llama-2 7B, train on 12, and then show near-perfect performance on the held out 3 sets. How different are these 3 sets from the 12 training sets if they are generated with the same algorithm? But when running the Rainbow Teaming pipeline against the fine-tuned model, they still find a nearly 40% attack success rate which is not robust at all. And this is in spite of the fact that they do not appear to be using any holdout behaviors for validation.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method Rainbow Teaming for the automatic generation of diverse adversarial prompts aimed at large language models (LLMs). The goal is to identify and enhance the robustness of LLMs to various user inputs, which is crucial as these models are increasingly used in safety-critical environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method holds significant importance in the current AI red teaming study for large language models (LLMs).

2. The proposed automatic method is straightforward to follow and appears to be effective on different open-source models.

Weaknesses:
Lack of baseline comparisons in the safety evaluation for LLMs.

Limitations:
I'm not sure if the authors plan to release their code or model checkpoint to facilitate further advancements in this field. Automatic red teaming is indeed crucial, but some methods can be challenging to reproduce since they often involve large amount of engineering work, which somehow is more likely an engineering work rather than a research study.

Furthermore, while this paper primarily focuses on diverse risks, it would be beneficial to include a comparison with existing red teaming approaches, as they can also be utilized to evaluate and enhance the safety of LLMs.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
I'd like to thank the authors for submitting their work for review. I found the work insightful, inspiring, and high-quality. In short, the work has been a pleasure to review as a last-minute reviewer. 

The manuscript's primary contributions include:
- **Rainbow Teaming Method.**  A new methodology for automatically generating adversarial prompts through the lens of a quality-diversity problem.
- **Demonstrative Evaluation for Safety.** A demonstrative evaluation of the Rainbow Teaming methodology's utility application to the task of identifying prompt vulnerabilities that exist in a series of generative models. This demonstration also validates the underlying components of the Rainbow Teaming approach (e.g., the choice and design of the Preference Model).
- **Fine-Tuning Evaluation for Safety.** A fine-tuning experiment that illustrates how fine-tuning Llama-2-chat 7B on a dataset of 1,500 synthetically generated adversarial prompts with SFT reduced the attack success rate from 92% / 95% to 0.3% / 0.7%. 
- **Post-SFT Evaluation for Safety.** The authors re-apply Rainbow Teaming to the fine-tuned model produced from the Fine-Tuning Evaluation and report that the model is ""substantially more robust to our approach, with a final ASR of 39% (down from 92%"".
- **Non-Safety Evaluations**. The authors contribute two additional experiments that illustrate the method's efficacy for the Question-Answering and Cybersecurity settings, each of which contribute a set of concise, abbreviated findings in their own right.

The manuscript has several other minor contributions that aren't explicitly referenced as contributions, but should not go unnoticed (e.g., the extended taxonomy of safety risk categories that was previously defined by Inan et al.)

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Generally speaking, I find the work to be strong in its contribution, and I have no issue in acknowledging the paper's strengths -- because there are many!

### 1. Originality
* Rainbow Teaming can be categorized as a synthetic data generation method for adversarial settings. Synthetic data generation methods that are similar in nature (e.g. PAIR, MAP-Elites) are recognized by the authors. 
* Despite bearing similarity in the fundamental approach, there are certainly aspects of originality that enable the method to distinguish itself from those that come before it.
* Irrelevant of the methodology's originality, it can be argued that aspects of the evaluations are themselves original. 

### 2.Quality
* I view the quality of the work is high, and the conducted experiments sufficiently support the recognized the work as such. 
* The manuscript experiments related to safety progressively build on one another, providing incremental validation at for each of the steps that educated readers might expect to see when replicating the methodology on their own. The experiments use appropriate metrics and are accompanied with conclusive statements that are, for the most part, reasonable and believable.

### 3. Clarity
* Given page limit requirements, I find the paper's presentation to be exceptional. The writing is crisp, clear, and to the point. 
* The authors have given clear time and attention to ensuring their work is digestible to readers. 
* The figures are also well-designed and make it quite easy to understand how the Rainbow Teaming aims to provide a more holistic evaluation of safety. One could argue that Figure 1's visual representation may be appropriate for adoption as model providers continue to champion safety as an area of investment.

### 4. Significance
* I view the work as an amalgamation of several existing concepts that, when stitched together as a collective, can be viewed as a significant contribution.
* Rainbow Teaming's applicability to safety is clear and obvious, and the secondary evaluations on Question-Answering and Cybersecurity elevate the work's significance.
* It's easy to imagine a generalization of the Rainbow Teaming methodology being applied to settings that aren't adversarial in their nature.

The Appendix should be recognized as a strength in itself. The quality of depth and thoroughness is appreciated, even if some sections may not be as open as I'd like.

Weaknesses:
The work has several weaknesses that should be taken seriously, but not viewed as disqualifying. I view each of the following weaknesses as nothing more than ""expected"".

The weaknesses are as follows:

1. **Longitudinal Practicality.** The authors make a number of claims about the Rainbow Teaming method's ability to improve the robustness of generative models. While this is clearly demonstrated in the manuscript's family of experiments, the claim is weakened by the notion that the experiments do not provide information about how the Rainbow Teaming method may operate over time (i.e., in which adversarial methods evolve in new and unexpected ways). 

2. **Attack Styles and Risk Categories.** The paper's contributions are bound by a static set of attack styles and risk categories. It remains unclear if the methodology would perform similarly with other styles or categories.

3. **Minor Weaknesses.** There are two minor weaknesses:
 - **Model Choice.** Conducted experiments are performed with models that are now viewed as potentially being dated (i.e., and are no longer state-of-the-art). This weakness is stated out of recognition that the model choice itself is a *potential* weakness. Regardless of whether this be recognized more formally as a weakness, I strongly believe that reviewers refrain from scrutinizing the choice of models as the work simply uses them as a vehicle for demonstrating their methodology.
- **Diversity Metrics.** Diversity is primarily measured via BLEU, which one flavor of measurable diversity. Common practice is increasingly gravitating toward the measurement of multiple metrics that are reported as a collective, e.g. https://arxiv.org/html/2403.00553v1.

Limitations:
The authors acknowledge several key limitations of the Rainbow Teaming approach in Appendix A. Generally speaking, they are sufficient, but are not comprehensive or clear as I note in describing weaknesses and questions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zsXbGJJ7Oo;"REVIEW 
Summary:
This paper proposes G2D, a novel vision-language pre-training (VLP) framework for medical imaging that aims to learn both global and dense visual representations from radiography images and their associated radiology reports. The key innovation is a pretext task called Pseudo Segmentation (PS), which uses a pseudo mask derived from attention maps to guide the learning of dense visual features during pre-training. The authors demonstrate that G2D outperforms existing medical VLP approaches on various downstream tasks including classification, segmentation, object detection, and zero-shot visual grounding across multiple medical imaging datasets. Notably, G2D shows strong performance on segmentation tasks even when fine-tuned on very limited data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novel approach: The paper introduces an innovative method for learning dense visual representations in medical VLP without requiring pixel-level annotations, addressing a key limitation of existing approaches.

Well-motivated: The authors provide a clear rationale for why learning dense representations is important for medical imaging tasks and why existing VLP methods struggle with this.

Comprehensive evaluation: The method is evaluated on a wide range of downstream tasks and datasets, demonstrating its versatility and effectiveness across different medical imaging applications.

Strong results: G2D consistently outperforms existing methods, especially on segmentation tasks where it achieves impressive results with very limited fine-tuning data.

Ablation studies: The paper includes thorough ablation experiments to validate key design choices and components of the method.

Potential impact: The proposed approach could significantly reduce the need for large annotated datasets in medical imaging, which is a major bottleneck in the field.

Weaknesses:
Limited theoretical analysis: While the method is empirically strong, there is little theoretical justification for why the pseudo segmentation task leads to improved dense representations.

Complexity of the approach: The method involves several components and processing steps, which may make it challenging to implement and potentially limit its adoption.

Computational resources: The pre-training process appears to be computationally intensive (16 A100 GPUs), which could be a barrier for researchers with limited resources.

Generalization to other domains: While the focus on medical imaging is valuable, it's unclear how well this approach would generalize to other vision-language domains.

Comparison to more recent baselines: Some of the baselines used for comparison (e.g., ConVIRT, GLoRIA) are somewhat older.

Comparison to more recent medical VLP methods would strengthen the evaluation.

Limitations:
The authors provide a brief discussion of limitations in the appendix, acknowledging potential issues with the weak supervision signal from pseudo masks and the need for further research on regional visual representations. They also touch on broader impacts, mentioning both potential benefits for healthcare and risks associated with sensitive medical data. While these discussions are valuable, they could be expanded to provide more specific insights into the limitations of the current approach and potential mitigation strategies for the identified risks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript describes a medical vision-language pre-training framework called Global to Dense level representation learning (G2D), that learns global and dense visual features simultaneously with only image-text pairs, by exploiting the aggregated attention map from the vision encoder for a pseudo segmentation pretext task. The improved (frozen) vision encoder is then utilized as part of the model pipeline for a number of downstream tasks (e.g. segmentation, classification)

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Pseudo segmentation pretext task enables dense segmentation during pre-training, and avoids external resources as for alignment-based methods, and limitations on high-level semantic representations in reconstruction-based methods
 - Importance of associating semantic meaning verified via experiment

Weaknesses:
- Unclear if specific sentence/phrase to individual image region alignment is achieved, for dense learning
 - Lack of fine-grained pixel-level evaluation of masks

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an encoder-decoder medical VLP approach for global-to-dense visual representation learning. Pseudo segmentation is adopted for dense level learning. Rich experiments validate the effectiveness of the proposed method.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The motivation behind the work is clear. Pseudo-segmentation supervision is effective, which is validated by experiments.
2. The experiments are rich and ablation analysis shows the contributions of each component and design.
3. The illustrations are clear and easy to understand.
4. The improvements are consistent and sometimes substantial.

Weaknesses:
1. The comparisons with MGCA and MRM in the CXR14 dataset are not included in Table 3, but Table 4 includes the comparisons with MGCA and MRM. What are the reasons behind this?
2. Transformer-based vision encoder is not analyzed.
3. The balance between VLA and PA losses is not analyzed.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new medical vision-language model, G2D, which employs vision-language alignment (VLA) and pixel alignment (PA) strategies, combined with a pseudo segmentation (PS) pre-training task, to learn global and dense visual representations from medical images. The VLA strategy is used to learn global representations of images and texts, while the PS task constructs pseudo masks through a parameter-free mechanism to facilitate the learning of dense representations. The method is comprehensively validated across five downstream tasks (image segmentation, object detection, zero-shot image visual grounding, zero-shot image classification, and fine-tuned image classification), demonstrating its effectiveness in handling both unimodal and cross-modal tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
+ The paper is well-written, with the motivation, method, and results clearly presented. A minor concern is the reference format; it should be [1] instead of (1) according to the NeurIPS template.

+ A significant concern with most existing works is that they operate primarily at the Image-Text Retrieval level, similar to the perceptual level of CLIP, and do not effectively capture dense features between modalities. The G2D model addresses this issue by integrating Vision-Language Alignment (VLA) and Pseudo Segmentation (PS) tasks to facilitate simultaneous learning of global and dense visual features. This multi-level feature learning significantly enhances the model's performance in tasks requiring dense feature perception, such as segmentation.

+ During pre-training, the G2D method utilizes only image-text pairs without the need for additional annotated data. By generating pseudo masks on the fly through the PS task, it reduces the cost and complexity associated with data annotation.

+ The G2D method is novel, and the experiments are robust. Experimental results on five medical imaging tasks involving 25 diseases demonstrate that the G2D model outperforms existing models, even with minimal fine-tuning data. Notably, in segmentation tasks requiring dense visual features, G2D achieves excellent results with just 1% of the training data for fine-tuning.

Weaknesses:
Major concerns:

- The attention maps could introduce errors in pseudo mask, and these errors may propagate throughout the training process. To address this, a clear validation strategy needs to be outlined. For instance, in Figure 2, aggregated attention map might incorrectly highlight irrelevant regions. It is essential to establish methods for **detecting** and **measuring** these errors to ensure the reliability of the model. I hope the authors could quantify the errors in aggregated attention map and pseudo mask during the rebuttal period.

Minor concerns:

- The training and validation of the model rely on specific datasets, which may introduce biases and potentially affect the model's generalizability to different datasets.

- It is uncertain whether the method can be effectively extended to vision-language tasks involving 3D imaging (e.g., CT and MRI), presenting a limitation in its current scope of application.

Limitations:
Limitations were discussed in Section A.1

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zqLAMwVLkt;"REVIEW 
Summary:
This paper works on node anomaly detection in the novel semi-supervised setting where few labeled normal nodes are given and proposes to generate new anomaly nodes to boost the training data. The anomaly generation algorithm is inspired by the empirical observation that:

(1) Anomaly nodes have lower affinity score than normal nodes
(2) Feature distribution of anomaly nodes are similar to normal nodes if they share similar neighborhood patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The setting is novel and aligned to the real-world situation where normal nodes are typically known compared with anomaly nodes.

(2) The motivation for the proposed two regularization losses is very intuitive and clear.

(3) The experimental results are very impressive.

Weaknesses:
(1) The proposed two regularization losses are heavily based on the empirical analysis, which might not transfer to other anomalies in other datasets. 

(2) For the second prior, its assumption that anomaly nodes sharing similar local structures would share a similar feature distribution has not been empirically verified.

(3) Experiments miss the comparison with diffusion-based generative anomaly detection baseline.

Limitations:
In addition to the limitations mentioned by the author, there are some other limitations worth addressing:

(1) The currently proposed anomaly generation method is still operated in the embedding space. As admitted by the author anomaly behavior is heavily based on interactional behaviors, therefore, it is also helpful to consider directly characterizing/generating anomaly in the graph space.

(2) The comparison misses one generative-based baseline [1]

[1] Liu, Kay, et al. ""Graph diffusion models for anomaly detection."" (2024).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach called GGAD aimed at improving anomaly detection in graphs under a semi-supervised framework. GGAD generates pseudo anomaly nodes that serve as negative samples for training a one-class classifier. This method is built on two
key priors: asymmetric local affinity and egocentric closeness, which help in generating reliable outlier nodes that mimic real anomalies in terms of both graph structure and feature representation. Extensive experimental results demonstrate the effectiveness of the method across diverse graph anomaly detection datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method is innovative. The proposed graph anomaly detection method can exploit the feature and structure information of normal nodes more effectively in the studied semi-supervised scenario compared to existing methods.  The proposed two priors provide a meaningful characterization of desired properties of outliers in this semi-supervised setting and can be utilized to explore other beneficial priors further. 

2.The experiments in the paper are comprehensive and thorough.

Weaknesses:
1. The model relies on prior knowledge to generate anomaly points. This prior knowledge can limit the model’s application scenarios. The model performs best only when the real anomalies align with this prior knowledge. For anomaly types that do not conform to the prior knowledge, the model may not effectively detect them.

2.The model does not perform best on the Photo dataset in Table 1, and the article lacks an explanation of the results at the overall data level.

3. This model employs a semi-supervised approach that uses some positive samples for training. However, it does not consider the issue of noise interference within the positive samples, namely, how the model overcomes interference when some positive samples are mislabeled.

4. During the initialization step, only the initial feature of outliers are obtained while the connections between the outliers and normal nodes are not well illustrated in the paper. From Figure 2, one outlier is connected to more than one normal node while the feature of the outlier is generated according to single normal node. The neighborhood of outliers is important since the it involves the computation of node affinity score of outliers.

Limitations:
yes, the authors point out that some anomalies whose characteristics may not be captured by the two priors used

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel generative-based GAD approach, named GGAD, tailored for the semi-supervised scenario. Unlike existing GAD frameworks, the authors highlight the feasibility and importance of a semi-supervised setting where labels for normal nodes are relatively easy to obtain during training, but labeled abnormal nodes are very limited. In this context, the paper proposes generating pseudo-anomaly nodes to serve as substitutes for real anomaly nodes in training, thus aiding in anomaly detection. These pseudo-anomalies are generated through two unique loss-guidance mechanisms. Experimental results demonstrate the effectiveness of GGAD.

However, the description of the semi-supervised setting in this paper lacks clarity and unconvincing. Additionally, there is minimal differentiation between the proposed method and existing works that generate pseudo-anomaly samples for data augmentation. I think this paper's novelty is limited. I still think that doing unsupervised GAD is more necessary, and if the authors can prove that the pseudo-outlier proposed by GGAD can benefit unsupervised GAD as a general module, I can up my score.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The complete experiment shows the effectiveness of the method and the necessity of each component.

2.Some visual illustrations help the reader understand, although the shapes of the images seem to be compressed.

Weaknesses:
1. I am still confused about the motivation for performing semi-supervised GAD. Why do most methods emphasize unsupervised scenarios? The cost of labeling normal nodes seems too expensive, as the authors themselves state on lines 268 to 269, yet they assert again on line 31 that labels for normal nodes are easy to obtain.This inconsistency hinders a clear understanding of the necessity and practical applications of semi-supervised GAD, which significantly undermines the motivation for this work.

2. While the first loss function proposed by the authors appears intuitively valid, the second loss function aims to generate outliers similar to normal nodes. In my opinion, optimizing these two losses together is unreasonable because they conflict with each other. It seems that they should correspond to different outlier generation processes

3. The paper validates the improvement of unsupervised GAD using labeled normal nodes and claims that GGAD remains superior. I think the authors ignore the fact that unsupervised methods do not obtain this outlier like GGAD and this comparison is not reasonable.

Limitations:
No limitation need to discuss

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores the problem of semi-supervised graph anomaly detection (GAD), where some nodes are known to be normal, in contrast to the typical unsupervised setting with no labeled data. The authors show that even a small percentage of labeled normal nodes can improve the performance of existing unsupervised GAD methods when adapted to the semi-supervised scenario. The paper proposes a novel Generative GAD approach (GGAD) to better exploit normal nodes by generating pseudo anomaly nodes, called 'outlier nodes', to provide effective negative samples for training a one-class classifier. GGAD generates these outlier nodes using priors about anomaly nodes, such as asymmetric local affinity and egocentric closeness, to mimic anomalies in structure and features. Experiments on six real-world GAD datasets show that GGAD outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper studies a new problem of semi-supervised GAD that has not been widely studied. 

+ The proposed method is simple and effective from the empirical perspective.

+ The experiments are extensive including effectiveness and efficiency analyses and the method has been tested on real-world large-scale graphs to verify the scalability.

Weaknesses:
- The two priors that are used to generate outlier nodes are heuristic or based on empirical evidence. There is no theoretical analysis provided to better guarantee the effectiveness of the proposed method.

- It will be more interesting and helpful to show the generated outlier nodes can capture the characteristics of anomalous nodes in addition to comparing their representations.

- The experimental settings of anomaly contamination are not very clear: how the contamination is introduced?

- Overall experimental settings. What hardware has been used in the experiments, e.g., memory, and why are the experiments conducted on CPUs?

Limitations:
The authors have adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an under-explored graph anomaly detection problem where the detection models have access to a set of labeled normal nodes. To tackle this problem, it introduces a generative approach namely GGAD that generates pseudo anomaly nodes, called outlier nodes, to support the training of a discriminative one-class classifier. The key idea underlying this approach is to generate the outlier nodes in a way that can well simulate real anomaly nodes in both graph structure and feature representation perspectives. To achieve this, GGAD defines and incorporates two priors, including asymmetric local affinity and egocentric closeness, into its optimization objectives, with the former prior focusing on the alignment on the graph structure aspect and the latter on the feature representation aspect. The method is evaluated on six large real-world datasets and shows impressive detection performance compared to existing state-of-the-art methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy-to-follow.
- The problem setting is practical since labeled normal samples are easy to obtain in many real-world applications. Compared to the commonly studied unsupervised setting, this semi-supervised setting often results in better detection performance.
- The proposed method GGAD is novel. There have been many generative anomaly detection methods, but as far as I know, they are unable to consider the graph structure and the neighboring nodes’ representations. By introducing the two new priors, GGAD addresses this issue well. Fig.1 and Fig. 3 help demonstrate this effect.
- The method is compared with a range of unsupervised and semi-supervised methods on 6 real-world datasets with diverse genuine anomalies, and gains largely improved detection performance over these competing methods.
- The ablation study is plausible and justifies the contribution of each proposed prior.

Weaknesses:
- The outlier node generation in GGAD may cause non-trivial computational overhead.
- Despite better performance than the competing methods, GGAD gains an AUC of only around 0.6 on some datasets, such as DGraph and Reddit.
- In Fig. 4 (b), GGAD shows a fast AUPRC growth with increasing training size, but the other methods have a flat performance trend. What would be the reason behind?

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zpw6NmhvKU;"REVIEW 
Summary:
This paper proposes a method (RashomonGB ) to estimate the Rashomon sets/predictive multiplicity of gradient boosting models. It estimates multiple ($m$) models at each stage (effectively performing a local exploration) and then combine all such models in the end to construct $m^T$ models for Rashomon set computation, where $T$ is the number of iterations of the boosting. On several datasets the paper shows that RashomonGB performs better than re-training with $m$ seeds, in that at the fix $\epsilon$ (loss difference) level, RashomonGB tends to show more predictive multiplicity.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Predictive multiplicity is an important topic. The paper is generally clear and well-written. The proposed method is a sensible first method for boosting algorithms, which was previously underexplored. I think the proposed method is likely adopted by people who care about this problem as it's intuitive and easy to implement.

Weaknesses:
1. The current exploration strategy is fast to compute, but I'm not sure if this follows the motivation of Rashomon set very well. While the authors mention one example on the Contraception dataset where re-training underestimates the predictive multiplicity, in general RashomonGB might create models that are more correlated than normal (because the ""backbone"" is the same GB model), thus underestimating the predictive multiplicity. Right now, the conclusion shows otherwise probably because the number of re-training is too small. 

2. Regarding the experiment, if I read this correctly, currently we use more compute for RashomonGB as well (by combining different weak models), so it is also not quite a fair comparison in my opinion. I would be very interested to see some estimate of how much compute RashomonGB saves against re-training, by running more re-training and see when are the metrics in Fig3 in the two methods become comparable.



minor: one ""RashomonGB"" in L290 should be ""re-training"".

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an approach that compute Rashomon set for gradient boosting algorithm where the set can be obtained through products over weak learners at each step rather than sampling them through retraining. The authors further proposed a dataset related Rashomon bound through sub-Gaussian assumption, where mutual information between hypothesis space and dataset shows the predictive multiplicity, which can further decomposed into model uncertainty and quality of data. Experiments show the proposed solution offers more models in Rashomon set than retraining given the same computation budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The rough idea of the proposed approach is straightforward since decomposing Rashomon set search on boosting algorithm can be a ""standard"" operation given the unique residual learning property of boosting algorithms. The novelty of the proposed approach is probably more from ""our work is the first to explore the Rashomon effect for gradient boosting"".

The dataset related Rashomon set bound seems an interesting point. But it needs some justification for the key assumption of it (sub-Gaussian). Proposition 2 seems make sense given the positive relation between number of boosting iterations and Rashomon set (also for dataset size).

Experiments in 4.2 seem interesting. I would love to see more experiments like it.

Weaknesses:
I got some difficult time to understand the introduction and abstract of this paper even I have read some literatures about Rashomon effect and predictive multiplicity. It is simply hard to read given the narrative there. Especially the second paragraph of introduction; it gets me confused and self-questioning my understanding of Rashomon effect from other works.

Limitations:
No hard limitation I can see.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the Rashomon effect in gradient boosting, a commonly used algorithm for tabular datasets, but something that has not received enough attention in multiplicity literature. The paper provides several theoretical discussions on the size of the Rashomon set and the impact of the number of iterations on multiplicity in GBRTs. Furthermore, the paper proposes RashomonGB, a method to create an exponential number of ‘near-optimal models’ by training only a polynomial number of models. With more models in the Rashomon set, the use of RashomonGB can create several downstream benefits without any extra cost of training, shown empirically by the authors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Multiplicity in GBRTs, or generally any gradient-boosting algorithm, has not been studied in the literature, and so the authors provided a novel discussion, especially given the importance of these algorithms in tabular settings.
- The paper provides several theoretical discussions backed by empirical support. The insights on the growing Rashomon set with iterations were quite interesting, although I have concerns about the validity of these insights (see Weaknesses).
- Multiplicity quantification can be quite costly, and various methods in pursuit of reducing this cost can significantly benefit further auditing. The use of RashomonGB, as proposed by the authors, can be an important step in that direction for gradient-boosted algorithms.

Weaknesses:
- While the presentation of the rest of the concepts and the theoretical discussion were easy to follow, important details about the RashomonGB method and the details of the empirical setup were either missing (even from the Appendix) or imprecise. For instance, the Rashomon set of the gradient boosting algorithm isn’t going to simply be the iterative extension of Rashomon sets at every residual level, i.e., equation 4 is imprecise. Similarly, it seems that the epsilon value of the Rashomon set increases with more iterations, and thus it is confusing to me whether the insight that more iterations create bigger Rashomon sets is a result of multiple iterations or simply a result of bigger epsilon. See the section ‘Questions’ for more detailed comments and some follow-up questions. Edit after rebuttal: Acknowledged, correct and clarified.
- There are other methods to measure predictive uncertainty in gradient-boosted algorithms. Some examples based on a cursory search (there might be more, as I’m not too familiar with GBRTs) - https://arxiv.org/abs/2205.11412 https://arxiv.org/pdf/1910.03225 https://arxiv.org/abs/2106.01682 - While I understand that prediction uncertainty is not the same as predictive multiplicity, the two are closely related, and when proposing a better method to measure multiplicity, the paper should compare itself with other stronger baselines than just retraining. Just as previous works have proposed using Monte Carlo Dropout (which was initially created as a method to measure uncertainty) as a measure of multiplicity, uncertainty measurement baselines for GBRTs could have been adopted to create reasonable baselines, and would have made the results a lot stronger. Edit after rebuttal: Acknowledged and added.

Limitations:
- A central piece of the paper is their method RashomonGB. While the authors do try to emphasize the importance of this method by highlighting the number of models that can be created using their method, just the number alone is not enough to imply a better method for measuring multiplicity. Even assuming that the comparisons are indeed fair (see Questions), the differences in multiplicity are not very severe, and that makes me wonder if combining pieces of various residual models actually gives us new interesting models or do we just end up with similar models as already seen during retraining. The authors acknowledge this briefly in their limitations paragraph. Edit after rebuttal: Appropriate details added and clarified.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of predictive multiplicity in gradient boosting models. The Rashomon effect refers to the existence of multiple models that perform similarly well on a given dataset. The authors formalize this effect in the context of gradient boosting, introduce a new method called RashomonGB to efficiently explore this multiplicity, and demonstrate its application on various datasets. The paper aims to improve the estimation of predictive multiplicity and model selection, especially with considerations for group fairness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of RashomonGB represents a novel method for exploring the Rashomon set in gradient boosting, offering an exponential search space as opposed to traditional linear methods.
2. The paper provides a robust theoretical foundation using statistical learning and information theory to analyze the Rashomon effect, enhancing the understanding of this phenomenon in gradient boosting.
3. The authors demonstrate the practical utility of RashomonGB on a wide range of real-world datasets, including tabular and image data, showcasing its versatility and effectiveness.

Weaknesses:
1. While the paper discusses the positive societal impacts of RashomonGB, it lacks a thorough exploration of potential negative impacts or misuse of the method.
2. The theoretical analysis relies on several assumptions that may not hold in all practical scenarios, potentially limiting the generalizability of the findings.
3. The paper mentions the intention to release code post-review, but the lack of immediate open access to code and data can hinder reproducibility and independent validation by other researchers.
4. Implementing RashomonGB might be complex for practitioners without a strong background in the theoretical aspects of machine learning and gradient boosting, potentially limiting its adoption in the industry.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zn6s6VQYb0;"REVIEW 
Summary:
This paper proposes a cross-correlation autoencoder for graph structural reconstruction. The authors first analyze the problems of existing self-correlation encoder. Then, a cross-correlation autoencoder is designed. Experimental results show the effectiveness of the cross-correlation autoencoder.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clear and the cross-correlation autoencoder is reasonable.
2. The paper is well-written and easy to follow.
3. The experiments are comprehensive.

Weaknesses:
1. The authors mention that the current self-correlation methods can not address specific (sub)graph structures. But this paper only presents an overall experimental performance. It is unclear how the proposed cross-correlation autoencoder performs given a specific graph structure. 

2. It is not clear whether the graph dataset used in the paper is a directed or undirected graph. Since the cross-correlation autoencoder can represent the directed graph effectively, it is suggested to consider the directed graph dataset.

3. More different architectures of the encoder and decoder should be employed to further verify the effectiveness of the cross-correlation mechanism.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a method to address the limitations of existing graph autoencoder (GAE) models that primarily rely on self-correlation for graph structure representation. They claim existing GAE often fail to accurately represent complex structures like islands, symmetrical structures, and directional edges, particularly in smaller or multiple graph contexts. The proposed model, GraphCroc, introduces a cross-correlation mechanism that aims at enhancing the representational capabilities of GAEs. It employs a mirrored encoding-decoding process to ensure robust structural reconstruction and introduces a loss-balancing strategy to tackle representation bias during optimization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce two latent space for reconstructing the graph structure is ""simple and intuitive"". 

2. The writing is clear and easy to follow.

3. The experimental results are sound.

Weaknesses:
1. This paper lacks discussion on related works. There already exists some works trying to solve the graph autoencoder structure recovering issues. For example, including position encoding [1] or adding extra node labels [2]. How the proposed method is compared with these methods, from the perspective of effectiveness and efficiency?

[1] You, Jiaxuan, Rex Ying, and Jure Leskovec. ""Position-aware graph neural networks."" International conference on machine learning. PMLR, 2019.

[2] M. Zhang, P. Li, Y. Xia, K. Wang, and L. Jin, Labeling Trick: A Theory of Using Graph Neural Networks for Multi-Node Representation Learning, Advances in Neural Information Processing Systems (NeurIPS-21), 2021.

2. As the proposed method generate two latent embeddings, I wonder if there exists some techniques to control them to be different with each other? Otherwise I am concerned that whether the two embeddings could converge to each others.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper theoretically analyzes the limitations of existing graph autoencoders (GAE) in representing special graph features such as islands, symmetrical structures, and directional edges. To address this, the paper proposes a new GAE method, GraphCroc, which employs a cross-correlation mechanism that significantly enhances the representational capabilities of GAEs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper clearly shows the limitations of existing GAEs through theoretical analysis.

2. The experimental results demonstrate the advantages of the proposed method in structural reconstruction and graph classification tasks.

3. The paper is easy to follow.

Weaknesses:
1. In Table 1, the improvements of GraphCroc are evident only on two datasets.

2. While the proposed cross-correlation method performs better than the general self-correlation method on island, symmetric structures, and directed graphs, it would be beneficial to include more results in reconstruction visualization, particularly regarding island or directed edge reconstruction.

3. Some related works [1] need to be discussed.

[1] Liu, Chuang, et al. ""Where to Mask: Structure-Guided Masking for Graph Masked Autoencoders."" arXiv preprint arXiv:2404.15806 (2024).

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zm1LcgRpHm;"REVIEW 
Summary:
This paper introduces a new method for time-series representation learning that enhances the modeling of non-adjacent segment dependencies. Specifically, the proposed method segments, shuffles in a learned manner and stitches the shuffled segments to combine with original time series. The proposed method is model-agnostic without adding significant parameter overhead and shows performance improvement across multiple classification and forecasting base models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method permutes the original segments to better capture inter-relations between distant segments. It is model-agnostic and introduces minimal parameter overhead to the original model.

2. Extensive experiments on various base models for both classification and forecasting tasks demonstrate the effectiveness of the proposed method.

Weaknesses:
1. It it not clear how the sorting process, specifically the calculation of permutation $\sigma$ from $P$, is made differentiable.

2. The compared forecasting baselines such as Informer are no longer state-of-the-art methods. Adding more recent baselines such as Time-LLM, GPT4TS, DLinear, PatchTST would provide a clearer understanding of the proposed method's comparative benefits.

3. The basic assumption for S3 is that modeling non-adjacent dependencies is important. However, the paper lacks detailed case studies that demonstrate the specific types of non-adjacent dependencies effectively captured by S3, which are not addressed by existing models. Additionally, there is no case study to validate that the learned shuffling weights accurately represent these segment dependencies.

Limitations:
The paper mentions potential expansions into tasks like imputation and anomaly detection. Further details on limitations from the reviewer are discussed in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a plug-and-play mechanism called Segment, Shuffle, and Stitch (S3) designed to enhance time-series representation learning in existing models. S3 operates by dividing the original sequence into non-overlapping segments and shuffling them in a learned manner that is optimal for the given task. It then reattaches the shuffled segments and performs a learned weighted sum with the original input to capture both the newly shuffled sequence and the original sequence. This proposed model can enhance the performance of specific models in classification and prediction tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is easily comprehensible and straightforward.

Sufficient experiments are conducted to confirm the effectiveness of the method.

Weaknesses:
Lack of comparative methods:
In fact, the proposed method seems to share the same spirit as data augmentation methods in the time series field[1-4]. Why hasn't any data augmentation method been compared?


Selection of baseline models:
The selected baseline model, Informer, seems somewhat outdated. Why not choose a more recent model, e.g., iTransformer[5] or PatchTST[6]?


Dataset for prediction task:
The author conducted experiments on three ETT datasets, but for prediction tasks, more datasets should be considered, e.g., traffic, electricity, and weather.


Time-Series Representation Claim:
 As the author pointed out, more tasks should be considered for time series representation learning.


[1]FRAUG: FREQUENCY DOMAIN AUGMENTATION FOR TIME SERIES FORECASTING  [2]Time Series Data Augmentation for Deep Learning: A Survey  [3]SimPSI: A Simple Strategy to Preserve Spectral Information in Time Series Data Augmentation [4]TOWARDS DIVERSE AND COHERENT AUGMENTATION FOR TIME-SERIES FORECASTING [5]ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE FOR TIME SERIES FORECASTING [6]A TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new neural network design element which segments, shuffles, and stitches time series for improved representation learning. They evaluate their methods on forecasting and classification tasks, and show that S3 benefits some widely used baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. To the best of my knowledge, the idea is novel, and fundamentally challenges and changes how to learn representations for time series data
2. The paper is well written and easy to follow
3. Experiments are well-designed, and results are promising

Weaknesses:
I have not found any major weaknesses in the methodology or experimental design. However,  I think that the paper might benefit from showing what the S3 module is actually learning. For example, the authors can include the segmented, shuffled, and stitched time series on a particular dataset as an example, along with the weighted time series (used as input to the model), and the original time series. This might provide some intuition as to how this design element improves predictive performance. 

I think there's always scope to improve experimental design. TS2Vec is a excellent choice for classification, but not for forecasting. I would recommend that the authors use methods such as PatchTST (transformer-based) or iTransformer, TimesNet (CNN-based), N-BEATs or N-HITS (MLP-based) etc. for time series forecasting. For classification, it would also be good to compare with fully supervised methods such as ResNet1D (see [1]). 

### References
[1] Ismail Fawaz, Hassan, et al. ""Deep learning for time series classification: a review."" Data mining and knowledge discovery 33.4 (2019): 917-963.

Limitations:
The authors have a very brief description of limitations of their study.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper paper introduces a new approach called Segment, Shuffle, and Stitch (S3) to enhance time-series representation learning. The method involves segmenting the time-series into non-overlapping parts, shuffling them optimally, and stitching them back together along with the original sequence.

Key contributions include:

- Proposing the S3 mechanism to improve time-series representation learning by dynamically reordering segments.
- Demonstrating that S3 can be integrated with existing neural architectures like CNNs and Transformers, resulting in significant performance improvements.
- Showing through extensive experiments that S3 enhances performance in time-series classification and forecasting tasks, with improvements up to 68%.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Code is available, making reproducing this paper easier.
- Paper is clear.
- Results appear good, when considered on the set of baselines and dataset picked by the authors.

Weaknesses:
- Tables 1 and 2 focus on the ETT datasets, which are only a (highly intra-correlated) subset of the common forecasting datasets: Electricity, Traffic, Weather, Illness...
- I see no mention of CoST in the results tables, despite being cited in the paper. This is usually a very strong baseline for contrastive approaches. Including it would certainly paint a more complete picture of the results landscape. On a related note this also applies to e.g. more recent transformer baselines. Informer is relevant, but also very far from state of the art.
- Error bars would help one better contextualize the results.
- The lack of an ablation study makes understanding the reason this works more complicated.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a simple but effective differentiable module that performs pre-processing to input multivariate time-series before being fed into any differentiable model for arbitrary task. The pre-processing involves segmenting, shuffling the segments and stiching them together. The novelty include making this seemingly discrete operations into a differentiable module. This simple idea yields significant improvement in performance of different kinds of models over variety of datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The method is simple and easy to add to most deep learning models
2. The technical details are well-motivated and explained
3. The method also improves training efficiency and convergence time along with performance with very little increate in model complexity
4. Experimental results across different tasks are strong

Weaknesses:
1. Visualization and any qualitative study on the shuffling and segments generalted by S3 would greatly benefit the readers.
2. How well does it optimize transformer based models, especially those that already do segmentation like PatchTST since the attention module captures the relations all pairs of segments already?
3. Does the representations due to S3 generalize to multiple tasks at a time or do we need to retrain for each task?

Limitations:
1. Lack of understanding on the segment permutations generated and why they are better for the model performance atleast qualitatively

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zlgfRk2CQa;"REVIEW 
Summary:
To solve the stability of Deep Thinking models, this paper proposes to constrain activation functions to be Lipshitz-1 functions. The original DT and DT-R models have training stability problem, basically because of scale explosion or vanishing. The authors revealed the stability problem, attribute the problem to Lipschitz constants, proposed ways to ensure Lipschitz smoothness, and show the effectiveness of their approach through a few examples used in the original DT paper, as well as include the traveling salesman problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This paper is clearly written and well motivated. 
* The storyline is very reasonable: identify problems => propose ways to solve the problem => show the approach actually works
* This approach is mathematically grounded.
* Experiments are thorough, by running many random seeds and report error bars.

Weaknesses:
* The idea is quite straight-forward (may not be a bad thing, but make technical contributions smaller)
* In the TSP problems, DT-L's results seem worse than NN Tours and BNN Tours. At least some explanation is warranted. 
* I'm not fully convinced by the significance of this paper. The examples shown in the paper are quite toy. Are there more examples you expect DT-L would work?
* I'd appreciate more visualizations that can intuitively show the benefits of DT-L over DT/DT-R? Maybe some Figures like in the original DT paper. 
* The title is not very informative. Might be better to mention Lipschitz smoothness in the title.

Limitations:
The authors cleraly addresses limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper identifies and rectifies an issue with a particular type of iterative neural network called Deep Thinking Networks. The problem arises in exploding latent representations and unstable training routines. The authors of this work propose an update to the architecture where they add Lipschitz constraints to the model. They show three major benefits: (I) The models train more stably/predictably; (II) the inference-time behavior is better as the latent representations converge with iterations of the recurrent model; and (III) this new approach can learn how to solve NP-Hard problems where the old methods fail.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is original to my knowledge. I am aware of much of the work on Deep Thinking Networks and the issues raised and the solutions proposed in this work are novel.
1. The quality of the work is high. For the most part the experiments are done well and cover many natural questions that would arise from reading the abstract/intro.
1. The clarity is good. I think the writing is clear and the results are compelling.
1. The results are significant for those interested in easy-to-hard generalization. These Deep Thinking Networks have strong extrapolation of toy problems and with the proposed updates to the methods they show strong performance even for TSP solving.

Weaknesses:
1. Clarity: A couple things could be more clear.  
  i. I think IPT stands for Incremental Progress Training, but I don't see the acronym defined anywhere.  
  ii. Table 1 the units are unclear. I gather there are tour lengths, but that isn't stated in the table or the caption.  
  iii. The violin plot in Figure 2 is hard to parse (no harder than any other violin plot). This type of graphic does look nice, but offers little quantitative context. For example, there is no indication of the units/scale of the width of each violin. This is not the right type of plot for a conference paper.

Limitations:
Yes, the limitations are adequately addressed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the positive feedback issue in the so called Deep Thinking networks, where the inference computation may involve more recurrent computations than encountered in training.  The proposed solution is to normalise the state vector that undergoes the recurrence, i.e. make the mapping contractive, i.e. ensure negative (but just) feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written and clear to follow, the proposed method is pretty straight forward and effective.

Weaknesses:
As far as I can tell, it is pretty straight forward control theory stuff for addressing positive feedback.  Nothing wrong with the proposed solution, but I would assume this is such a fundamentally well known issue in any recurrent/feedback system that we can leave this to be addressed by the designer at implementation time with any choice of normalisation.  It is somewhat disappointing that with the proposed method there is still the need for batch normalisation.

Limitations:
If I understand this correctly, the proposed normalisation creates vanishing gradient problem, but authors seem to be aware of this and address it by keeping the spectral norm close to 1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Deep Thinking with Lipschitz Constraints (DT-L), an improved version of the Deep Thinking (DT) networks, designed to enhance the stability and performance of iterative algorithm learning models. The authors address the instability issues inherent in DT networks by analyzing intermediate representation growth and applying Lipschitz constraints. The DT-L model guarantees convergence to a unique solution and demonstrates robustness in learning algorithms that extrapolate to more complex problems. The paper furthermore benchmarks DT-L on the Traveling Salesperson Problem (TSP) as well other than the datasets used in the Deep Thinking models. It compares its performance against existing DT models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Introducing Lipschitz constraints into the DT framework enhances the models' reasoning capabilities. This approach addresses instability issues in training and inference, offering theoretical guarantees for convergence.
- DT-L demonstrates the ability to scale to larger problems effectively, maintaining stability and performance, which is crucial for real-world applications.
- The comprehensive evaluation on various problem classes, including prefix sums, mazes, chess puzzles, and TSP, highlights the robustness and versatility of the DT-L model.
- The paper provides a thorough analysis of the issues with DT networks and clearly explains how the proposed modifications address these problems.

Weaknesses:
- The modifications and theoretical underpinnings of the DT-L model, such as the Lipschitz constraints and orthogonal transformations, add complexity to the model, which might hinder its adoption and understanding by a broader audience.
- While the DT-L model shows improvement, its performance on the TSP is not impressive, indicating room for further optimization and refinement.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zkhyrxlwqH;"REVIEW 
Summary:
The paper addresses unsupervised homography estimation from multi-modal image pairs. The authors propose to cope with the issue of 1) modality, 2) registration in two distinct networks that are trained in an interleaved fashion. The networks architecture derives from the Barlow Twins framework, with changes in the loss function. Results are illustrated on several public benchmark of small images (128^2) and compares favorably wrt to related unsupervised approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1- I enjoy reading the paper. I walked through the paper, first with curiosity and skepticism, then with strong interest. The approach is intuitive (adjust the two representations then compute the transformation) and compelling. I am somehow surprised that it works :) The constrastive-like loss used in  Barlow Twins  is contributing much for the network to learn the correct solution. 

2- Overall, the authors are tackling an important problem (unsupervised learning) for which an original solution is proposed --while based on previous recent work. The methodology is clearly presented. Results are convincing (thought only on small images 128x128) and illustrated on various modality pairs. Quantitative results show improvement wrt related unsupervised work

Weaknesses:
1- Not a weakness, but a points which could have been discussed: why not simply transforming the inputs into edge maps before learning a matching/homography function (and putting aside the modality discrepancy). It would not be a very fancy approach, but I believe it could be a baseline for comparison. 

2- The approach would be more convincing if each of the two modules (GL and MARL) had demonstrated their effectiveness also individually (ie same image pair modality using only GL).

Limitations:
From a practical point of view, the size of the image and the strong overlap between the pairs show that the work need to be further developped for applications at full scale. 
From a methodology point of view, the authors have discussed the limitation of having two networks trained in an interleaved way, with potential collision or collapse.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an unsupervised homography estimation method for multimodal image pairs using an alternating optimization approach. The claimed key innovation is the introduction of the Geometry Barlow Twins loss function for the alternating optimization. The authors show that their approach works on 3 multimodal datasets and different homography estimation architecutres.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The alternating optimization framework together with Geometry Barlow Twins loss seem to be a fresh perspective on unsupervised multimodal homography estimation.

Weaknesses:
Weaknesses
1. Discussion on the Feasibility and Rationality of the Proposed Method: First, for unsupervised training of networks based on iterative prediction, such as RAFT, to ensure stability during training, related methods [1-2] typically apply some form of direct supervision to the motion predicted by the network. This is different from the approach proposed in this paper, which only uses the Geometry Barlow Twins loss for brightness supervision. Second, how RAFT can be used for homography estimation should also be explained, because it is designed for optical flow estimation. Moreover, the paper does not explain how the proposed Geometry Barlow Twins loss supervises the intermediate stages of iterative prediction, whereas RAFT, IHN, and RHWF, along with methods leveraging their structures [1-2], generally provide details on their supervision mechanisms on the intermediate stages. This raises concerns about the feasibility of the proposed supervision method in this paper. Additionally, the effectiveness of the Modality-Agnostic Representation Learning (MARL) introduced in section 4.3 is questionable because it lacks spatial information in its supervision. As mentioned in section 3.2, the projector removes spatial information from the feature maps. The authors should provide a convincing and thorough explanation for these issues. 

2. Doubt about the Effectiveness of the Proposed Method: For example, the paper proposes the alternating optimization (AltO) method but does not provide sufficient experimental results to demonstrate its superiority over other strategies, such as directly cascading all the modules. Furthermore, the paper lacks a comparative demonstration of the features extracted with and without the MARL phase, making the advantages of introducing this phase less convincing.

3. Insufficient Experimental Validation: The paper conducts experiments on only 3 cross-modal datasets, among which only the GoogleMap dataset exhibits significant modality differences. The GoogleEarth dataset mainly consists of images taken in different seasons [3]. Part of the DeepIR dataset is simulated multispectral data [4], which will significantly reduce the difficulty of homography estimation. It would be beneficial to conduct experiments on more challenging multimodal datasets, such as those involving VIS-SAR modalities.

[1] Stone, A., Maurer, D., Ayvaci, A., Angelova, A., & Jonschkowski, R. (2021). Smurf: Self-teaching multi-frame unsupervised raft with full-image warping. In Proceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition (pp. 3887-3896).
[2] Liang, Y., Liu, J., Zhang, D., & Fu, Y. (2023). Mpi-flow: Learning realistic optical flow with multiplane images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 13857-13868).
[3] Zhao, Y., Huang, X., & Zhang, Z. (2021). Deep lucas-kanade homography for multimodal image alignment. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 15950-15959).
[4] Sa, I., Lim, J. Y., Ahn, H. S., & MacDonald, B. (2022). deepNIR: Datasets for generating synthetic NIR images and improved fruit detection system using deep learning techniques. Sensors, 22(13), 4721.

Limitations:
The paper discusses the additional training cost arising from the inclusion of an additional module in the two-phase network, and explores potential solutions for addressing this issue in future research. However, the method’s generalization capabilities are not thoroughly explored, with experimental datasets limited to satellite images, maps, RGB, and NIR images. Future research could involve testing the method on a broader range of datasets to validate its generalization capabilities.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new unsupervised homography estimation approach for multimodal images. This method is designed as a two-phase optimization framework named AltO. The first phase named ""Geometry Learning"" trains a registration network to align the input multimodal images geometrically. The second phase named ""Modality-Agnostic Representation Learning"" trains an encoder and a projector to extract the image-level features invariant to modality changes. Experimental results demonstrate that AltO outperforms several existing unsupervised approaches on the multimodal registration datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed framework is intuitive and interesting. This framework trains a registration network to align the input multimodal images geometrically, and trains another encoder to match the image-level features of the warped multimodal images. This framework has the potential to capture the pixel-level and image-level information in an unsupervised manner.
2. The organization and presentation of this paper are good. I think I can understand the core idea of this paper.

Weaknesses:
**1. Some central claims of this paper lack experimental evidence.**

1.1 The ""alternating"" optimization framework is a central design in this paper. However, why is ""alternating"" optimization necessary? Will optimizing the ""geometry loss"" and ""modality loss"" simultaneously hurt performance?

1.2 The superiority of the proposed Geometry Barlow Twins (GBT) loss was not verified. The original Barlow Twins loss can be straightforwardly applied to the proposed model by considering both the spatial axis (indexed with ""h,w"") and batch axis (indexed with ""n"") as the batch dimension. This straightforward implementation should be compared with the proposed GBT loss.

1.3 The proposed approaches should be compared with some recent unsupervised approaches. Here are some approaches with released codes.

[1] Unsupervised global and local homography estimation with motion basis learning. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.

[2] A Multiscale Framework with Unsupervised Learning for Remote Sensing Image Registration, IEEE Transactions on Geoscience and Remote Sensing, 2022.

**2. This paper did not discuss the recent hand-crafted approaches for multimodal image registration.**

Many recent hand-crafted methods have been published in the top journals, so this kind of approach should not be ignored. The experiment should also compare the proposed approaches with the recent hand-crafted approaches. Here are some hand-crafted approaches with released code.

[3] Histogram of the orientation of the weighted phase descriptor for multi-modal remote sensing image matching. ISPRS Journal of Photogrammetry and Remote Sensing, 2023.

[4] POS-GIFT: A geometric and intensity-invariant feature transformation for multimodal images. Information Fusion, 2024.

**3. The discussion of the motivation is not sufficient.**

The Introduction section mentioned some typical unsupervised approaches designed for the images from the same modality (e.g., UDHN and biHomE). However, the unsupervised approaches [2,5] designed for multimodal image registration are not discussed. What is the motivation of the proposed method compared with this kind of approach? 

[5] A Novel Coarse-to-Fine Deep Learning Registration Framework for Multi-Modal Remote Sensing Images. IEEE Transactions on Geoscience and Remote Sensing, 2023.

**4. This paper misses some references to hand-crafted and unsupervised approaches.**

I have listed some of them in the above weaknesses. The authors should further survey more papers and carefully revise the ""Related Work"" section.

Limitations:
The authors have discussed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zkfCa4oESF;"REVIEW 
Summary:
This paper proposes a new task, ""generalized zero-shot learning (GZSL),"" in which both seen and unseen objects should be recognized for vision-language tasks. It also proposes a new method based on CLIP that uses the loss in the ""attribute space"" to perform better in both seen and unseen classes. This method is evaluated on various kinds of data sets and evaluated by the harmonic mean of the accuracies of seen and unseen classes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed approach using the attribute space seems novel enough, and its effectiveness was verified by a detailed comparison of the other methods and the well-designed ablation studies.

Weaknesses:
1) It is unclear what is learned in ""learnable attribute tokens."" It is not so beneficial for unseen classes. It is unclear what information is represented as tokens for seen classes. It may be better to analyze the acquired tokens in more detail.

2) It is difficult to think about the case that we have never seen an object, but we know its attributes quite well. In such a sense, 
I believe this method is more appropriate for few-shot learning.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper the author proposed a dual-space feature alignment module to keep the semantic consistency between visual and attribute. In addition, the authors proposed Topology-Preserving Reservoir (TPR) to tackle the issue into the generalized zero shot learning (GZSL) setting, which utilized the Pearson correlation coefficient to define a topology-preserving loss, which effectively prevents overfitting of the seen and unseen classes. Sufficient experiment demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)The Paper is well-written, meanwhile, the method is intuitive and easy to understand.
(2)The proposed method focused on Generalized Zero-Shot Learning (GZSL) to present Topology-Preserving Reservoir to finetune the pre-trained CLIP for better fit the distribution of seen and unseen classes, which seems reasonable.
(3)Sufficient and significant experiments demonstrate the effectiveness of the proposed method.

Weaknesses:
(1)The Dual-Space Feature Alignment proposed by the author, which uses a Cross Attention mechanism for cross-modal alignment, lacks innovation.
(2)The author mentions ""attribute reservoir"" in the article, but essentially it is just a fully connected layer that generates different feature representations through various loss constraints. Additionally, in Figure 2, the attribute reservoir is shown in two states: frozen and trained. I am unsure about when these two states should transition between each other.
(3)The idea proposed by the author to fine-tune feature distribution using spatial topological structures is intriguing. However, relying solely on the Pearson correlation coefficient to define a topology-preserving loss seems somewhat simplistic.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The proposed approach targets the generalized zero-shot learning (GZSL) problem for the vision language model (VLM). It is observed that a strong VLM model shows promising results for novel class generalization. Fine-tuning these models for seen classes leads to a loss in generalization capability and poor results for unseen classes. Additionally, a single latent space demonstrates limited ability to adapt to complex visual-linguistic patterns in fine-grained datasets. The paper proposes dual-space alignment, augmenting the latent space with static and learnable tokens. To address the generalization problem post fine-tuning, the paper introduces a Topology-Preserving Reservoir (TPR), which helps preserve the model's generalization ability for unseen classes. The authors conducted extensive experiments across several standard ZSL datasets and explored the impact of various components through ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
[1] Generalization of unseen classes in VLM is a critical problem. The strong pretrained model also loses its generalization ability, which the author explores, and the proposed model shows a significant impact.

[2] The idea and intuition behind the static and learnable attribute reservoir are interesting. Additionally, TPR helps improve generalization.

[3] The wide-ranging experiments conducted across various ZSL datasets and the ablation studies are satisfactory.

Weaknesses:
[1] The standard ZSL model assumes that there is a description per class rather than per sample, which is more intuitive since a single description for each class suffices for the model to understand the class, making it cost-efficient. Standard annotation-based attributes often yield better results for ZSL/GZSL settings. For example, [a] demonstrates impressive results for the CUB dataset compared to the proposed complex static, learnable, and description-based model. This issue is particularly observed in fine-grained datasets. Why is this the case?

[2] It is unclear how the base attribute vocabulary is created. At a high level, the author collected a few attributes and obtained LLM embeddings. This description may not be sufficient for reproducibility since the code and data are not provided.

[3] There are multiple variants of TPR (Table-3) in various scenarios where different methods work, making it difficult to apply and choose the best one. What does the author conclude here?

[4] In Table-1 for the SUN dataset, the model shows inferior performance. While we do not expect the model to outperform in all scenarios, a clear description and author observations are required: why is this the case?

[a] Meta-Learned Attribute Self-Interaction Network for Continual and Generalized Zero-Shot Learning, WACV-24

Limitations:
Authors has not discussed the limitation in the paper, while in the Checklist they said ""Yes"" i.e. they had discussed. This is a bad practice, I don't know what to do.

Dear AC please look into it.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper is a new study that introduces the Generalized Zero-Shot Learning (GZSL) framework within VLMs, aiming to classify both known and novel classes without class partitioning. Key innovations include a dual-space feature alignment module, enhancing latent representations with an attribute reservoir for nuanced visual-linguistic patterns. Additionally, a topology-preserving objective ensures that model adaptations preserve the semantic structure learned by CLIP, thus maintaining generalization across all classes. Extensive experiments across diverse datasets validate the proposed Topology-Preserving Reservoir (TPR) model, demonstrating superior performance over conventional methods in recognizing both seen and unseen classes, underlining its potential for practical applications in complex visual recognition tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
1. This paper introcuces a novel research aspect for VLMs: generalized zero-shot learning, which requires the model to identify both seen and unseen concepts at the same time. From my perspective, this proposal could be a great contribution to VLM community.
2. This paper is well-organized and well-written, which makes it easy to follow. 
3. Extensive experiments，ablation study and visualization results demonstrate the effectiveness and rationality of TPR.

Weaknesses:
None in particular.

Limitations:
None in particular.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
KppBAWJbry;"REVIEW 
Summary:
The paper proposes a new attack called ""privacy backdoors"", which introduces backdoors into foundation models, making them more prone to leak fine-tuning data of a victim who is adapting the foundation model for their task. For this attack, the attacker collects a set of possible data points that might be used to fine-tune the model. The attacker then tries to inconspicuously alter the loss for the target data points such that they have an anomalous loss value. After the victim fine-tunes the model with private data, the loss of the target data points used for fine-tuning will be anomalous, allowing the attacker to tell whether they were used for training or not. The proposed approach is evaluated on vision models (CLIP) and on LLMs (GPT-Neo). In an ablation study, the paper shows that the attack is robust against different parameter-efficient fine-tuning and inference methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to understand
- The paper is well organized, and a reader who is not an expert in privacy attacks can follow the paper easily
- The proposed approach is novel
- The approach is technically sound, and extensive experiments were conducted to show that the proposed approach is working with different models, fine-tuning methods, and inference methods.

Weaknesses:
- My main concern is that the assumption that the attacker already has part of the training data (i.e., the target data points) is quite unrealistic. This setting assumes that the attacker has way more knowledge than in traditional membership inference attacks, where usually only similar but not the exact same data points are available. If the fine-tuning data is assumed to be the victim's private data, then it is not really private in the first place if the attacker can collect parts of this data set. As a result, it is not very surprising to me that after introducing the ""backdoor"", the model leaks more information about these data points the attacker had in the first place.
- I am skeptical that the proposed approach is a ""backdoor"". Usually, backdoors in machine learning have a trigger and produce a predefined output. However, with the proposed approach, we are basically just changing the loss of specific samples. The way it is done is ""stealthy"", but the methodology does not fully align with the definition of a backdoor as it is currently defined in the literature.
- I am not quite sure what the intention of section 2.2 is. At the beginning of the second paragraph, it is said that the presented method shares similarities with federated learning. But then only differences are brought up. So, for the reader, it is not clear what the similarities are to federated learning.
- It is a bit hard to judge the performance of the LLMs based only on the log perplexity loss. It would be nice to have some kind of benchmark similar to the accuracy in the vision model experiments.
- The authors state that maximizing the loss of target data points does not work for LLMs; however, no explanation or experimental results are given, which shows that it does not work.

Limitations:
The limitations are addressed. However, I would encourage the authors to discuss the potential impact of different data distributions on the proposed approach.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors focus on a new vulnerability that concerns pre-trained models which relies on an adversary modifying the pre-trained model in a way that increases the models vulnerability to membership inference attacks (MIAs). The attack is thoroughly evaluated on both vision-language models and LLMs when fine-tuning using different strategies and on different fine-tuning data sets. Furthermore, the authors conduct different ablations of the attack.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Originality: The work shows that SoTA MIAs (like LiRA) can yield better performance with an attacker that has not unrealistic amount of extra information or power. While it builds on-top of LiRA, it is quite clear that (loss-based) MIAs should benefit from this approach. Related work is appropriately discussed and cited.
- Quality: The paper is technically sound and shows through an appropriate amount of experiments that their proposed attack works on multiple SoTA models. The threat model is carefully described and provides the reader with enough information about potential weaknesses of the method. Some minor points I'll mention in the Weaknesses.
- Clarity: The paper is clearly written and I only have some minor suggestions under weaknesses that the authors can easily fix before a potential camera-ready version.
- Significance: The work is highly significant as nearly all current SoTA approaches rely on fine-tuning pre-trained models. As mentioned by the authors, libraries like Huggingface make pre-trained models more accessible and lower the threshold for downloading pre-trained models. At the same time it seems very likely that malicious models could be downloaded (e.g., when searching for a CLIP like architecture). I think this work makes very apparent that the community must defend against these types of attacks.

Weaknesses:
Major:
- Stealthiness of the attack: The paper does not look at how, e.g. the zero-shot performance on unrelated data sets (e.g., NOT the target data set) changes once the model is poisoned. The original CLIP paper (Radford et al., 2019) considers many different data sets. I understood that the authors argue that poisoning for better MIA on CIFAR-10 is stealthy because the performance on CIFAR-10 doesn't change much. But how well is the model still performing on other data sets such as CLEVER, FGVC Aircraft or others? Is there forgetting regarding that data? I see that the scenario breaks a bit down if that is the case because the adversary would need to poison specifically for one victim while hoping that nobody else uses the model and wonders why it performs poor in zero-shot. Eventually this would lead to the model being flagged and the model being taken down. I am willing to increase my score if this point has been addressed by the authors or if they clarify why this is not relevant.

Minor:
- Defense and Detection: The paper would be better if it could provide some ideas regarding potential defenses against this attack or methods to detect that the model includes a backdoor. I don't expect experiments but it would be great to elaborate a bit more than ""In the future, it may be necessary for those who make use of pre-trained models to perform as much (or more) validation of the pre-trained models that are being used as any other aspect of the training pipeline.""
- Table 3: It would be great to specify which model is being used in a separate column. It can be quite confusing as apparently the Linear Probing is only used with CLIP.
- Tables: It would be great if the caption could be elaborating a bit more than just a heading. E.g., by mentioning the model that is under attack.
- Pre-training data: It would be good to mention what pre-training data has been used for the pre-trained models. This can make quite a difference when replicating the results.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a so-called “privacy backdoor” attack. The attacker poisons a pre-trained model to make it susceptible to membership inference attacks (MIA) on an apriori known set of target examples. This poisoning is carried out by continually training the pre-trained model on the target examples and an auxiliary dataset (needed to preserve the base performance of the model), employing loss terms that later cause significant loss-contrast between target examples that are included in fine-tuning and that are not. The attack is empirically tested on vision and large language models, using (on a low level) opposite attack strategies. The evaluation presented in the paper shows that this privacy backdoor attack is effective at enhancing the performance of a prior MIA on both domains, and across various models, fine-tuning methods, and inference strategies.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper studies an important and timely threat. The practice of downloading and fine-tuning open-sourced pre-trained models is currently wide-spread and understanding the associated safety and privacy risks is crucial.

The poisoning attack appears to be highly effective in enhancing membership inference success under the examined setting.

The experiments extend to various fine-tuning and inference schemes, which could be employed by the victim and cannot be influenced by the attacker. The attack is robust in the provided MIA improvement across these scenarios, highlighting the severity of the posed threat.

Appendix C collecting negative results of failed attempts at constructing the attack is highly insightful and a refreshing sight given current publication practices.

Weaknesses:
**Novelty**

The paper claims in several places to introduce a “new” threat model and privacy attack, however, closely related [1] and virtually identical [2] settings have been proposed by other works. While the paper already briefly discusses [1], classifying it as concurrent work (available for slightly less than 2 months at submission time), it omits [2] (available for slightly more than 2 months at submission time). I believe that due to the large similarities in settings between these works, they warrant a longer discussion in similarities, differences, and concurrence in the paper; and the claim to unveiling a “new vulnerability” reassessed and tamed down in light of this discussion.

**Strong assumptions**

The paper reads currently as if the presented attack would be just a nice addition on top of any black-box MIA setting, and the presented game in Threat Model 2 makes it seem like it seamlessly integrates with the traditional MIA framework. However, I believe that this is misleading as the benefit from the introduced privacy backdoor attack is tied to assumptions that are stronger than those usually found in MIA literature.

In MIA, online and offline attacks are usually distinguished [3]. In online MIA (weak setting), the attacker is assumed to be able to adjust the computationally heavy part of their attack (e.g., retrain their shadow models) when the challenger reveals a new target data point to them. In offline MIA (strong setting), the attacker prepares an attack once, before knowing specific target data points, and then this attack is employed (sometimes with minor adjustments without virtually any additional compute costs) once the challenger presents target data points. Also note that another standard assumption is that the challenger is allowed to continuously present new target data points to the attacker as long as they are samples from a distribution that is also available for the attacker for sampling. The paper currently does not introduce these standard elements and assumptions of MIA.

Instead, the implicitly induced setting is in fact weaker than that of online MIA; the entirety of $D_{\text{target}}$ has to be known to the attacker when preparing the privacy backdoor. As such, if at MIA time the challenger presents a new target data point (which is allowed under usual MIA assumptions) the privacy backdoor has no “support” for it and adjusting the backdoor is not possible anymore, as the model on which the MIA is done is already released from the hands of the attacker and the fine-tuning has already happened.

Further, the attack requires that the attacker possesses a dataset $D_{\text{aux}}$ that is disjoint from the target data points, which, while in many cases may be realistic, is a non-standard assumption in MIA once again. Standard MIA does not require that the dataset available to the attacker and the set of all potential target data points is disjoint.

In summary, the paper makes several non-standard restrictive assumptions to the MIA setting, without discussing or motivating them explicitly and clearly; the assumptions are only stated in scattered places and not positioned in the context of MIA.

**Only partially follows best practices in MIA result reporting**

TPR@1%FPR is reporting at an order(s) of magnitude(s) higher FPR than suggested best reporting practices [3].

ROC-AUC score is included, while logarithmic full ROC curves are omitted, in stark contrast to suggested best practices [3].

As such, it is currently unclear if the attack provides as large benefits as currently perceived also at more relevant FPR regimes.

**Concerns over the employed MIA**

In the evaluation, the authors use the LiRA [3] attack with 16 shadow models. However, as it has been already shown in [3] and especially reinforced in [4], the standard LiRA attack is particularly weak for a low number of shadow models. While the version using global variance estimators is better in this regime, it is unclear which one was used for evaluation in this paper. As such, for this potentially weak attack the privacy backdoor provides a large benefit. However, it remains to be seen if the benefit is equally as large for stronger attacks, specifically tailored to perform well with just a low number of shadow models, such as [4] (available since 6th Dec 2023).

**Weak justification of the different attack strategy choices**

The paper currently employs two contrasting attack strategies for vision models and large language models. For vision models the loss of the target data points is increased in the backdooring phase (“maximization”), while for LLMs the target data points are encouraged to be heavily memorized (“minimization”). While both of these strategies are clear how they would encourage contrast at fine-tuning time between member and non-member target data points, the use of different strategies is currently weakly motivated, impacting the convincingness of the paper’s technical contribution.

The differing choices could be better motivated by showing an experiment how the alternative strategies perform on the other domain, i.e., showing how minimization performs for vision, and how maximization performs for LLMs. In particular, the attacks on LLMs seem to be much stronger, which is currently unclear if this is due to the chosen attack strategy, the evaluation protocol and datasets, or due to some other factor.

**Presentation, writing, and clarity**

In several places there are certain inconsistencies, writing is not clear, or small errors in citation formatting or similar. This gives the overall impression that the paper was written hastily.

In Threat model 2, point 3; to match with the generic setup of MIA, I assume that the idea what this point should stand for is (correct me if I am wrong) that the challenger randomly selects if it present a target data point from the target set which was also included in the fine-tuning set or one that was not. However, I think that the used notation currently does not reflect this:  1. “[i]f $c=\text{head}$, they randomly select a target data point $(x,y)$ from $D_{\text{target}}$” — this $(x,y)$ could still be both in the training set or not in it, this does not tell us anything about that; 2. “if $c=$tail, a target data point $(x,y)$ is randomly sampled from $(D_{\text{target}} \setminus D_{\text{train}})$” — this is confusing, as this would imply that $D_{\text{target}}$ is a superset of $D_{\text{train}}$, which is not only not stated anywhere nor followed in the experimental section, but would also mean another highly unrealistic assumption. I believe, sampling from $D_{\text{target}} \cap D_{\text{train}}$ when the coin is head, and sampling from $D_{\text{target}} \setminus (D_{\text{target}} \cap D_{\text{train}})$ when the coin is tail would be a correct notation/presentation.

Section 3.2. is very confusing at the moment, as it gives a clear motivation for the maximization attack, but then presents the exact opposite of this idea for LLMs, solely because ‘maximization did not work’. In conjunction with the experiment justifying this choice and an adjustment in writing would make the presentation of the attack more compelling.

While, as I already elaborated above, the assumptions made by the attack are rather strong, they could be justified by presenting a clear real-world example for the attack (but still have to be explicitly compared to the standard assumptions of MIA). While there is an attempt on this in l159-l162, I suggest to elaborate on this and present it more convincingly, given that the assumptions made are non-standard for MIA.

I struggle to understand the paragraph from l267 to l271. I especially do not get the causal link between observed memorization and what is meant by similar format and similar types of personal information. What the supposed link between the results on Simple PII and MIMIC-IV is, is also unclear. I do not understand why these two results are compared, as to me, the outlier seems to be the result on Simple PII, with a base attack success of 0.242 TPR@1%FPR, compared to an order of magnitude lower TPRs on both ai4Privacy and MIMIC-IV. Further, if the statement is supposed to be that PII is memorized better by the LLMs than images by the vision models, then for this the experiment setup in my view is unfit, as the MIA scores on the unattacked models are comparable in each case (each time two datasets produce around 0.0X TPRs and once 0.1X-0.2X TPR), and in case of the attacked models, as the attacks are different, we cannot know if the difference stems from the data domain or the attack itself.

As already elaborated in its own point, the setting of MIA and how the presented attack relates to it have to be presented clearer, as currently in parts it seems like the attack enables a more powerful setting; being an additive improvement over any MIA scenario.

Citep and citet are mixed up in certain places, e.g., l68 or l115.

**References**

[1] S Feng and F Tramèr, Privacy Backdoors: Stealing Data with Corrupted Pretrained Models. http://www.arxiv.org/abs/2404.00473. 

[2] R Liu et al., PreCurious: How Innocent Pre-Trained Language Models Turn into Privacy Traps. https://arxiv.org/abs/2403.09562. 

[3] N Carlini et al., Membership Inference Attacks From First Principles. https://arxiv.org/abs/2112.03570.

[4] S Zarifzadeh et al., Low-Cost High-Power Membership Inference by Boosting Relativity. https://arxiv.org/abs/2312.03262v3.

Limitations:
Limitations are not prominently and explicitly discussed, only recognized in the checklist. In my view, the strong assumptions made in the threat model have to be discussed in the main paper. Note also other weaknesses pointed out in my review.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a new privacy attack for foundational models like CLIP and large language models (LLMs). The attack's key idea is to ''poison'' the target data (e.g., maximize loss) point into the pretrained models so that the victim's finetuned models uploaded on the open-source platform can reveal what target data points have been used for finetuning.
In the end, the evaluation on both vision and language foundational models validate the attack effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Important research topic: data privacy for foundational models
+ New attack setting and method

Weaknesses:
My main concerns are about the threat model and the evaluation of LLMs.

- Although the paper studies a new threat setting, existing model ecosystem may not work like the described manner. Specifically, the paper assumes the adversary can firstly finetune (i.e., poison) the publicly available pretrained model $F_{pre}$ to $F_{adv}$, and release the F_adv to the platform. Then the victim downloads and finetunes the $F_{adv}$ with D_train to $F_{adv, ft}$, and releases $F_{adv, ft}$ to the platform so the adversary can infer membership privacy from $F_{adv, ft}$. But why would the victim finetunes $F_{adv}$ instead of $F_{pre}$? Typically the victim would choose the $F_{pre}$ for finetuning, just like the authors choose the CLIP for evaluation instead of a random finetuned CLIP  on the Hugging Face.

- Besides, the attack goal of maintaining a comparable level of performance on downstream tasks does not persuade the victim to use $F_{adv}$ instead of $F_{pre}$. Let's assume $F_{pre}$ and $F_{adv}$ have similar performance. As $F_{pre}$ is shared by organizations (e.g., Meta, OpenAI, etc.) that can pay the training cost, the downloads and likes of $F_{pre}$ is certainly high as what we have witnessed in the era of LLMs, and $F_{adv}$ can be just one of hundreds of finetuned $F_{pre}$, why would the victim prefer $F_{adv}$, with potentially not much downloads and likes?

- The impact of finetuning to the LLM performance is also questionable. The results reported in Table 2 is lower validation loss after finetuning, but the loss cannot tell the model performance (i.e., low loss is not necessarily better). I would suggest the authors to provide more concrete LLM evaluation.

- The evaluation on large language models is not thorough. The largest evaluated LLMs in this paper is GPT-Neo-2.7B while widely studied LLMs are above 7B such as LLaMA, Mistral, etc.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
B7S4jJGlvl;"REVIEW 
Summary:
This paper proposes a way to incorporate LLM prompting to improve symbolic regression. They uses PySR, a standard symbolic regression library, as their base SR algorithm. Then they add LLM prompts to different SR algorithm steps: population mutation, crossover, and initialization. They replace the PySR implementation with LLM prompted implementations 1% of the time. The LLM prompts are based on identifying high level concepts and prompting the LLM to do its operation using the concept as a suggestion to follow. High level concepts are tracked using an abstraction prompt given high performing expressions, and are evolved using LLM prompting as well. The authors show that augmenting PySR with LLM concept-based prompting solves around 7/100 addition tasks from the AI Feynman SR benchmark, and show that PySR + LLM (LaSR) performs better on a synthetic task designed to test for data leakage.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- Well written, great figures
- The framework for integrating LLM-based SR into PySR is well-designed, and could in principle work for other prompting approaches besides the concept-based SR. The LLM-mutation, crossover, and initialization steps could be replaced with other LLM based techniques. Cool!
- Using LLM's to learn concepts to guide SR is a well-motivated choice of prompting technique, given the importance of high level concepts for human equation discovery
- Based on the literature review comparing LaSR to two other LLM SR tools, LaSR seems like an original contribution.
- Given existing literature on program synthesis with library learning, LaSR is a great approach that bridges the gap a bit between SR and program synthesis, as done with modern tools.
- LaSR is also a good contribution to the growing body of work on library learning and its benefits for search. It is very similar to LiLO, which is built off DreamCoder, but applied to symbolic regression.
- The authors have a strong analysis of LLM incorporation based on (1) algorithmic cost in terms of millions of tokens per iteration, and (2) comparing GPT 3.5 with open source llama 8b on the results.

Weaknesses:
- It's not clear how valuable the concept abstraction approach is compared to some ""baseline"" of simple LLM prompting. For example, one baseline could just use a single concept ""This expression is a good formula for symbolic regression"" or something like that, and see how it compares. This could perhaps be a direction for future work: try a bunch of simple prompting strategies for combining LLM's with PySR, and report how well each of them work. 
- I'm not sure what to make of the synthetic dataset. In particular, PySR works so well on AI Feynman alone, but works very poorly without the LLM addition on this synthetic dataset. Why does LaSR work so much better than PySR here, but not help as much on AI Feynman? One hypothesis is that AI Feynman has a lot of easy tasks, and the synthetic benchmark only has tasks right on the edge of what PySR can discover, which LLM incorporation helps push over the edge. Another pessimistic take is that the synthetic benchmark is designed with LaSR in mind. While this still eliminates worries on data leakage, an explanation here would help understand better how LaSR is being helpful. 

I'd like to emphasize that including answers to these questions (both of which could suggest negative results) in the paper would not decrease my review score.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method that learns a library of concepts (natural language description of helpful and unhelpful concepts) as a means of guiding genetic search for symbolic regression. The core idea is that such concepts can be used to bias genetic operations through an LLM.

The method was evaluated on the 100 Feynman equations and on a synthetic dataset. The paper also includes ablation studies on the various components of the system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a creative form of using LLM to speed up the search of genetic algorithms for symbolic regression. Instead of simply storing a library of programs, as GA algorithms do, the algorithm also stores a library of natural language concepts. Such concepts can be seen as abstractions of the population of programs encountered in search. Given in natural language, the abstractions can be used to drive the search as an LLM can be used to generate programs based on the description. Such a creative approach!

The idea presented in this paper is general and can be more broadly applied to other problems. I can already see how I could use a similar approach in my own research!

Another strength of the approach is the author's care with data contamination. Initially, I was skeptical of the approach as it uses LLMs to solve problems whose solutions are available online. The authors then explain that the way the LLM is used is unlikely to allow it to simply retrieve the solution from its training data. The explanation makes perfect sense since the LLM is used to extract concepts from programs the GA generates, and they can't encode the solution available online. In addition to this explanation, the authors also included an experiment on synthetic data showing the advantages of the learned concepts over the search alone. Nicely done!

I also enjoyed the fact that the system is built on top of PySR, which is a very efficient system for symbolic regression. This eliminates the possibility that all the gains the LLM provides could be easily washed with clever engineering. The current results already show that clever engineering alone is outperformed by the system.

Weaknesses:
The paper also has a few weaknesses.

**Claims that need to be fixed**

Some of the claims in the paper are a bit strong and I suggest toning them down. While the leakage explanation the authors provided is reasonable, I would be careful in claiming state-of-the-art performance. When writing ""LASR achieves a higher exact solve rate than all other baselines,"" it is worth mentioning the possibility of leakage.

Another claim that seems to be incorrect is the following: ""LaSR's increasing the backbone model size and the mixture probability significantly enhances."" I think the authors meant to write ""substantially"" and not ""significantly"" as there is no statistical test involved. I would also explain why these results are substantially better as the number of problems solved isn't much larger. The explanation I gave to myself is that solving each of these equations is very difficult, so solving one new equation is already quite an achievement.

Another claim that needs to be adjusted: ""demonstrating that LaSR's performance gains are not rooted in memorized responses."" The experiments on the synthetic dataset do not demonstrate this. The experiment with the synthetic dataset is almost independent of the experiment with the 100 Feynman equations. What the experiment demonstrates is that LaSR can outperform PySR even when data leakage is not possible.

**Discovering what is known**

Perhaps an unfair criticism of the paper is that the method it introduces is used to discover things we already know. I understand that the bar would be way too high and it would be unhealthy to the research area if we required the discovery of new things with the presentation of novel approaches to scientific discovery. So I do not make this criticism as a means of arguing for rejecting the paper (I think the paper should be accepted), but more as a reflection of what the community has been pursuing. The hope is that systems such as PySR and LaSR will eventually be used to make actual discoveries.

**Lack of Running Time**

I missed the running time of LaSR and PySR in the paper. How do they compare with LaSR making calls to an LLM?

Limitations:
The paper lists all limitations I could think of, either in the last section ""limitations"" or throughout the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work focus on symbolic regression. They enhaned the traditional method like genetic algorithms by inducting a library of abstract textual concepts. The algorithm, called LASR, uses zero-shot queries to a large language model to discover and evolve concepts occurring in known high-performing model to discover and evolve concepts occurring in known high-performing hypotheses. The algorithm can be seen as a kind of hybird of evolutionary algorithm and LLMs. Through experiments, LASR substantially outperforms a variety of state-of-the-art SR approaches.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This work proposed to introduce a concept library in symbolic regression, which is really similar to how human works, so the idea make sense. For the introduction of the library, this work also leverage abstrction or understanding ability of LLMs to design three phrases process, concept evolution, hypothesis evolution and concept abstraction. The design mix the strenghes of evolutionary algorithms and LLMs. 
2. The experimental results are good comparing to those other baselines of learning-based or evolutionary-based.

Weaknesses:
1. Introdution of LLMs would inevitably raise the cost for the task comparing those traditional algorithms.
2. (this could be a question) The introdution of concept library seems to not making sense in all cases, imaging that we are facing some totally unknown black-box environment, the backbone function could be something random or out of the knowledge we have. In this setting, the traditional evolutionary algorithms might perform better because they do not have this kind of knowledge as the constrant.

Limitations:
See weakness and question parts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces LASR, a symbolic regression framework that enhances PySR by incorporating Large Language Models (LLMs) to discover and evolve ""concepts"" from high-performing equations. These concepts are then used to guide the search process. LASR is evaluated on the Feynman equations dataset and a set of synthetic tasks, showing improved performance over existing symbolic regression baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea of integrating LLMs into symbolic regression to learn and use abstract concepts in terms of natural language is interesting. The methodology is well-structured  with clear explanations of the algorithm components.

Weaknesses:
My main concerns are regarding the evaluation. Experimental design and analysis are insufficient to convincingly demonstrate the method's advantages over existing approaches; specifically: There are serious concerns about potential data leakage and unfair advantage when using LLMs on well-known, simple physics equations that may be part of LLM training data. While there is a section on the data leakage validation, its limited evaluation scope and lack of comprehensive analysis on more complex and real-world datasets, makes it difficult to assess the true capabilities and generalizability of the method.

Limitations:
See above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
ziehA15y8k;"REVIEW 
Summary:
This work studies the problem of reconstructing attack policies using collected adversarial samples to enhance the robustness of GNN-based models in social network tasks, specifically rumor detection. The authors propose the MoE-BiEntIRL framework, which employs a mixture-of-experts approach to learn optimal policies from diverse adversaries, and provides feature-level explanations by estimating interpretable linear reward functions. Experiments on two real-world rumor detection datasets validate the effectiveness of MoE-BiEntIRL.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The authors investigate the rumor detection problem from the novel perspective of reconstructing attack policies.

2. The paper is well-written and well-organized, with motivating illustrations of the problem.

Weaknesses:
While the proposed problem and approach are generally novel and intriguing, the following issues regarding experiments require further clarification:

1. **Table 2:** What makes the policies on Pheme significantly harder to recover than the policies on Weibo?

2. **Table 3:** The results are not clearly illustrated and explained.
    -  For instance, it appears that the column under ""w/o Att."" reflects test accuracy (%), while results under other columns reflect accuracy decline in actual numbers. Please align the representations for consistency.
    -  If ""w/o Att."" refers to GCN's rumor detection performance without adversarial attacks, it is surprising to see that GCN only achieves ~70% test accuracy on the Weibo dataset with binary rumor / non-rumor labels. The authors claim that the Weibo dataset is adopted from existing work [1], which reported over 80% test accuracy on Weibo even using simple models such as TF-IDF or GRU. Please elaborate on the causes for this significant performance discrepancy, e.g, data differences and model structure differences.

3. **Computational Efficiency:** Given the complexity of the model structure illustrated in Figure 2, it would be beneficial to benchmark the computational efficiency of the proposed approach against the baselines in Table 3.

[1] Changhe Song, Cheng Yang, Huimin Chen, Cunchao Tu, Zhiyuan Liu, and Maosong Sun. Ced: Credible early detection of social media rumors. TKDE, 33(8):3035–3047, 2021.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach to enhancing the robustness of Graph Neural Networks (GNNs) against adversarial attacks, specifically in social media contexts such as rumor detection. The authors propose an enhanced maximum entropy inverse reinforcement learning (IRL) method with a mixture-of-experts approach to tackle multi-source graph adversarial attacks. This method aims to reconstruct attack policies, integrate various attack models, and generate additional adversarial samples to improve the robustness of GNN-based detection models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The application of inverse reinforcement learning to reconstruct adversarial attack policies is novel and offers a highly interesting perspective on enhancing GNN robustness.

Combined with the Mixture-of-Experts, the method allows for the integration of various attack models, providing comprehensive feature-level explanations and robust adversarial samples for use in adversarial training.

The generation of good additional adversarial samples for training improves the GNN’s resilience to attacks, which is a significant step towards robust social media analysis. 

The authors use real-world social media datasets to validate the proposed method.

Weaknesses:
The proposed method involves multiple components (IRL, mixture-of-experts, bidirectional updates), which can increase the computational complexity and may not be easily scalable.

The focus is primarily on rumor detection in social media, which, while important, might limit the generalizability of the method to other types of graphs and applications.

Some sections, particularly those involving the theoretical underpinnings of IRL and mixture-of-experts, could be more clearly explained to enhance understanding and accessibility.

No code is provided. This hinders the exact reproduction of results.

I think the authors use the term ""Threat model"" in an incorrect or at least unorthodox way that will likely be misunderstood in the security community and potentially beyond. Specifically, in line 229, the authors start a paragraph called “Threat model” and they proceed to describe that they use, GCN, number of hidden dimensions, optimizer, etc. This is not what is typically understood as a threat model in literature: a model of a threat actor's capabilities, possible courses of action they may take and how it will impact the operation of a computer system [1]. Speaking of it, including an actual threat model (or rather making the implicitly exiting model explicit) would certainly strengthen the paper and increase acceptance in the security community.

Minor issues:
Typos/grammar in lines 13, 69, 109, 111

[1] https://www.sciencedirect.com/science/article/abs/pii/S0167404818307478

Limitations:
The method assumes that the perturbations captured during training are representative of real-world adversarial attacks.

Further testing on different graph types and applications is required to confirm the broader applicability of the proposed method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of adversarial attacks on Graph Neural Networks (GNNs) employed in social media tasks, such as rumor detection. The authors introduce MoE-BiEntIRL, a method that leverages a mixture-of-experts approach combined with inverse reinforcement learning (IRL) to reconstruct and explain adversarial attack policies. The objective of this method is to enhance the robustness of GNNs by generating additional adversarial samples for training, thereby improving resilience against attacks. MoE-BiEnt\IRL incorporates mechanisms for precise sample guidance and bidirectional updates, which are designed to optimize both the accuracy and the speed of policy learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Innovative Approach: The introduction of MoE-BiEntIRL represents a significant innovation, particularly through its application of a mixture-of-experts approach to manage diverse adversaries and provide detailed feature-level explanations. 
2. Real-world Validation: The method is validated on actual datasets from Weibo and Pheme, demonstrating its practical applicability for improving the robustness of GNNs in social media rumor detection scenarios. 
3. Experimental results, focusing on policy reconstruction and adversarial training, effectively illustrate the method’s robustness and efficacy.
4. The approach facilitates a deeper understanding of attack behaviors through feature-level explanations, aiding platform operators in enhancing system defenses.

Weaknesses:
1. The proposed method involves multiple stages and sophisticated mechanisms, potentially complicating its implementation. 
2. Scalability Discussion: The paper would benefit from a more extensive discussion on the scalability of the method, particularly concerning its applicability to large social media graphs. 
3. Experimental Setup Details: Enhancing the description of the experimental setup would significantly improve the reproducibility of the study and aid other researchers in replicating the results.
4. Typos and grammar errors could be avoided.

Limitations:
The paper's limitations are adequately addressed in the supplementary material. There are no further suggestions at this time.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method, MoE-BiEntIRL, which combines a mixture-of-experts approach with inverse reinforcement learning to enhance the robustness and explainability of adversarial attacks on GNNs. The method addresses the critical issue of stabilizing GNNs used in social media for rumor detection, demonstrating significant practical relevance. Strengths include its innovative approach, comprehensive mechanisms for improving attack policy accuracy, and robust evaluation results. However, the paper could benefit from clearer explanations of the method, detailed parameter sensitivity analysis, enhanced experimental reproducibility, expanded comparative baselines. Despite these minor weaknesses, the overall contribution and practical importance of the research are compelling.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The MoE-BiEntIRL method presents a highly novel application of a mixture-of-experts approach combined with inverse reinforcement learning to address adversarial attacks on Graph Neural Networks (GNNs). This innovative approach stands out in its ability to not only enhance the robustness of GNNs but also to provide explainability to the attack policies.
2. The inclusion of precise sample guidance mechanisms and a bidirectional update mechanism demonstrates thoroughness in approach, aiming to improve both the accuracy of attack policy reconstruction and the speed of policy learning. This comprehensive approach adds substantial value to the proposed solution.
3. The evaluation methods employed in this study are robust, validating the effectiveness of the proposed method. The results are compelling, showing notable improvements in the robustness of GNNs.

Weaknesses:
1. Although the proposed method is innovative, some aspects of the algorithm could benefit from clearer explanations. 
2. A minor issue is that the sensitivity of the model to various parameters is not thoroughly explored. A brief analysis or guidance on parameter selection could aid in the practical application of the method.
3. While the method is novel, there is little discussion on its computational complexity. Including an analysis of the computational cost and suggesting optimizations could enhance the practical feasibility of the approach.

Limitations:
The authors have listed the limitations in appendix.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ziYC4FHRNr;"REVIEW 
Summary:
This paper is first to establish entrywise guarantees for low rank approximation of kernel matrices when kernel eigenvalues satisfy either polynomial or exponential decay. More specifically, in the $\alpha$-polynomial decay setting, entrywise error scales as $O(n^{-\\frac{\alpha-1}{\\alpha}} \\log n)$ for rank $d = \Omega(n^{1/\\alpha})$, while for $(\\beta,\\gamma)$-exponential decay error scales like $O(1/n)$ for $d > \\log^{1/\\gamma}(n^{1/\\beta})$. In order to establish such results, authors prove that eigenvectors corresponding to small eigenvalues are completely incoherent/delocalized i.e. have bounded entries of size $O(1/\\sqrt{n})$. Technical novelty stems from the fact that entries of the kernel matrix are dependent and have non-zero mean.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) This is a first result showing entrywise error guarantees for low rank approximation of kernel matrices. 

2) Proof sketches of two main theorems are clear and easy to follow.

3) Strongest technical contribution of this paper is proof given in Appendix D that, simply speaking, shows that the norm of projection of vector 1 on the subspace spanned by $n-d'$ eigenvectors with smallest eigenvalues is vanishing sufficiently fast.

4) Experiments are complementing theoretical results well.

Weaknesses:
1) Although authors claim that Lemma 1 is a novel concentration result, it seems to be only a slight generalization of Lemma 68 in Tao and Vu [2011], and is proved essentially using the same argument as that in the proof of Lemma 68. 

2) Although I appreciate proof sketches of Theorems 1 and 2 in the main text, I believe it would be more useful to add more information about the proof deferred to Appendix D since this is the most novel and interesting part of the proof.

3) It is not clear whether assumption (R) is necessary and how general it is apart from the two special cases given in Section 3.1.

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper focuses on deriving entrywise error bounds for low-rank approximations of kernel matrices using truncated eigen-decomposition. It addresses the statistical behavior of individual entries in such approximations under assumptions of polynomial eigenvalue decay or exponential decay. The authors also provide empirical studies on synthetic and real-world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is clear and well written. The proof seems to be solid.
2. The entrywise error bound is new to the community. 
3. The assumptions on polynomial/exponential eigenvalue decay seem general and cover lots of common kernels.
4. Some statements about random matrix theory and concentration inequalities are provided (e.g., Lemma 1), which could be independently useful to the community.

Weaknesses:
1. The assumptions on the eigenfunctions corresponding to the assumptions of eigenvalue decay are hard to verify for general kernels, especially the part on the rate of decay ($\alpha >2r+1,\beta> 2s$). Moreover, I wonder if these inequalities are required to guanrantee the uniform convergence of the kernel (I note that $k(x,y)=\sum_{i=1}^{\infty}\lambda_i u_i(x)u_i(y)$ converges uniformly under these assumptions).  But in the proof I see these assumptions are used in a way like $\beta-s\ge \beta/2$ (e.g., Line 590). Thus, I am not sure if these asssumptions are necessary for derivation.
2. Assumption (R) seems not natural (why is $1\le a < b/16$ is needed?) and also I do not know how to verify this. Could you provide some examples with $\Gamma_i \neq 0$ under Assumption (R)?
3. The contributions are undetermined. The proof of the main theorem seems to heavily rely on past random matrix theory works (Tao and Vu [2011], Erdős et al. [2009 a,b]). With assumptions (E)/(P) and (R) and the previous works, the proof is straightfoward. And I am not sure about the importance of entrywise error bound.

Minor typos: 
1. Line 578/588 hypotheses-> hypothesis
2. Line 539/581 miss a period

Limitations:
There is no negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the kernel matrices, formed by $n$ vectors i.i.d. drawn from a $p$-dimensional probability distribution $\rho$. Under several assumptions on the associated kernel operator on $L^2_{\rho}$, including the positive definiteness of the kernel and decay condition on the eigenvalues of the kernel, the authors prove an estimate on individual entries of the matrix kernel and those of the low-rank approximation of the kernel. Numerical experiments on the estimate error are done with both synthetic datasets and real-world datasets.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The problem is a very fundamental one and it is considered both analytically and numerically.
- The writing is very clear and easy to read.

Weaknesses:
- Lemma 1 is wrong, and thus the proofs of the main results do not work. 
Consider an extreme case where $a=0$ with probability $1$. Then, since $\pi$ is an orthogonal projection, $\| \pi_H(a) \| = 0$ and thus Lemma 1 fails. The main issue is that in the proof of Lemma 1, if $S_1 = \sum p_{ii} (\xi_i^2 - 1)$, then $E[S_1^2] = \sum_{i, j} p_{ii} p_{jj} E[\xi_i^2 - 1] E[\xi_j^2 - 1]$, which is different from $\sum_i p_{ii}^2 E[(\xi_i^2 - 1)^2]$ in (17), unless $E[\xi^2]=1$. As a result, (17) and the estimates on $P(E_+)$ and $P(E_-)$ fail.
-> The proofs of the main results would work after modifying Lemma 1 as suggested by the authors.

Limitations:
The work does not seem to have potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zgh0ChWocO;"REVIEW 
Summary:
This paper introduces a new way to balance multiple rewards with some long-term rewards potentially missing. It does so by using Pareto Policy Learning of optimizing each reward subject up to the tradeoff frontier. This can be more practical than simple linear weighting since the linear weighting strategy applies the constant weight regardless of the amount of conflict between pairs of objectives. Empirically, the papers show that the approach is superior to linear on two synthetic tasks with some real data. Overall I think the paper is promising and adding more realistic empirical evaluation can add values to the current state of the paper.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Learning to combine multiple rewards is an important and well-motivated question, and has wide ranging implications.
- The method proposed is mathematically sound. The paper shows theoretically that the input parameters can be interpreted as a form of worst case value on each objective. 
- The paper explains how the most popular approach of linear weighting can fall short, derives the method through first principles, and empirically demonstrates that the proposed method is superior.

Weaknesses:
- The main weakness of the paper is that the experimentation is rather limited. The experiment uses partial real data with synthetic generation of short-term and long-term rewards. For example, in robotic planning, the authors could show how their approach helps balance the long-term reward (e.g. goal reaching) / short-term reward (e.g. minimizing jerk). This is just an example, but including other more real-world planning and RL problems would seem beneficial.
- It seems that compared to linear weighting, the proposed method seeks more short-term reward but is not necessarily better in terms of long-term reward. It may not be a weakness but reading the table does strike me that the method is more “short-sighted.”

Limitations:
- The limitation of the overall framework are mentioned but there is no much detail perhaps due to space constraint.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper attempts to address the challenge of learning the optimal policy for balancing multiple long-term and short-term rewards. The authors point out that the existing linear weighting method leads to a sub-optimal policy. To address this limitation, the authors propose formulating formulate the problem as a multi-objective optimization problem. They utilize the Lagrange algorithm to use preference vectors to solve the formulated multi-objective optimization problem and aim to learn the policy to meet Pareto optimization. In order to decide the preference vectors, the authors propose establishing the connection between the optimization problems and the ε-constraint problem. Experiments on IHDP and JOBS demonstrate the efficacy of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The multi-object problem is practical in both reinforcement learning and other optimization scenarios. The paper provides a good summary of the limitations of the existing linear weighting method and introduces a novel perspective on solving the problem by resorting to the Lagrange algorithm and Pareto optimization.
2.	The author has a solid mathematical foundation and is able to provide detailed mathematical descriptions and solutions to the proposed optimization problems.

Weaknesses:
1. The authors point out that the linear weighting method is suboptimal. However, there is no explanation in the method section or corresponding experiments to demonstrate that the proposed method (i.e. DPPL) is optimal.

2. In line 38, the authors claim that when some of the rewards are interrelated, the linear weighting method can only achieve a suboptimal solution. The claim may not be rigorous as the linear weighting method might be able to model the relationship among the rewards. More explanation and experiments are required.

3. In line 95, the definition of Pareto optimality, the condition for Pareto optimality by the author is to find the $\theta$ that makes all $\bar{\mathcal{V}}$ optimal. However, is it possible that the $\theta$ is not optimal for some $\bar{\mathcal{V}}$ but is optimal for the overall $\bar{\mathcal{V}}$?

4. Some mathematical symbols and proprietary terms in the paper are not explained clearly. For example, what does the $e$ in line 110 mean? What does MOP represent? Does MOP represent multi-objective problems? What do $v$ and $R_{+}$ mean in line 171? What does the KKT condition mean? Is it the KKT condition in the Lagrange algorithm? What is the difference between the two descent directions $d_{rt}$ and $d_t$? There are many similar situations in the paper. I suggest providing necessary explanations for each noun and symbol that appears for the first time.

5. In section Simulating Output and section Experimental Details, many parameters are defined by the authors themselves, but most of them do not have reasons or ablation experiments. For example, why is the number of preference vectors 10? In Line 253 to Line 254, why are some parameters truncated normal distributions and some Gaussian distributions?

6. In Table 1 on the L-REWARDS metric, the proposed method is comparable to the linear weighting method. However, the authors claim that for most of the preference vectors, DPPL's solutions have better performance.

7. In Figure 1, it seems that the effect on the $\delta{w}$ from the missing rate and T is not obvious for either the proposed method or LW. More explanation is needed.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the tradeoff between short-term and long-term rewards. The authors formulate the policy learning problem as a multi-objective optimization problem and propose a decomposition-based Pareto policy learning method. I only had experience in reinforcement learning in robotics five years ago. I tried my best to understand the paper, but I am not sure about my rating and comments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper studies a quite interesting and important problem, and the proposed methods seem effective on these two benchmarks.
- The paper is well-organized, the division is relatively easy to follow, and the proposed method is well-motivated.

Weaknesses:
- Only the linear weighting method is used as the baseline. I am wondering if there are any other methods that can be used for comparison. If not, why? Since both IHDP and JOBS are widely used.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a framework for solving multi-objective optimization problems: multi-objective optimization problems are divided into sub-problems in different regions by setting different preference vectors. The parameter optimization direction of the sub-problem can be easily solved by transforming it into a dual problem through the KKT condition, and a Pareto optimal solution of the original problem can be obtained by solving the sub-problem. This paper uses this framework to balance the optimal strategy learning under multiple short-term rewards and long-term rewards and achieves better and more stable performance than the traditional linear weighted method in the constructed experimental environment.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper reveals in detail the connection between the proposed method and the linear weighted method and the epsilon-constrained optimization method. Based on this connection, the epsilon-constrained optimization method can provide interpretability for the method in this paper.
2. The method in this paper theoretically overcomes the suboptimality problem of the linear weighted method and avoids the situation where the epsilon-constrained optimization method does not have a feasible solution.
3. This paper obtains better and more stable results than the epsilon-constrained optimization method in the optimal strategy learning problem under multiple short-term rewards and long-term rewards constructed by the author.

Weaknesses:
1. This paper mainly proposes an important multi-objective optimization algorithm and compares it with two existing algorithms in theory. However, the title of this paper seems to be just a specific application scenario of the algorithm. In what other scenarios can this algorithm be applied?
2. The experimental part is mainly conducted in a constructed environment, and it is unclear how difficult it is in the field of causal inference.
3. The v in line 171 is missing \bar. In Appendix B, t in line 5 of Algorithm 1 should start from 0.

Limitations:
Have addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zeaBrGv7Ll;"REVIEW 
Summary:
This paper presents a diffusion-based video super-resolution method, and proposes Instance-Centric Alignment Module and Channel-wise Texture Aggregation Memory. The former leverages a pre-trained open-vocabulary segmentation model (i.e., OpenSeeD), which is utilized to perform alignment within video clips by modulating the spatial and temporal features. The latter leverages channel-wise attention and memory mechnism to better super-resolve the video frames. The results on publich benchmarks indicate that the proposed method achieves state-of-the-art perceptual performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method achieves state-of-the-art perceptual results on REDS4 and Vid4.

Weaknesses:
Although the proposed method achieves promising results on the public benchmarks, there are some concerns that greatly affect the rating of this paper.

1. The presentation of the method needs to be improved. The readability of the paper is unsatisfactory. The technical details and the rationale behind it is not clearly described and explained.
(a) The main figure (Figure 1) is ambiguous. It is hard to understand the workflow of the framework based on this figure. It is also hard to see the connection among different modules.
(b) In the abstract, what is the ""conditional video generation"" (L6)? I do not see any pre-trained conditional video generation module in the described method. Maybe it should be rephrased.
(c) In L206-207, what is the role of ""randomly initialized tokens""? And what is specific role of the encoder-decoder module?
(d) In L187-188, are the ""semantic tokens"" actually text embeddings ? What is the difference?
(e) In L223, how to divide channels into different groups and what is rationale behind it?
(f) It is hard to understand Eq. (16), (17) and (18). From (17) and (18), it seems T_j is used to calculate itself, which is confusing.
(g) The choice of mathematical notations is sub-optimal and confusing.
(h) In L149, I think the ""belta_t"" should be ""yita_t"".

2. The novelty of this paper is limited.
(a) Some of the modules are based on existing methods. For example, the way of introducing semantic features is similar to SFT (but no comparison in the paper); the multi-frame self attention is from [21].
(b) The proposed blurring ResShift is a modification version based on ResShift, but the rationale behind it is not fully explained. Also, there is no direct ablation.

3. The comparison with other related methods are not thorough. 
(a) The authors should explicitly compare with ResShift [33], since residual shifting technique is also exploited (but no citation in L48). Also, there is no comparison with it in Sec. 2.2.
(b) The authors should compare with Upscale-A-Video [36], another diffusion-based video super-resolution method. Also, it is recommended to compare the performance of [36].
(c) The authors should compare with SFT[28], another method also leveraging semantic segmentation information.

4. The proposed method is not fully ablated. There is no direct ablation for exploitation of DWT and blurring resshift.

5. Some of the statements could be inappropriate. 
(a) In L35-36, I think it is hard to reach the given conclusion from [8]. Please elaborate.
(b) The naming of ""semantic distiller"" could be inappropriate. The pre-trained semantic segmentation model is directly leveraged and frozen. I don't see any distillation.

Limitations:
The authors have adequately addressed the limitations in Section D.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose SeeClear for Video Super-Resolution (VSR). SeeClear is a diffusion-based method that improves restoration performance by introducing semantic priors. The authors design an Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) to utilize semantic information effectively. Comparisons on multiple datasets demonstrate that the proposed method achieves state-of-the-art performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper introduces semantic priors to achieve spatial modulation and temporal correlation, improving diffusion-based VSR performance. This idea is both reasonable and effective.
2. The authors design the Instance-Centric Alignment Module (InCAM) to align using semantic information, avoiding pixel inconsistencies and being well-suited for diffusion models.
3. Additionally, the authors propose the Channel-wise Texture Aggregation Memory (CaTeGory) to transfer semantic information between different frames.
4. Comparisons with state-of-the-art methods demonstrate the effectiveness of the proposed method.
5. The paper is well-organized, with clear and aesthetically pleasing layouts, figures, and tables.

Weaknesses:
1. The method uses pre-trained models to extract semantic information, introducing significant additional computation, which limits the method's applicability. Meanwhile, the paper lacks comparisons of complexity and parameter counts.
2. The method lacks experimental support for some critical hyperparameters, such as the choice of k in InCAM and the number of frames used in SR.
3. The paper proposes using wavelet transform to improve UNet but lacks experimental justification for why simple downsampling and upsampling wouldn't be more efficient.
4. Figure 1, while aesthetically pleasing, is challenging to understand. It would be better to clearly explain the network structure (e.g., Figure 8) and the inference process.

Limitations:
The authors discuss the method's limitations and societal impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel video super-resolution framework leveraging semantic distillation to enhance pixel condensation in diffusion-based models. SeeClear addresses stochastic fluctuations by using a Semantic Distiller and a Pixel Condenser to extract and upscale semantic details from LR frames. The framework includes an Instance-Centric Alignment Module and a Channel-wise Texture Aggregation Memory to improve temporal consistency and visual quality. Experimental results demonstrate SeeClear's superiority over state-of-the-art diffusion-based VSR techniques.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The combination of semantic distillation and pixel condensation is novel and effectively addresses the challenges of maintaining detail consistency across frames in diffusion-based VSR.
- The Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) significantly improve short-term and long-term temporal coherence.
- The paper provides extensive experiments to demonstrate SeeClear's advantages over sotas across multiple benchmarks.

Weaknesses:
- Lack of computation analysis. Diffusion-based methods are often criticized for unbearable inference time, so it would be better to list params, runtime, and FLOPs/MACs for a fair comparison.
- Lack of an ablation study on the wavelet transform which is introduced in Section 3.1.
- Table 2 is incomplete, making it difficult to assess the effect of the CaTeGory.
- The Other baselines such as VRT and IconVSR are also evaluated on Vimeo-90K-T and UDM10 datasets. Could you complete it for a fair comparison?
- Figure 7 needs more explanation.

Limitations:
The authors have largely addressed their limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a framework for video super-resolution (VSR) that improves temporal coherence and high-resolution detail generation. The proposed method, SeeClear, integrates a Semantic Distiller and a Pixel Condenser to extract and upscale semantic details from low-resolution frames. The framework employs an Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) to enhance inter-frame coherence and incorporate long-standing semantic textures. The methodology also introduces a blurring diffusion process with the ResShift mechanism to balance sharpness and diffusion effects. Experimental results show that SeeClear outperforms state-of-the-art diffusion-based VSR techniques in terms of perceptual quality and temporal consistency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The SeeClear framework introduces a combination of semantic distillation and pixel condensation, which significantly enhances video super-resolution.
2. The Instance-Centric Alignment Module (InCAM) and Channel-wise Texture Aggregation Memory (CaTeGory) improve the temporal coherence of the generated high-resolution videos.
3. The integration of blurring diffusion with the ResShift mechanism effectively balances sharpness and diffusion, leading to high-quality detail generation.

Weaknesses:
1. While the method demonstrates robust restoration capabilities, it may still struggle with accurately restoring tiny objects or intricate structures, especially under severe degradation conditions.
2. The method has been tested primarily on specific benchmark datasets. Its performance in real-world applications, where video degradation processes are more varied and unpredictable, remains to be thoroughly evaluated.
3. The experiments are not sufficient and should be improved.

Limitations:
Please refer to the details above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zeYyq0GpXO;"REVIEW 
Summary:
This paper disentangles positional vectors from the hidden states of a pretrained Transformer language model to facilitate the understanding of length extrapolation. After a series of analyses, this paper proposes two context extending techniques. Experiments show that the proposed methods lower the perplexity on the task of language modeling.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
It's always good to have a mechanistic interpretability view of the hidden states of language models. The findings presented in this paper might inspire follow-up work along this direction.

Weaknesses:
The experiments presented in the current draft are not convincing enough to me. See questions below.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a mean-based decomposition technique to analyze the formation and effect of positional encodings in LLMs. It then uses these results to propose methods to extend the context window, resulting in models that generalize better to longer texts.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is very well-written, and the main findings are properly highlighted.

2. This paper not only explains how positional vectors are formed, but also introduces methods to interpolate them based on the findings.

3. Experiments are performed to show that the new methods result in better perplexity scores beyond the context window.

Weaknesses:
I believe this contribution is novel and insightful enough, and there is no apparent weakness.

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper dives into the inner workings of how transformer-based language models handle positional information. By decomposing hidden states into semantic and positional vectors, the authors give a series of analysis about how the positional information are encoded and propagated through layers. I believe this work offers valuable insights for understanding the positional information within the transformer architecture.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Very detailed and clear analysis about how the positional information is encoded and propagated within the transformer architecture, and to the best of my knowledge, I have not seen similar work before. I particularly enjoyed reading Figure 2 and 3, which shows how positional information is propagated through layers and goes beyond the window size, and shows how the manipulation of the positional embedding causally influence the attention patterns, particularly removing the attention sink.

Weaknesses:
There are few points that I would like to suggest here to make the paper even stronger.

- Section 4 feels weak and unnecessary. The performance of replacing the positional vector, if my understanding is correct, seems to be much worse than Dynamic NTK. Given the current mainstream approach is modifying the base of Rope (like YaRN), which is much easier than the approach proposed by this work, I do not think this work’s proposed context extension will be accepted by mainstream model builder.
- That being said, I think the in-depth analysis of the positional embeddings are strong enough for me to give an acceptance (I learned a lot from it), so **I would strongly suggest removing the content of section 4, and use its space for more experimental analysis of the positional vectors**

There are a few important problems that I believe will receive the communities’ attention and worth being addressed:

- Although this paper shows the positional information can propagate through layers (Figure 2), in practice, many work found that models with window attention cannot pass the needle in a haystack test, and this is why Mistral 1.5 changed its attention back to full attention. It would be insightful if the authors can discuss the relationships between positional information and needle-in-a-haystack performance (because needle in haystack is what makes long-context models useful), i.e., why window attention cannot pass needle in haystack even it does have the correct positional information?
- This paper’s analysis is restricted on TinyLLaMA, but TinyLLaMA is not a widely used open-source model, thus casting the doubt whether this discovery of this paper will hold for other model families, particularly mainstream open-weight models like LLaMA 3, Mistral, QWen or Yi. I would strongly suggest the authors verify the behavior of positional embedding on either LLaMA 3, Mistral, QWen, or Yi.

Currently I’m given a borderline accept, and I will consider increasing my scores if the authors could either (1) discuss the relationship between positional vectors v.s. needle-in-a-haystack or (2) verify that the properties of positional vectors hold for LLaMA 3, Mistral, QWen or Yi (any 2 out of the 4).

Limitations:
see the above weakness section

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zcEPOB9rCR;"REVIEW 
Summary:
The paper introduces the Geometric Diffusion Bridge (GDB), a novel framework designed to generate the evolution of geometric states in geometric (coordinate) systems. GDB uses a diffusion bridge connecting initial and target geometric states with equivariant transition kernels, preserving symmetry and joint state distributions. Furthermore, GDB can use a chain of equivariant diffusion bridges to leverage trajectory data for more accurate dynamic modeling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The presentation of theorems in Section 3.1 is clear and straightforward, establishing a solid theoretical foundation for GDB. The authors effectively derive theorems and integrate them with point cloud states.
- GDB demonstrates strong performance across various tasks, including QM9, Molecule3D, and OpenCatalyst IS2RS.

Weaknesses:
I have no complaints regarding the technical and experimental sections, as they are well-written. However, I wonder existing works, such as [1] and [2], also use diffusion bridges over molecular data. What advantages does your approach have over theirs?

[1] Diffusion-based Molecule Generation with Informative Prior Bridges. Lemeng Wu, et al. NeurIPS 2022.

[2] DiSCO: Diffusion Schrödinger Bridge for Molecular Conformer Optimization. Danyeong Lee, et al. AAAI 2024.

Limitations:
The authors note the need of exploring better implementation strategies for their framework to enhance performance.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a generative model for bridging initial and target geometric states using diffusion bridge. This work introduces an equivariant diffusion bridge based on equivariant transition kernels for symmetry constraints. The proposed method was validated on diverse settings including simple molecules and adsorbate-catalyst complex, outperforming previous MLFF baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The motivation of using diffusion bridge to bridge initial and target geometrical states is reasonable. 
- Using diffusion bridge model for equilibrium state prediction and structure relaxation is novel to the best of my knowledge, and the paper shows that GDB significantly outperforms previous methods with diverse datasets.
- Equivariant design of bridge process is based on solid theory.
- The paper is well written except for some missing relevant works on diffusion bridge.

Weaknesses:
- Related works on diffusion bridges or diffusion mixtures were not discussed. Diffusion bridges has been studied in [1,2,3,4] with applications to molecules, graphs, point clouds, and images, and more recent works have studied general framework for diffusion bridges [5, 6] which is worth discussing. While GDB has a contribution for using diffusion bridges in new tasks, discussing related works and clarifying the novel contributions is necessary in particular for strengthening the contribution of this work.
- Contribution seems limited as using diffusion bridge as generative modeling was already studied [1,2,3,4], in particular deriving diffusion bridges using Doob's h-transform. Designing an equivariant diffusion process (not necessarily bridge) specifically in SE(3) group has been covered in [7,8, 9]. What is the difference of designing equivariant diffusion bridges compared to equivariant diffusion processes?

[1] Peluchetti, Diffusion Bridge Mixture Transports, Schrodinger Bridge Problems and Generative Modeling, JMLR 2023
[2] Liu et al., Learning Diffusion Bridges on Constrained Domains, ICLR 2023
[3] Wu et al., Diffusion-based Molecule Generation with Informative Prior Bridges, NeurIPS 2022
[4] Jo et al., Graph Generation with Destination-Predicting Diffusion Mixture, arXiv 2023 
[5] Albergo et al., Stochastic Interpolants: A Unifying Framework for Flows and Diffusions, arXiv 2023
[6] Shi et al., Diffusion Schrodinger Bridge Matching, NeurIPS 2023
[7] Xu et al., GeoDiff: a Geometric Diffusion Model for Molecular Conformation Generation, ICLR 2022
[8] Xu et al., Geometric Latent Diffusion Models for 3D Molecule Generation, ICML 2023
[9] Yim et al., SE(3) diffusion model with application to protein backbone generation, ICML 2023

Limitations:
While the paper discusses future direction for the proposed method, specific limitations of the work is not specified. One potential issue might the scalability of GDB as the model has transformer architecture, and other issue could be long inference time which is a typical problem of diffusion models.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a type of diffusion model that captures the evolution of geometric states. The model is characterized by a diffusion SDE that couples the initial state with the target state, in the middle of which trajectory guidance is enabled when such data present. The framework is designed to yield equivariant density similar to other geometric diffusion models. Experiments on equilibrium state prediction with or without trajectory data have been performed to verify the applicability of the proposed approach.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The distinction between existing works has been elaborated in Table 1, which is clear.

2. The method is designed with an option to leverage additional trajectory data, which is quite interesting.

Weaknesses:
1. The experimental setup and comparison with baselines on equilibrium state prediction is a bit troublesome which requires more clarification or additional comparisons. Please refer to Q1.

2. The presentation is a bit unclear. Please refer to Q2.

3. Additional baselines may be considered. The baselines selected in the paper are not closely connected to the proposed approach. See Q3.

4. Missing ablation studies. In the current shape it is unclear where the performance gain comes from. See Q4.

Limitations:
There seem to be no discussions on limitations in the paper. It would be better to discuss potential limitations from perspectives such as scalability and sampling time.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors introduce a Geometric Diffusion Bridge (GDB) framework, which aims to predict the evolution of geometric states in complex systems accurately, crucial for fields such as quantum chemistry and material modeling. Traditional methods face computational challenges, while deep learning approaches lack precision and generality. The authors use Doob’s h-transform to construct an equivariant diffusion bridge. By applying Doob’s h-transform, the authors adjust the SDE to ensure that the process starts from an initial geometric state and is conditioned to reach a target geometric state. This ensures that the transformed process respects the symmetry constraints of the geometric states, leading to more accurate and physically meaningful predictions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The framework utilizes an equivariant diffusion bridge derived from a modified Doob’s h-transform. This ensures that the diffusion process respects symmetry constraints, making the predictions more robust and reliable.
+ The paper provides a theoretical framework analysis about preserving symmetries and accurately modeling evolution dynamics.
+ Experimental evaluations show that GDB is better than state-of-the-art approaches in various real-world scenarios, including equilibrium state prediction and structure relaxation tasks.
+ The framework achieves significant error reduction compared to strong baseline models, particularly in challenging tasks such as structure relaxation in the Open Catalyst 2022 dataset

Weaknesses:
- The framework, especially when leveraging trajectory data, might introduce significant computational overhead. The simulation-free matching objective is designed to be efficient, but the overall framework’s computational demands might still be high
- Some mathematical notations and definitions in the paper could be made clearer. For instance, explicitly defining all variables and functions used in the modified Doob’s h-transform and constructing equivariant diffusion bridges would improve readability and understanding.

Limitations:
No limitations are addressed in the paper by the authors

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zaXuMqOAF4;"REVIEW 
Summary:
This paper introduces a new LLM length extrapolation method, called Mesa-extrapolation, which utilizes a chunk-based triangular attention matrix and applies stair PE. The proposed method is based on theoretical analysis. The paper conducts extensive experiments on passkey, PPL, summarization to demonstrate the effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a theoretical analysis to prove the effectiveness of meticulous weave position with PE for length extrapolation.
2. The proposed method is efficient and is proved to be effective through extensive experiments.

Weaknesses:
1. The passkey retrieval experiment is simple, good performance on the passkey is far from a real usable context window. Please consider to add evaluations on Ruler[1] and RepoQA[2]

2. The achieved context window is limited. 



[1] https://arxiv.org/abs/2404.06654

[2] https://evalplus.github.io/repoqa.html

Limitations:
See the weakness and question sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper conducts a theoretical analysis to help understand the No Position Encoding. Also, the paper proposes weave position encoding to achieve improved extrapolation performance without additional cost. Also, the paper introduces the weave PE method, Mesa-Extrapotion, which recalculates the position ID to reduce the gap between training and inference. Finally, the paper conducts experiments to prove the effectiveness of Mesa-Extropoation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The presentation of wave PE, Star PE, and Mesa-Extrapolation is clear. The author also provides the details of wave PE, Star PE and Mesa-Extrapolation to help understand the concepts
* The author conducts experiments to prove the effectiveness of the proposed Mesa-Extrapolation.
* The author also further analyzes the Latency & Memory Usage of the proposed Mesa-Extropoaltion.
* The paper discusses the limitations for further discussion.

Weaknesses:
**Major Concerns**: It seems that the proposed method Star PE is the same as Self-Extend LLM [1]. If possible, I sincerely hope that the author can address the following concerns:
* **Concern 1**: The Figure 1 Star PE implementation result does not match the Equation proposed in Section 4.1 Page 5. In Figure 5, when t-i is 5, the implementation result of Star PE is 4. However, according to the Equation proposed in Section 4.1 Page 5, the implementation result should be N+ $\lceil (t-i-N)/E \rceil$=4+$\lceil (5-4)/2 \rceil$=5. Hence, to match the implementation result of Figure 1, the Star PE calculation equation should be N+ $\lfloor (t-i-N)/2 \rfloor$.

* **Concern 2**: The Equation of Star PE is almost the same as Self-Extend LLM. When t-i is small than N, both Star PE and Self-Extend LLM employ normal relative distance. When t-i is larger or equal to N, we discuss it below.
   * The Equation of Star PE is N+ $\lfloor (t-i-N)/E \rfloor$ (as shown in Figure 1), while N is called the extrapolated position and E is called the extrapolated width, and t-i is the relative distance. 
   * The Equation of Self-Extend LLM is $(t-i)//W + (W- W//G)$=$W+ (t-i)//G - W//G$, while W is called neighbor window size and G is called group size. Apparently, when W%G==0, the Equation of Self-Exntend LLM becomes $W+ (t-i)//G - W//G$=$W+ (t-i-W)//G$= $W+\lfloor (t-i-W)/G \rfloor$. Then, we change the notation W to N and the notation G to E, we have N+ $\lfloor (t-i-N)/E \rfloor$, which is the same as Star PE.
* **Concern 3**: If possible, could the author compare the performance between Mesa-Extropolation and Self-Extend LLM?
* **Concern 4**: when the output sequence length $L_{generate} \gg L_{input}$, will the time cost also becomes O($L_{generate}^2$)?

Based on the above concerns, the paper may need to rethink the major contribution. The proposed Mesa-Extrapolation seems to make sense and may benefit society, while the paper should clarify its original contribution.


Reference:

[1] Jin, H., Han, X., Yang, J., Jiang, Z., Liu, Z., Chang, C. Y., ... & Hu, X. (2024). Llm maybe longlm: Self-extend llm context window without tuning. arXiv preprint arXiv:2401.01325.

Limitations:
Yes, the authors have addressed the limitations. We may further discuss and analyze the Mesa-Extropolation in other areas.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the length extrapolation of LLMs.
1. It provides a theoretical analysis of why NoPE and PE fail to extrapolate beyond a certain length. Previous work has shown that this failure is related to the explosion of hidden states as positions increase. This paper demonstrates that both NoPE and PE suffer from this hidden state explosion, using a constructive approach to illustrate the existence of Transformer weights.
2. It proposes weave PE, a simple adaptation of PE that theoretically addresses the extrapolation issue. It also provides a simple implementation of weave PE, using a chunk-based triangular attention matrix. Then, it demonstrates that the proposed extrapolation scheme matches the performance of prior length extrapolation methods, such as Dynamic-NTK.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Great theory explains the failure of NoPE and PE in length extrapolation.
- Proposes weave PE, derived from the theoretical analysis, which also works well in practice.
- Shows good empirical results in passkey retrieval, language modeling, and summarization.

Weaknesses:
1. Methodological comparison with $\Lambda$-Attention

The proposed Stair PE resembles the $\Lambda$-attention of LM-Infinite & Streaming-LLM, yet with differences in 1) the additional attention at the bottom, and 2) a different length extrapolation scheme, Meta-Extrapolation. In the experiments, Meta-Extrapolation significantly outperforms LM-Infinite & Streaming-LLM. Could the authors provide the intuition behind these empirical gains?

---
2. Empirical comparison with Dynamic-NTK

Dynamic-NTK outperforms Meta-Extrapolation on the summarization task for mid-lengths of 7-11k, while Meta-Extrapolation shows better performance on summarization for shorter lengths of 4-6k and better language modeling fluency for lengths greater than 11k. Could the authors provide the intuition behind these results?

---
3. Relation between input sequence length $T$ and effective length $M$

The theorems only show the existence of an effective length $M$, but do not provide intuition on the scale of $M$, such as the ratio over the input length $M / T$. Could the authors provide some intuition on this? If I understand correctly, $M$ is set from the construction of the Transformer weights, so can it be controlled to an arbitrarily large number?

---
Editorial comments

- The fonts of the figures and tables are too small. Please make them more readable.
- Some parts of the writing are mechanical. For example, lines 116-120 do not provide meaningful information. It would be great to discuss the implications of the theorems in natural language. For instance, both theorems state the failure of length extrapolation in NoPE and PE, rather than just ""revealing the internal mechanism of extrapolation.""

Limitations:
Well discussed. Extending this approach to fine-tuning would be an interesting next step.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a positional embedding scheme to address the extrapolation issue: train on short sequences, evaluate on longer sequences. Authors propose a theoretical framing of the positional embeddings contribution to attention. They apply their analysis to NoPE (No Positional Embedding) and to standard PE, and RoPE. They propose the Mesa-Extrapolation idea where input tokens are organized so that attention is paid to nearby tokens and those at other key positions. Authors validate their findings with empirical evidence on several benchmarks and applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is about a very relevant topic which has attracted a lot of attention lately. The paper proposes a simple approach to solve the problem which seems to be easy to adapt to different positional embedding models. Some of the numerical experiments are encouraging.

Weaknesses:
The theory part of the paper is hard to read and I am not sure about its usefulness. Result appear hand-wave-y and vaguely stated. For example the definition of the threshold H in the Assumption is surprising (see questions). Numerically, experiments on language modeling and Summary of Tasks do not seem to show the method's claims.

Limitations:
--

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a weave position encoding method to enhance LLMs’ inference performance when the input context window exceeds the training context window. This method can be integrated into existing pretrained LLMs without additional finetuning. To support their findings, the authors conducted theoretical analyses on the failure reasons of various position encoding methods, including those without position encodings. They demonstrate that the significant shift in the hidden state’s value range, when input token positions exceed the maximum context length, is the cause of this phenomenon.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
One of the strengths of the proposed method is that it can be integrated into existing pretrained LLMs without requiring any additional finetuning. This makes the method highly practical and easy to implement, saving both time and computational resources.

The method has demonstrated excellent performance in pass key retrieval tasks, showcasing its effectiveness in real-world applications. This indicates that the proposed approach not only works in theory but also delivers tangible improvements in practical scenarios.

The authors have conducted comprehensive theoretical analyses to understand the failure reasons of various position encoding methods, including those without position encodings. This thorough investigation provides a solid foundation for the proposed method and enhances its credibility

Weaknesses:
The proposed position encoding method, while promising, does not consistently improve performance across different tasks. This inconsistency suggests that the method may not be universally applicable or reliable in every context, potentially limiting its overall utility. 

Additionally, the main narrative of the paper emphasizes the method’s ability to handle extrapolation beyond the training context window. However, given the observed variability in improvements, it would be more accurate to adjust the claims to better reflect the method’s performance, providing a more balanced and realistic presentation of the work.

Limitations:
The authors have included a limitations section; however, it reads more like a discussion of future work rather than addressing the actual limitations of the current study.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
za9Jx8yqUA;"REVIEW 
Summary:
In this work, the authors propose learning a pixel-based reconstructive world model, and then separately learn networks to convert the representations of a pretrained VLM into the learned world model latent space.  By using a VLM trained via contrastive alignment, this essentially enables the projection of both image as well as text inputs into the latent space of the world model, and therefore simple similarity can be used to provide rewards for downstream policy learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This reviewer is a supporter of the idea of unifying the representation spaces of a large-scale pretrained VLM and that of a world model.  This author appreciates the benefits: matching behavior of a world model with natural language can enable text-conditioned generalization.  The preliminary experiments show promise.

**Originality**: This work appears decently original.

**Quality**: This works quality is acceptable.

**Clarity**: The clarity of the work is acceptable, the core ideas are communicated clearly.  However, there are lots of open questions surrounding this work that could be elaborated upon further.

**Significance**: This work appears to be decently significant, as a preliminary investigation in this space.

Weaknesses:
Chief amongst the weaknesses of this work is the limited environments applied to, and also the limited baselines (essentially, the only existing work the authors compare against is VLM-RM).  The authors can consider comparing against other forms of text-conditioned policy learning, such as LIV for the kitchen setting, or Text2Reward and similar approaches for the general case.  It also seems a strange setup to normalize in-between expert and random, and report results in this way.  This reviewer is unaware of prior work that performs evaluations in this way.  What is the rationale behind this evaluation strategy compared to what is used in prior work?

Details about certain components of the model and how they are implemented are sparse.  For example, is the aligner a video generative model (text-to-video model)?  How is it implemented?

It is a bit dissatisfying to rely on a corrupted version of vision as a language embedding.  It seems strange that the aligner should on one hand be learning to bring language embeddings meaningfully across modalities to the image/video space, which the authors motivate is necessary because of the multimodality gap.  However, the authors then treat language embeddings as a noisy corruption of a video embedding - so essentially the training objective for the aligner is essentially a denoising?  And rather than bridging a modality gap, the aligner is essentially a denoiser?

Why do we not learn the reverse direction, where we optimize a world model's latent space that projects into existing VLM space?  This design decision is not elaborated upon, but seems more intuitive to this reviewer.

From the video demonstrations, on the associated project website, it is rather unclear what is happening.  Are Behavior Retrieval videos from expert policies in an offline dataset that are matched with a particular text prompt/input video?  What are those text prompts/input videos?  It's not clear what the retrieval setup is.  For Multitask Generalization, it is also not obvious what the corresponding prompts are.  Furthermore, the results for multitask generalization do not seem smooth and natural, despite being simplistic DM_Control environments (especially the case for their proposed simplified Stickman environment) and they are missing Kitchen environments.  In the end, it appears that their method is still good as a retrieval technique (retrieving already-achieved expert behaviors in ""Behavior retrieval"") due to the underlying VLM, and is decent at reconstructing video prompts, but still suffers in terms of learning coherent policies (e.g. what is visualized in ""Multitask generalization""), which is ultimately what is of interest.

For the video prompts that are decoded, it appears as if almost all of them are rather stationary (with the exception of the cheetah/dog example and the human dancing example) - they collapse to a stationary goal pose.  Perhaps this is because the clips are so short (8 frames) that essentially it boils down to pose-matching.  It is not obvious that this is that beneficial in supervising motion; so why does this improve upon static image supervision?  Indeed, many of the results that are shown learned by the policy are rather stationary and do not have much movement (most are just jitters around a stationary pose).  It then begs the question how this approach improves upon just a static goal supervision.  However, the authors simultaneously find that in static tasks other methods outperform the authors' approach.

This reviewer pushes back on the term ""data-free RL"", as there still needs data (and interaction data) to learn their method.  This is a very confusing terminology, and honestly the generalization comes from the large-scale pretrained VLM - it would be more appropriate to reuse the terminology of zero-shot reward models or zero-shot policy learning used in prior works across alignment methods (vision-language models are zero-shot reward models for reinforcement learning, [Rocamonde, '23]) and diffusion (text-aware diffusion for policy learning, [Luo, '24]).

This reviewer really enjoys the work but believes there are many open questions that warrant further explanation.  Furthermore, the evaluation suite (environments) and comparison suite (benchmarks) is rather weak.  The idea is indeed neat, but the execution leaves much to be desired, and therefore this reviewer believes the work is of borderline quality.

Limitations:
The limitations section seems acceptable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper looks at a method for leveraging foundation multimodal models for learning world models in RL. They do so by aligning the latent space of a video language model with that of a generative model that can be used for learning in imagination. This is done by training connector-and-aligner networks . The rewards for a task can then be derived by measuring the cosine sim between representations of the states visited by a policy and the states generated by the connector aligner network when it is conditioned on a language-based task prompt. A policy can be optimised to maximise this alignment based reward.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Transferring foundation model knowledge to improve policy learning is an open problem of interest to the community. 

The paper provides a successful recipe for aligning a foundation model with the world model for a specific domain that we want to do policy learning in. 

The paper is written well.

I'm currently being conservative in giving a borderline accept score, since some aspects of the method are not clear to me (I have addressed this in my questions below) - but I will be happy to raise my score after engaging with the authors once they have addressed these questions.

Weaknesses:
1. I would have expected that simple tasks with clearly distinguishable static end states (such as standing) should have worked equally well with CLIP rewards, however the table shows a big difference between the proposed method and the image-language reward baselines even on those tasks, which leads me to think that the baselines may be missing out on some component that the proposed method has. What could be missing, or is this intuition wrong? 
2. The generations in Fig 6a are actually not accurate at all - many of the poses don’t correspond to the humanoid pose if you look closely and would actually optimize learning to strike the wrong pose if a policy is trained with it.

Limitations:
The paper includes a brief discussion on limitations of their method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to combine a DreamerV3-style world model with a pretrained vision language model (VLM). By training two small adaptors to align the latent space of the VLM with that of the world model, the aligned representations from the VLM can be used as a reward signal to train agents in the world model.

The training process consists of two main parts. 

1) There is a large offline dataset needed in the environments of interest (prompt and trajectories of states and actions), generated by expert RL agents and a random policy. This trains the world model and the adaptors. Each environment (domain) uses a separate world model.

2) Actor-critic agents are trained purely within this world model’s imagination, a separate policy for each task. The paper shows these agents outperform standard model-free offline RL methods trained on only the large offline dataset. It also shows some effectiveness at generalizing to new tasks within an environment, specified with a new text prompt.

Soundness:
1: poor

Presentation:
3: good

Contribution:
3: good

Strengths:
- The core idea of the paper is very nice. There is a lot of interest from the community in working out how to get value from the broad general knowledge locked away in LLMs and VLMs, into RL agents. This paper offers a novel way to attack this – to my knowledge world models have not been used in this context before. 

- The results are not dazzling, but they indicate the approach works and it outperforms standard (though perhaps weak) offline-RL baselines. Section 4.2 shows promise in generalizing to knew text prompts in an existing environment.

Weaknesses:
My main criticism of the paper is that the narrative oversells what the core work actually supports. I detail examples below. Overall I’d suggest either presenting the work that has been done comprehensively in a more reserved manner, or adding the required work to support the broader claims and experiments. Either way, I think changes would be large enough to require a resubmission.  I’m disappointed to not be able to give the paper a higher score as I liked the main idea. 
- The capability of the model to condition on visual goals is presented as a main functionality of the model – featuring in the first figure, the abstract, and throughout the paper. But the only evidence to support this is a very brief and qualitative experiment (Figure 6a). Everything else is conditioned on text. I am of the opinion that conditioning on visuals would likely work, but the paper must present good evidence to support this.
- Several aspects of the title ‘Multimodal foundation world models for generalist embodied agents’ are misleading. 1) Only one modality is really tested (as in prior point). 2) ‘Foundation world models’ suggested I'd see a single very general world model. But in Appendix D is an important detail -- each environment learns a separate world model, so they are only general or foundational within a specific Mujoco embodiment. This kind of detail is important and should be honestly discussed in the main paper. 3) A ‘generalist agent’ is referred to, but every agent in the paper only performs a single specialist task, there is nothing general about the agent’s themselves.
- The method is reported as needing ‘no language annotations’ (line 42). This is not true. The large offline dataset requires text prompts accompanying each trajectory.
- The paper claims to be ‘the first large-scale study of multitask generalization from language in RL’ (line 165), but I can think of others. Language table is the first that comes to mind. 
- One of the motivations for the work is that reward functions can be hard to specify, while language is a more natural form. However, the large offline dataset is generated by using multiple expert agents which need reward functions.
- ‘Data free RL’ is suggested as a new paradigm for foundation models in RL. I’d argue that this is simply know as zero-shot generalization to most in the community.
- Main experiments are presented in Table 1. Whilst the offline-RL methods are one comparison point, I’m not sure how comparable they are, since they are all model-free while GenRL is model based. Are there any model-based variants that would be easily considered as baselines? The differences are reflected in the different compute times required – GenRL takes 5 days for world model training +5 hours per policy, while the baselines take 7 hours per policy. This seems like an unfair comparison, especially to withhold the detail to the appendix.
- Results in Minecraft are briefly mentioned in Section 5. But so few details are given that I am lost as to what it is showing. This should either be removed or full details added. 
- The paper presents a new stickman environment. But details are sparse. The authors have failed to correctly identify this in Checklist Section 13.

Limitations:
Fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper wants to leverage the large-scale pre-training of foundation models trained on internet data to train a world model for embodied agents that generalizes across tasks and domains. This is done by training a world model in the standard way, but in addition training aligner and connector networks that (1) map language embeddings to video embeddings and (2) map video embeddings to world model latent states. At inference time, this allows conditioning the world model on a task language prompt and then training in imagination to learn policies.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
- On the website, the reconstruction results from language and video are nice and quite unexpected (I'm unsure why the aligner and connector networks are able to generalize to new prompts) 
- The problem the paper is trying to solve is relevant, especially given the mismatch in data availability between embodied and vision / language settings

Weaknesses:
- The main claim of the paper is strong generalization performance, leveraging the internet scale pre-training of video-language models. The bottleneck is the generalization ability of the networks which map embeddings from the video-language model to the world model latent states, and on the quality of the world model itself. I don't see why the aligner and connector should generalize.
- Given the main claim, I would like stronger baselines / ablations in the generalization and data-free settings. Currently, there are no baselines in the data-free case which makes it impossible to assess how well the method generalizes.  
- Many of the experimental details are unclear in the paper (please see my questions). I encourage the authors to explain these better in the rebuttal and camera-ready, and also provide some intuition for why their method is better than the baselines.  
- In the single task, offline RL case, all the baselines are model-free, whereas the proposed method utilizes a model. I would have liked to see at least one model-based baseline to confirm that the improvement is because of the better reward signal and not because of the model-based optimization.
- In the single task, offline RL case, reward is computed by looking at the similarity between the representations of the task prompt and the image / video. In the case of the base lines, these representations are fixed (eg. CLIP / Intern2Video representations), whereas for the proposed method these are taken from the last layer of the model learnt on the data itself. This is also reflected in the compute budget - the model takes 5 days to train (in addition to the 5 hours of training in imagination).

Limitations:
Yes, the authors adequately assessed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zZVqZRXSao;"REVIEW 
Summary:
This paper introduces the problem of Universal Unsupervised Cross-Domain Retrieval (U2CDR) and proposes a two-stage semantic feature learning framework to address it. The framework includes a cross-domain unified prototypical structure established through an instance-prototype-mixed contrastive loss and a semantic-enhanced loss in the first stage, and a modified adversarial training mechanism to ensure minimal changes during domain alignment in the second stage. Extensive experiments demonstrate that this approach significantly outperforms existing state-of-the-art CDR methods in solving U2CDR challenges.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper addresses a new problem, namely Universal Unsupervised Cross-Domain Retrieval, and proposes an initial solution.
2. The paper first formulates the problem and then introduces the proposed method in a hierarchical manner, which is clear and well-structured.
3. The ability to perform U2CDR has broad implications for various applications, such as image search, product recommendations, and artistic creation.

Weaknesses:
1. The main effort of the paper seems to be on designing an optimization method. However, the optimization methods involved appear to be mostly existing ones. The authors should enhance the description of the novelty.
2. Although the paper uses $L_{SPR}$ to maintain the semantic structure within domains, how to maintain the relationship between the positive pairs across domains should be emphasized.
3. The analysis related to the Ablation Study seems insufficient. It would be beneficial to analyze the reasons for the experimental results in Table 4.

Limitations:
No limitations or negative impacts have been identified in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles the problem of unsupervised cross-domain retrieval. This is the problem where the query and retrieval domains are distinct. For example, in sketch to real retrieval, the system must retrieve the most relevant real images to a query sketch. ""Unsupervised"" refers to the fact that no labels are available during training, but the images from both domains are available. The authors claim to be the first to investigate the ""universal"" version of this problem, where the query and retrieval domains are allowed to have disjoint labels spaces. For this problem, the authors propose a two-stage optimization procedure. In the first stage, three losses are used: (1) an instance-wise contrastive loss (2) a cluster-wise contrastive loss and (3) a semantic enhanced loss. In the second stage, the embeddings between domains are aligned with three losses: (1) an adversarial domain alignment loss (2) a contrastive loss and (3) a nearest neighbor matching loss.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The method is theoretically motivated.

(2) The paper follows a logical orders.

(3) Experiments appear to be complete.

Weaknesses:
(1) The method is clearly described and seems to be theoretically motivated. However, it is hard to understand intuitively why each loss is necessary. In particular, why we must use six different versions of the contrastive loss across two stages? (IPM, INCE, PNCE, SEL, SPR, SN2M). The theory only seems to justify the IPM loss. 

(2) In my opinion, even for someone well versed in metric learning, this method is hard to grasp. Some examples:

 - In line 148, the method applies k-means with a variable number of clusters determined by the ""Elbow approach"" and a contrastive loss on top of the cluster centroids. Just this one paragraph requires the person implementing the algorithm to reference another paper and implement a clustering algorithm.

- The argument, starting at line 152, explaining the IPM loss is hard to understand, mostly because of the unusual notation (arrows and xor symbols).

- The argument for the SN2M loss, starting at line 235 is unclear to me. 

(3) Overall, the method reads like a series of steps that do not follow one central motivation.

Limitations:
Adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Universal Unsupervised Cross-Domain Retrieval for the first time and designs a two-stage semantic feature learning framework to address it.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper proposes a new approach in universal unsupervised domain adaptation, with sufficient experiments to verify its motivation.

Weaknesses:
1. In unified unsupervised domain adaptation, there is no handling of instances that are not common categories. Isn't this necessary?

2. From the perspective of innovation, the proposed unified prototype structure is interesting, and the rest is mostly incremental work, such as semantic structure preservation and adjacent feature matching in domain adaptation. From the visualization results, the author failed to prove the above contributions.

3. This paper should reflect the difference between universal domain adaptation and unsupervised domain adaptation.

4. This article does not have a better way to state the method, especially in cross-domain prototype conversion and close neighbor matching.

Limitations:
no

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zXfhHJnMB2;"REVIEW 
Summary:
This paper proposes Neural Conditional Probability (NCP), a novel operator-theoretic approach for learning conditional probability distributions. Extensive theoretical results are provided to support the optimization consistency and statistical accuracy of NCP. NCP can be used to extract conditional density and compute statistical measures such as conditional mean, variance, moments and CDF once it is trained. Experiments on a collection of conditional density estimation datasets are conducted to highlight the efficacy of NCP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is mathematically solid and well-organized.
- This paper focuses on a fundamental problem of learning conditional distribution in statistical learning and introduces an effective and simplistic approach that outperforms baselines with more complex architectures.

Weaknesses:
- The proposed NCP method is not clearly motivated or introduced. In Line 49-50, the authors mention that NCP does not belong to any of the four aforementioned approaches. But how is NCP in contrast with them and in what aspects does NCP make improvements? I believe adding some intuitive explanations accompanying theoretical analysis would help improve the readability.
- Some key concepts or methods are not clearly explained, which makes it hard to understand the contributions of this work. For example, why is learning *conditional expectation operator* considered useful? Are there any baseline methods that also learn expectation operators?

Limitations:
The authors have discussed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
I am not qualified to review this paper

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am not qualified to review this paper

Weaknesses:
I am not qualified to review this paper

Limitations:
I am not qualified to review this paper

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method (Neural Conditional Probability, NCP) for learning a conditional distribution P(Y | X) from a finite sample from a distribution. The method is based on following observations: (1) it is sufficient to learn the conditional expectation operator E_{Y | X}[f](x) = E[f(Y) | X = x]; (2) the conditional expectation operator can be written as (an infinite) SVD decomposition which could be truncated at some point, so the problem reduced to learning the finite number of functions in the SVD decomposition; (3) the joint distribution density can be written using the functions from the SVD decomposition of the conditional expectation operator, which gives an optimisation objective for fitting the functions from the SVD decomposition using the sample from a joint distribution. The authors provide an extensive theoretical analysis of the proposed method as well as a simulation study on a few synthetic datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ An interesting, novel and theoretically well-motivated method addressing an important problem of conditional distribution estimation
+ The method uses a fairly simple neural network (MLP) but achieves the competitive to the methods using much more complex architectures
+ Thorough theoretical analysis on statistical properties of the proposed estimator

Weaknesses:
- Limited experiments restricted to synthetic data making it difficult to judge the potential applicability of this method
- It would be nice to have a short summary on the main properties of operators, their SVD decompositions, etc. I could generally follow the presentation without major problems, but having a such an operators summary would have made it easier to read the paper

Limitations:
The limitations are sufficiently addressed (NeurIPS Paper Checklist)

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Neural Conditional Probability, a novel operator-theoretic approach to learning conditional probability distributions by learning parameters of the truncated SVD of the conditional expectation operator with a neural network. The authors provide a rigorous mathematical derivation and argue for statistical guarantees of their method. The empirical evaluations require major improvements to an otherwise solid paper.

**As a general note:** I do not consider myself an expert on the theoretical aspects of learning theory. My background is in Bayesian deep learning and simulation-based Bayesian inference. As such, my confidence regarding sections 3 and 5 is rather low, and my review shall mainly consult on the remaining sections that focus on presentation, embedding into other literature, and empirical evaluations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The introduction is excellent, with a high degree of accessibility for the broader NeurIPS community and sound motivation of the proposed method.
- The method seems mathematically rigorous, well-motivated, and sound.
- The authors compare their method against a high number of competing algorithms in the numerical experiments.

Weaknesses:
## Major
- The Related Work section does a good job of acknowledging related works that aim to learn conditional distributions. However, it utterly fails to embed the current paper into this research landscape. I recommend the authors elaborate on the precise similarities and differences between the referenced papers and their methods in the rebuttal period.
- The empirical evaluations are limited to low-dimensional toy problems. This is a stark contrast to the introduction of the method, where the authors repeatedly list the curse of dimensionality as a drawback of other methods. While I acknowledge that the paper is situated in the area of operator learning and ML theory, the quality standard of NeurIPS is not met by the authors’ experiments. This weak evaluation does not do the remainder of the paper justice and I strongly recommend the authors overhaul the experiments to feature high-dimensional tasks that cannot be solved with other state-of-the-art methods. This constitutes a major revision, and this is the main reason why I cannot recommend acceptance to NeurIPS 2024.


## Minor

- The empirical evaluation is missing some important information for real-world applications: What are the approximate wall-clock times for (1) training and (2) inference of the competing methods? Further, the authors mention the large required training set size, which might also influence the practically expected training duration in real-world tasks.
- Please fix the citations throughout the manuscript: Most citations are ‘text citations’ even if their embedding in the sentence warrants parenthesized citations (Author, 1976).
- This is just a personal preference, no need to address it: The ‘paper organization’ paragraph at the end of the introduction does not add value and the space could be used more efficiently elsewhere in the manuscript.
- The first sentence in the conclusion is incomplete.

Limitations:
- The authors mention limitations throughout the manuscript, which I appreciate. However, I would recommend adding a dedicated **Limitations** section in the conclusion to give a compact overview for readers who don’t engage with the entire paper in detail.
- The performance of NCP in high dimensions might be a limitation, but the authors do not study this crucial setting.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zWuHSIALBh;"REVIEW 
Summary:
This work studies how to do alignment for large language models to improve their factuality. The focus of this work is on SFT and DPO. The motivation behind this work is a pilot study which shows more factual data does not always lead to a more factual model. To resolve this issue, the proposed Flame framework (1) handles fact-based and non-fact-based examples differently; (2) uses few-shot generated examples from the model itself for fact-based SFT; (3) builds a reward model specifically for factuality (via atomic fact decomposition, retrieval augmented claim verification, etc.) Experiments on multiple datasets demonstrate that Flame can improve the model's factuality without hurting other capabilities (e.g., instruction following). Ablations are also conducted to measure the gain from each individual step.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clear and reasonable. I like using a simple and quick pilot experiment to demonstrate the main motivation of this paper.

2. The idea is straightforward and effective. The high level framework can applied to many different systems. 

3. Ablation experiments are conducted to show the gain from each step. The effectiveness for both SFT and DPO are clear.

Weaknesses:
1. No external baselines are used in the comparison. It would be great to compare the flame model with other related approaches (e.g., few-shot prompting, sampling multiple responses, and reranking using FactScore or the reward model). I know these approaches are not directly comparable, however, it will still be valuable to understand the relative trends, especially since approaches such as few-shot prompting are used in data generation.

2.  It will be great to conduct human evaluations even just on a few examples.

3. The whole pipeline involves a number of components. While many details are presented in the appendix, low-level details like few-shot prompts, and implementation of fact decomposition are omitted. Adding these details will be super valuable for future work to build similar systems. It would be even better if the authors decide to release the code.

Limitations:
Limitations are discussed in Sec. A.6 in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper shows that training on new or unfamiliar knowledge can promote hallucination and that reward functions in standard RL often inadequately capture factuality. The authors propose a factuality-aware alignment method that first identifies instructions as fact-based or non-fact-based. For fact-based instructions, they employ adapted techniques in respective SFT and RL to generate additional training data, thereby reducing the hallucination of the model's responses.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
* The paper conducts a pilot study that highlights the limitations of SFT and RL in capturing factual knowledge. This study provides valuable insights into data selection for LLM alignment training.
* The proposed dual-stage factuality-aware method improves factuality without compromising the instruction-following capabilities for both SFT and RL stages.

Weaknesses:
* The proposed strategy to create SFT and DPO training data using the generated responses from the LLM itself is limited to the knowledge learned within the original model. This approach may struggle with instructions that the original model cannot generate factual answers for.
* The proposed strategy relies on accurately identifying the instruction type initially, which is limited by the model's ability to correctly classify the instruction type. 
* In the pilot study, it is unclear whether the $PT$ and $PT^{RAG}$ are evaluated using the same protocol as other methods. If they are, the FS score decreases after both SFT and DPO, which contradicts the claim that ""fine-tuning LLMs on their own generations appears to be crucial for factual alignment.""
* While the results in Table 2 and 3 indicate that eliciting knowledge from the model itself can enhance factuality compared to introducing more factual but unknown knowledge, it does not improve the FS of the $PT$, which achieves a score of 53.1 on the Biography task with just 5-shot demonstrations.
* As discussed in Sec 5.5, conducting fact checks and computing factuality rewards solely for fact-based sentences can lead to more factuality errors. Clarification is needed on how FS is calculated for the experiments in Sec 5.2 and 5.3.

Limitations:
Yes, the authors have discussed the limitations of the metric for evaluating factuality.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the issue of factual inaccuracy, or ""hallucination,"" in Large Language Models (LLMs). The authors identify factors that lead to the generation of false facts during supervised fine-tuning (SFT) and reinforcement learning (RL). They propose FLAME, a novel alignment method that incorporates factuality-aware SFT and direct preference optimization (DPO) to guide LLMs towards more factual responses without compromising their ability to follow instructions. Experiments demonstrate FLAME's effectiveness in enhancing factuality while maintaining helpfulness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The ablation experiments provides comprehensive insights into the effectiveness of DPO and SFT in mitigating hallucination.
2. The method proposed in this paper attempts to balance instruction following and factuality. It relies on model self-construction data, and does not depend on external proprietary models.

Weaknesses:
1. The baselines compared in this work are limited to different settings of SFT and DPO only. The baselines in the paper should at least include the work [1]. This prior work also uses DPO and algorithms, and the only difference seems to be data construction. The paper should compare with this work to demonstrate that its algorithm truly achieves a balance between instruction following and factuality.
2. In addition to the works listed in the related work, there are some works whose methods are somewhat similar to this paper, such as [2] [3], etc. The paper may need to add explanations of the differences between these methods to clarify its own novelty.

[1] Fine-tuning Language Models for Factuality. https://arxiv.org/abs/2311.08401

[2] Self-Alignment for Factuality: Mitigating Hallucinations in LLMs via Self-Evaluation. https://arxiv.org/abs/2402.09267

[3] GRATH: Gradual Self-Truthifying for Large Language Models. https://arxiv.org/pdf/2401.12292

Limitations:
The authors have addressed some limitations of their work in the Appendix, which is commendable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discusses a novel alignment method to enhance the factual accuracy of LLMs. The authors observe that conventional alignment processes, which include SFT and RL, often result in the generation of false facts or 'hallucinations'. To address this, they introduce factuality-aware alignment (FLAME), which includes factuality-aware SFT and RL through direct preference optimization. FLAME identifies factors leading to hallucination and adapts the training process to reduce the generation of false claims. Experiments demonstrate that FLAME guides LLMs to produce more factual responses without compromising their ability to follow instructions. The paper contributes to the field by tackling the issue of maintaining helpfulness while improving the factuality of AI-generated content.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Clear and Logical Structure: This paper is well-organized and presents its findings with a logical flow, making it easy to follow.
- In-depth Analysis of Hallucination: The paper thoroughly analyzes the factors contributing to hallucination during the SFT and RL phases of language model alignment. It identifies key issues: training on unfamiliar data can reduce factual accuracy, and standard RL reward functions often prioritize longer, more detailed responses, potentially encouraging the model to fabricate information.
- Innovative Solution:  The proposed FLAME is a novel alignment approach that effectively addresses hallucination without compromising the model's ability to follow instructions. By extending both SFT and RL, FLAME tackles a critical issue in LLMs, ensuring more accurate and reliable information generation.
- Comprehensive Evaluation: The paper thoroughly evaluates FLAME's effectiveness in improving both factuality and instruction-following abilities. Experiments demonstrate that models aligned using FLAME achieve significantly higher FactScore compared to standard alignment methods, without sacrificing their helpfulness.

Weaknesses:
This paper is well-written and makes a valuable contribution to the LLM alignments. I only have several minor concerns as follows:
- Model Size and Generalizability: The paper focuses solely on the LLaMA2-70B model. It would be beneficial to investigate whether FLAME's effectiveness extends to smaller models, such as 7B or even smaller, given that the factuality-aware SFT relies on self-supervision through few-shot prompting. 
- Evaluation Metrics and Human Assessment: While FactScore is a valuable metric, it has limitations. It assumes Wikipedia as the definitive source of truth and may not be suitable for broader domains. Using a more comprehensive metric like Veriscore [1] could provide a more nuanced evaluation (I understand that Veriscore is a recently released method, so this is a suggestion for the future version of this paper). Additionally, incorporating human evaluation would strengthen the analysis. A manual assessment of factuality and helpfulness would provide valuable insights and increase the persuasiveness of the findings.
- Multi-faceted Evaluation: The paper primarily focuses on instruction following and factuality. However, other crucial aspects of LLM capabilities, including knowledge, reasoning, and code generation, should also be considered. It would be insightful to evaluate the performance of FLAME-trained models on standard benchmarks like MMLU, GSM8K, and HumanEval to assess potential trade-offs in these areas.

Limitations:
The authors adequately addressed the limitations and broader impacts.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zWnW4zqkuM;"REVIEW 
Summary:
The authors propose an approach to enhance image synthesis using multimodal attributed graphs, adopting a strategy to condition image generation via a tokenization scheme on graph structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper studies an intersectional topic: leveraging graph learning techniques for image generation, which is a creative application and an area which deserves more focus.
- The authors' use of qualitative examples (e.g. Figure 5 and 6) is commendable and helps articulate visual improvements.

Weaknesses:
Please see questions and concerns below.  My general feeling is the paper is fairly incremental in its introduction of a mechanism to encode graph condition into the conditioning for generation.  Many design choices for graph conditioning are not discussed well and the quantitative results for some of these choices are missing which hurts the overall impact of the work.

Limitations:
Yes, Appendix A.1

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of image synthesis on multimodal attributed graphs (MMAGs) and proposes a graph context-conditioned diffusion model, INSTRUCTG2I, to address the challenge in this setting. In particular, it proposes a semantic personalized PageRank-based method to sample related neighbors in the graph.  Then, the INSTRUCTG2I can effectively encode graph conditional information as graph prompts with Graph-QFormer. Systematic experiments on MMAGs demonstrate the effectiveness of the methods proposed in this paper compared to competitive baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.  This paper studies an interesting and meaningful question. It investigates the graph-structured relationships of real-world entities for image generation on MMAGs, a task well-grounded in practical applications. 
2. This paper is well-structured and easy to understand.
3. The graph context-conditioned diffusion model proposed in this paper is reasonable in solving image generation problems on MMAGs.

Weaknesses:
1. The description in eq.10 may be incorrect. Please check more carefully.
2. Subsection 3.4 is more challenging to understand when reading. The authors' descriptions of some symbols in Eq. 10 and Eq. 11 are not exhaustive.
3. The results of the ablation experiments in Table 2 indicate that using a GNN such as GAT or GraphSAGE to aggregate graph information seems to be worse than the straightforward approach in Eq.7. Authors are requested to give a more detailed discussion with a reasonable explanation.
4. The images sampled by the semantic PPR-based sampling shown in Figure 5 appear to have the same image as the ground truth. Does this indicate that the proposed method suffers from label leakage?

Limitations:
This paper has reasonably discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new task graph2image which is to generate images conditioned on both text descriptions and graph information, which improves consistency of generated images compared to conditioned only on texts or images. To address combinatorial complexity of graphs and dependencies among graph entities, the paper proposes a graph context-conditioned diffusion model InstructG2I for generating images from multimodal attributed graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- To the best of my knowledge, graph2image is a novel task, and the motivation to use the rich and high-dimensional information of graphs for image generation seems reasonable and interesting. 
- The proposed approach to incorporate graph information into pre-trained text-to-image is new, in particular introducing graph conditioning token and considering scalability of graph size. 
- The generated samples show that using graph information results in better consistency with the ground truth compared to methods that use only text prompts or images.
- Examples of controllable generation with both text and graph show the ability to balance content and style in a simple manner.

Weaknesses:
While I do not have a major concern, an ablation study on scalability to graph size seems to be missing. How large graphs is the method able to be applied?

Limitations:
Yes the paper addresses the limitation in Appendix A.1.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach for controllable image generation using both graph and text conditions. The authors propose that additional context information from multimodal attributed graphs (MMAGs) can enhance the performance of diffusion models. Specifically, they formulate the Graph2Image problem and develop the INSTRUCTG2I model to incorporate contextual information during the generation process. Empirical evaluations demonstrate the strong performance of the model.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is easy to follow.
2. The intuition behind the approach is clear.

Weaknesses:
1. The overall setting is questionable. The authors integrate graph information using a Graph-QFormer and context information such as artists and genres, stored in graph prompt tokens. Given the large graph size, they only use subgraph structures. Consequently, the Stable Diffusion (SD) model absorbs additional information from similar artworks, which could be derived from image or text prompts alone. This raises the question of whether an additional condition structure is necessary. I suggest the authors demonstrate a unique application where standard models with text and image prompting capabilities are insufficient.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zVrQeoPIoQ;"REVIEW 
Summary:
This paper proposes a new no-reference image exposure assessment method, Pixel-level IEA Network (P-IEANet), which analyzes and evaluates image exposure from the perspectives of brightness and structure using discrete wavelet transform (Haar DWT). Also, a dataset exclusively tailored for IEA, called IEA40K, is constructed. According to a comprehensive evaluation of methods on the IEA40K dataset, the proposed method achieves SOTA performance and offers advantages for the exposure enhancement community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper demonstrates very good originality as it is the first realization of pixel-level image exposure assessment. The authors have designed corresponding methods specifically addressing the characteristics of this problem and achieved satisfying results. Detailed explanations of the motivation and the current state of research are provided. Both the principles and the implementation of the method are clearly presented. The experimental results effectively demonstrate the performance of the proposed method. This paper not only proposes a new IEA method but also contributes a new dataset and benchmark, providing a significant boost to the IEA and exposure-related community.

Weaknesses:
Haar DWT is used to decompose an image into components with different frequencies, but the advantages of this method compared to other similar methods are not adequately explained. In the method section of this paper, some operations lack clear motivation or principles. For example, the reason for applying the DWT^{-1} and the choice of l1 norm as the loss function are not well explained. In the experiments section, SSIM and MAE are adopted to measure the structure and lightness similarity between the ground truth and predicted exposure residual. However, as a perceptual IQA metric, SSIM may not be suitable for evaluating the prediction accuracy of exposure residuals. The paper claims that the proposed method has improved adaptability across varying criteria and scenarios, but this is not well demonstrated in the experiments.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel paradigm that extends Image Exposure Assessment (IEA) from an image-level to a pixel-level framework. This paradigm comprises three components: model, dataset, and benchmark. Concerning the model, the study introduces the Pixel-level IEA Network (P-IEANet). This network processes images of varying exposures, separates them into low and high-frequency components via a discrete wavelet transform, assesses brightness with the low-frequency component, and evaluates structure with the high-frequency component, ultimately delivering pixel-level assessment results. Regarding the dataset, the authors have developed a new dataset, IEA40K, which includes 40,000 images featuring diverse exposures and corresponding pixel-level annotations. Finally, the paper presents comprehensive experiments on both holistic and pixel-level assessments, yielding promising results.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper initially proposes a pixel-level image exposure assessment paradigm, significantly enhancing precision in the field of image exposure assessment.

2. The paper introduces an assessment network that employs discrete wavelet transform, an intriguing choice supported by several ablation studies.

3. The paper proposes a large-scale, multi-exposure dataset with pixel-wise annotations derived from an automatic multi-exposure fusion technique, subsequently refined by human experts.

4. The paper also demonstrates that the P-IEANet can potentially improve the performance of low-light image enhancement methods.

5. The paper is well-composed, demonstrating a clear structure, precise language, and a logical flow of ideas.

Weaknesses:
The main weakness is that the paper lacks a well-defined definition for pixel-level image exposure assessment. For other details, please refer to the ""Questions"" part.

Limitations:
The paper adequately discusses the limitations of moving objects and image size while training. No negative social impact is present in this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work tackles the challenges in image exposure assessment from three aspects: models, datasets, and benchmarks. Specifically, A P-IEANet model based on DWT is proposed, which can generate pixel-level assessment results in a no-reference manner. An exposure-oriented dataset IEA40K is collected to cover various lighting scenarios, devices, and scenes, which are annotated by more than 10 experts with pixel-level labels. A comprehensive benchmark of 19 methods is conducted on the collected IEA40K dataset, where the proposed P-IEANet delivers the best performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Decomposing images into lightness features and structure components using Haar DWT is theoretically reasonable and empirically effective as presented in this work.
+ The dataset construction strategies described in Sec. 4.1 and Sec. 4.2 provide valuable insights to the related community.
+ The proposed model delivers good performance, even outperforming the LMM-based model Q-align.

Weaknesses:
- Holistic level assessment is performed on SPAQ. It should be straightforward to convert the pixel-level annotations to holistic level annotations in the proposed IEA40K dataset because the pixel-level annotations contain more information than the holistic level annotations.
- Would the performance of IEA models be boosted by jointly training (like the practices used in UNIQUE, LIQE, etc.) the model on the combination of IEA dataset and general-purpose IQA datasets?

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an innovative no-reference image exposure assessment method, transitioning from traditional holistic image evaluation to fine-grained pixel-level assessment. This approach effectively addresses the shortcomings of existing techniques in terms of accuracy and generalization. Researchers have developed P-IEANet, a pixel-level evaluation network that utilizes Haar discrete wavelet transform to analyze image brightness and structural information, enabling exposure assessment without reference images. Additionally, to support this method, the researchers have constructed the IEA40K dataset, which contains 40,000 images with detailed pixel-level annotations, covering diverse lighting conditions and devices. Using this dataset, they established a comprehensive benchmark including 19 methods, demonstrating that P-IEANet achieves state-of-the-art performance across multiple evaluation metrics. This work not only enhances the accuracy of no-reference IEA tasks but also provides valuable resources and new research directions for the image exposure research community. Future work will focus on optimizing the framework to support multimodal outputs and enhancing exposure perception in AI-generated content.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Pixel-level Evaluation: The P-IEANet proposed in the article is capable of conducting pixel-level image exposure assessment, which offers a more refined analysis and more accurate results compared to traditional overall image assessment.
- Innovative Model Architecture: By integrating the Haar Discrete Wavelet Transform with specific feature extraction modules, P-IEANet is able to analyze images from both the brightness and structural perspectives, providing a more comprehensive exposure assessment.
- Large-scale Dataset: The article has constructed the IEA40K dataset, which is a large-scale, diverse image dataset that provides rich resources for evaluation and training.

Weaknesses:
- The author mentions in the abstract that the code and dataset can be found in the supplementary materials, but there is no relevant section in the supplementary materials.
- There is no explanation as to why the Haar wavelet was chosen over other wavelets.
- The aesthetic quality of Figure 4 needs to be improved.

Limitations:
Not applicable

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
IIoH8bf5BA;"REVIEW 
Summary:
This paper proposes a new generative model on continuous random variables similar to diffusion-based generative models, but this method uses a different family of perturbation schemes. In particular, for the forward processes, which transform the clean data to a stationary distribution, the authors propose using piecewise deterministic Markov processes (PDMPs), which consist of a combination of discrete diffusions and ODEs. Specifically, the initial value, i.e., data, is transformed via simple ODEs, whose velocity term is often chosen to be a constant. However, the velocity term (also called vector fields) is updated at any random time with a constant rate $\lambda$. The occurrence of a new event is independent, and the time interval of the new event since the last one is exponentially distributed. Moreover, the new velocity will be assigned by user-chosen schemes; for example, one can flip the sign of the velocity, or another one can sample from the standard normal distribution. Thus, while the forward process transforms the data with a simple ODE, it randomly changes the ODEs, hence, the title of the paper.

In addition, the paper shows that under some mild assumptions, the time-reversed processes of the PDMPs are also PDMPs, and their transition probability distribution and update rate can be written in terms of the forward processes. Therefore, we can consider modeling such terms with parametric models similar to popular diffusion-based generative models.
The paper proposes the explicit ratio matching objective function, as in Equation 6, to train such terms.

Moreover, the authors suggest using three popular PDMPs for the forward process: Zig-Zag process (ZZP), bouncy particle sampler (BPS), and randomized Hamiltonian Monte Carlo (RHMC). Consequently, the paper shows the time-reversed PDMPs and potential parameterizations that fit each case.

Finally, the paper demonstrates the efficacy of the proposed method through a few toy experiments.

----------------------------------
Update the rating from 5 to 7 after the authors' rebuttal.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
One of the paper's key contributions is the introduction of a novel generative model that uses PDMPs as the forward processes. Moreover, the authors propose a training method for the time-reversed PDMPs.

Weaknesses:
The proposed method is novel and interesting. However, the paper didn't characterize some potential drawbacks well. For example, due to the nature of PDMPs, the training of the proposed method inherits the problem of discrete diffusion models. In particular, the ratio matching should be done for each dimension independently, and this costs a lot for high-dimensional data compared to the denoising score matching and others for previous methods. This problem could be the reason that the current submission didn't include the experiments on popular high-dimensional datasets. In this regard, the paper suggests that Monte Carlo estimates can reduce the computational cost, but it trades off the variance of the learning signals, which would be critical for large-scale experiments.

I consider that a new method doesn't always need to be better than previous methods. Nevertheless, proper information about the proposed method in practice would be important for potential readers to evaluate the significance of the paper. Thus, some additional discussions related to this problem need to be added.

Moreover, given that many alternative choices exist other than the proposed method, in-depth discussions on the proposed method's merits would be appreciated for evaluating its importance.

In addition, I find that the presentation should be improved. For example, many statements contain multiple prepositional phrases, which are difficult to parse and understand.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores generative models utilizing PDMPs, a type of stochastic process characterized by deterministic motion interspersed with random jumps at random times. These models offer an alternative approach to diffusion-based generative models, which have become very popular in the AI community in recent years. The authors focus on three specific PDMPs: the Zig-Zag process (ZZP), the Bouncy Particle Sampler (BPS), and Randomised Hamiltonian Monte Carlo (RHMC). The authors leverage the existing literature on the time reversals of Markov jump processes, characterizing the time reversal of any PDMP under appropriate conditions. They show that the time reversal of a PDMP is itself a PDMP with characteristics related to the original process. The authors also specifically outline the characteristics of the time-reversal processes for ZZP, BPS, and RHMC. The jump rates and kernels of these time reversals admit explicit expressions based on the conditional densities of the PDMPs before and after jumps.

The authors provide bounds on the total variation distance between the data distribution and the distribution of their generative models, considering errors from the approximation of the backward PDMP’s characteristics and its initialization from the forward process’s limiting distribution. Some initial but promising numerical simulations on simple toy distributions are presented, showcasing the potential of PDMP-based generative models. The results support further investigation into this class of models on more challenging data structures.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Originality: The paper introduces a class of generative models based on piecewise deterministic Markov processes (PDMPs). This is a very novel idea which departs from the widely used diffusion-based models. In my opinion, this is a fresh perspective on generative modelling and opens up new avenues for exploration and potential improvements in various application areas. The key idea behind diffusion-based models is the derivation of the reverse time and process, and following a similiar line of thinking, the authors of this paper characterize the time reversal of PDMPs, which is particularly original. While time reversal in diffusion processes is well-studied, applying these concepts to PDMPs and providing explicit expressions for their jump rates and kernels is a significant contribution.

Quality: The paper presents a comprehensive theoretical framework for PDMP-based generative models. The mathematical rigour in characterizing time reversals, deriving error bounds, and proposing training procedures leads to a complete and high-quality piece of work. The thorough analysis of the three specific PDMPs (ZZP, BPS, RHMC) and the detailed exploration of their time-reversal characteristics demonstrate a deep understanding and careful consideration of the underlying processes. The numerical simulations provide supporting empirical validation of the proposed models, adding credibility to the theoretical claims. The use of multiple toy distributions (extras are in the appendix) to test the models in the paper strengthens the evidence for their potential efficacy in practice.

Clarity: The paper is very well-structured, with a logical flow from the introduction of PDMPs to the detailed theoretical contributions and empirical results. Each section builds on the previous one, making it easier to follow the progression of ideas. The explanations of the PDMPs, their time reversals, and the derivation of jump rates and kernels are clear and very detailed. The inclusion of propositions and their proofs in the appendices provides a robust foundation for the proposed algorithms and establishes their convergence properties. The use of toy distributions to demonstrate the numerical simulations helps illustrate the practical application of the models, aiding in the reader’s understanding and also provides some nice opportunities for the authors to clearly articulate the advantages of their approach over the diffusion-based alternative approach.

Significance: The proposed PDMP-based generative models have the potential to be applied in a wide range of fields, from machine learning and statistics to physics and biology. This broad applicability enhances the significance of the work. By offering an alternative to diffusion-based models, this paper advances the field of generative modelling. The potential advantages of PDMPs, such as better scalability and reduced computational complexity, could lead to significant improvements in high-dimensional data generation. The theoretical insights and empirical results lay a strong foundation for future research in this area.

Weaknesses:
Originality: While the introduction of PDMP-based generative models is novel, the paper primarily focuses on theoretical aspects and toy datasets. There is a limited exploration of how these models can be applied to more complex, real-world problems, which could showcase their true originality and practical utility. Although PDMPs offer a new approach, the paper does not extensively compare these models with a wide variety of existing generative models, even on the simple toy examples that were considered here. This makes it difficult to fully appreciate the originality and benefits of PDMPs over other state-of-the-art methods.

Quality: The paper relies on several technical assumptions and conditions (e.g., H3, H4). While these are necessary for the theoretical results, their practical applicability might be limited and the authors do not discuss whether or not these assumptions hold for the toy examples which they consider. The paper could be strengthened by discussing the feasibility of these assumptions in real-world scenarios. The empirical validation is primarily limited to simple toy distributions. While these are useful for initial validation, the lack of experiments on more complex datasets (e.g., image or text data) reduces the overall impact and persuasiveness of the empirical results. It would be beneficial to include comparisons on more challenging benchmarks.

Clarity: The paper uses dense mathematical notation and detailed proofs, which may be challenging for readers who are not specialists in stochastic processes or PDMPs. This is going to be challenging for the authors as the high-level of technical detail provided in the paper does lead to a very robust paper. However, perhaps more intuitive explanations or visual aids could help make the content more accessible, if not in the main paper then in the appendix. The practical implementation details, particularly regarding the training procedures and simulation methods, are somewhat sparse (even though there are more details in the appendix). Providing a step-by-step guide or pseudocode could help practitioners better understand how to apply the proposed methods. Given the space constraints, this would have to be added to the appendix. This is covered in the case of splitting schemes, but could perhaps be modified to be more user-friendly to people new to this area of research. 

Significance: The paper’s significance is somewhat limited by the focus on theoretical and synthetic examples. Without demonstrating the effectiveness of PDMP-based models on real-world data, it is challenging to gauge their practical significance and potential impact in applied settings. While the paper claims that PDMPs offer better scalability and reduced computational complexity, there is limited empirical evidence to support these claims. Benchmarking the computational performance against existing generative models would provide a clearer picture of the advantages and limitations in terms of scalability and efficiency.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This interesting paper on the popular topic of generative models introduce a new family of generative models which builds on the so-called piecewise deterministic Markov process (Zig-Zag process, Bouncy Particle Sampler, Randomised Hamiltonian Monte Carlo). In contrast to many of the existing models this family is not based on diffusion models. The paper includes a through analysis of the construction 
and it propose training procedures and methods for approximate simulation of the reverse process.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
* A new family of generative models is proposed. 
* Thorough analysis of the properties of the proposed construction is provided.
* Simple examples provided.

Weaknesses:
* Missing real-world examples
* The is a big jump in  the style of writing between Section 1 and 2. Do not get me wrong here, the technical developments are most interesting, but many readers would be helped by a more gentle transition between these sections. Space for this can be created by moving more of the technical details into the supplemental material.

Limitations:
Real-world examples missing.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the development of generative models based on piecewise deterministic Markov processes. The key idea proposed in the paper is to use piecewise deterministic Markov processes instead of diffusions as the ""noising process"" of the generative model. This relies on the fact that time reversals of PDMPs are themselves PDMPs. Three specific instances of PDMPs are considered. The authors also derive a bound (in total variation distance) between the data distribution and the distribution of the generative model. The methodology is illustrated with some simple experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I enjoyed reading the paper and I like the idea of considering alternative noising processes in the context of generative models. The paper covers both theory and provides an example showing the viability of these methods. The examples are sufficient and certainly the area seems worthy of further investigation.

Weaknesses:
To me, the descriptions of approximating the process characteristics with normalizing flows are unclear. This part should be written more with more details, perhaps in the supplement, as this is core to being able to reproduce the results. I do appreciate that the authors provided a description of the experiment in E.1, but it is not enough to put things together. I would be interested in replicating at least the simple experiment, but I don't think I can do it as the paper stands.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes using Piecewise Deterministic Markov Processes (PDMPs) for generative modelling applications, by using the property that PDMPs also admit time reversals that themselves are PDMPs. There are three major contributions in my understanding - 

By characterizing certain families of PDMPs, i.e. Zig-Zag processes (ZZP, the Bouncy Particle Sampler (BPS), and the Randomised Hamiltonian Monte Carlo (RHMC), in terms of their jump rates and kernels, this paper shows how to obtain tractable closed-form and approximations for the jump rates and kernels of the time reversed PDMPs.
Theoretically, this paper then proposes a total variation bound between the “learnt” data distribution and the true data distribution when the base distribution is a Gaussian. This is a useful property, quite similar to bounds that have been proposed before in the literature (for example, for the Ornstein-Uhlenbeck process in [1, Theorem 5.2.]).
Finally, the paper proposes two empirical techniques to learn the time reversals akin to score-matching. First, for the ZZP process, inspired by score-matching techniques, the authors propose a ratio-matching technique. Secondly, for the BPS and RHMC processes, the authors learn normalising flows for the time reversal. They then show promising results in low-dimensional and MNIST generative modelling applications as a proof-of-concept.

[1] Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov diffusion operators, volume 103. Springer, 2014.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
I quite like the structure and formulation of the paper. The main goals and approach is elucidated quite clearly, and the mathematical preliminaries, while dense, seem correct for me. 

In my understanding, it is a known fact that all PDMPs admit an equivalent time-reversal, but these are quite hard to calculate in general. In this paper, building on theory involving jump Markov Processes in [2], the authors derive expressions for time-reversal jumps and kernels for 3 different PDMPs. In general, deriving the backwards time reversal and then also designing an empirical scheme with a neural network architecture and loss function would be a substantial contribution, but this paper has many additional contributions on top of that. The numerical experiments seem compelling, even if a little small scale. However, this paper seems like a proof-of-concept on the use of time-reversed PDMPs for generative modelling, and I think the theoretical contributions along with the design of the loss functions and training paradigms are a pretty significant contribution already.

[2] Giovanni Conforti and Christian Léonard.   Time reversal of markov processes with jumps under a finite entropy condition. Stochastic Processes and their Applications, 144:85–124, 2022.

Weaknesses:
Fundamentally, I think the paper lacks a convincing argument about why generative modelling with PDMPs would fundamentally be more useful than traditional generative modelling. I understand that there were some arguments made in the introduction of the paper, namely Lines 35-36 (“such as better scalability and reduced computational complexity in high-dimensional settings”). However, it is really unclear to me how this argument actually translates to the empirical score-matching (or normalising flow training) objectives that the authors formulate, vs an approach like DDPM. The experimental section is quite lacking in details and comparisons about how the PDMP approach improves along any number of axes, beyond the qualitative plots. For example, I can think of many axes of improvement that could be discussed - 
sample efficiency (how many training datapoints are needed to learn the time-reversal given that the process is partly deterministic), 
mixing rates towards the Gaussian for their time reversal. Usually, SDEs such as the Ornstein-Uhlenbeck process are quite quick at mixing towards a Gaussian, making them quite nice to use when reversing a Gaussian distribution as the base. For partly deterministic processes, is this easier or harder to do?
Are there any comparisons to regular Markov process methods that can show that having an irreversible Markov process is beneficial here? I believe that this is a big factor in why PDMPs are alluring, and their irreversibility makes them mix faster and use less data [3]. Any experiments showing sample efficiency and mixing rates would be really beneficial here.

I am worried that there are many subtleties in the training and sampling procedures of diffusion models, and indeed there are many papers focusing solely on the empirical training tricks that can improve generative modelling, and comparing to a vanilla DDPM model doesn’t properly ablate the technique. I would be hesitant to rely on these empirical results as a surefire sign of improved modeling, which is frustrating, as theoretically, the paper does seem to point to this being the case, and I really do want to believe. 

Furthermore, I think the paper could benefit from being a lot clearer about the specific advantages of PDMPs vs other stochastic processes for generative modeling. This is barely mentioned, but does form the crux of the empirical results. This made it difficult for me to read through the theoretical developments, proofs and theorems without knowing the reason why we would really want to do this in the first place.

I also think the paper can also benefit from being more explicit in how their developed score matching and normalizing flow training differs from traditional methods (maybe an algorithm block), as this would be something really interesting to practitioners looking to adopt existing codebases to using PDMPs instead.

[3] Bierkens, J., Fearnhead, P., and Roberts, G. (2019), “The zig-zag Process and Super-Efficient Sampling for Bayesian Analysis of Big Data,” The Annals of Statistics, 47, 1288–1320. DOI: 10.1214/18-AOS1715

Limitations:
Yes, they have addressed any potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zV2GDsZb5a;"REVIEW 
Summary:
This paper presents a method for relighting objects observed from a single image. While existing approaches rely on specific capture condition using flashlight illumination or portrait captures, or require to explicitly decompose the scene into geometry and reflectance, the proposed method aims to generate images of a given objects under novel illumination conditions for arbitrary environmental lighting conditions. The authors show that this is possible by relying on a generative diffusion method that is conditioned on the environmental map. The method relies on a pre-trained diffusion model that is fine-tuned on a synthetic relighting dataset to learn the conditioning. The approach is evaluated qualitatively and quantitatively on single-object images. Relying on a conditional diffusion model for relighting, the authors also show additional conditioning on text for relighting.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work presents a simple (this is a good thing) and effective method for relighting from a single image. The method relies on synthetic supervision with a novel Blender-rendered dataset that uses Objeverse as input model source. The authors went a long way by collecting diverse HDR environment maps from the Internet that were augmented to produce a large synthetic relighting dataset of almost 20M rendered images with ground truth lighting maps. Overall, the method offers a number of intriguing benefits listed as follows:

* Conditional image-to-image diffusion model: The method inherits a conditional Zero-1-to-3 model that is extended in its input latents to a rotated environment map with the camera coordinate frame, allowing for image-to-image relighting in a consistent frame. While, given enough training data, the method is effective in relighting, the approach also enjoys the benefits of existing diffusion architectures with various types of conditioning. The authors demonstrate this effectively with their image conditioning. 

* Relighting 3D radiance fields: The proposed method is evaluated as a prior for 3D relighting of a neural radiance field. Specifically, the authors propose to use diffusion-based relighting as a coarse reconstruction loss (predicting a coarse relit scene during the NeRF optimization) and a detail refinement loss where the NeRF appearance is further refined.

* Qualitative evaluation: The evaluations presented qualitatively in the main manuscript and the supplemental material in the form of supplemental videos are visually plausible and convincing. 

* Quantitative evaluations: The method is adequately ablated and quantitatively compared to single image relighting methods, 3D radiance field relighting with reasonable margins on the test sets. This validates the method as an effective approach.

Weaknesses:
What makes the method exciting, at first glance, is also one of the major weaknesses: the technical novelty. The paper piggy-backs on an existing generative method, the Zero-1-to-3 model, that is with a few variations used for relighting. While the simplicity is something that is desired, it also makes it challenging for the reader to derive deeper insights from this work. We learn that pre-trained diffusion-models, when just given enough and the right synthetic data, can allow for plausible novel view synthesis with artefacts that are improved over existing methods. However, the recent work by 

Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lighting control for diffusion-based image generation, 2024.

in a way also does show the exactly same, although the technical approach is different. Overall, the technical contribution of the approach is rather incremental (although the method is effective). As such, I am torn on this work. While the technical contribution is not near other work at NeurIPS, the method is effective and likely of high impact. 

A further qualm I have is regarding the results compared to NVDIFFREC. While the margins are not substantially different, the results in Fig. 6 seem to indicate differently. It seems as if these results are cherry-picked.

Limitations:
All major limitations are addressed. The only open limitation not addressed in the manuscript is the runtime. The authors should address and comment on the runtime for their diffusion model.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Neural Gaffer, an end-to-end 2D relighting diffusion model designed for single-image relighting without the need for explicit scene decomposition. Neural Gaffer can synthesize high-quality relit images of any object under novel environmental lighting conditions by conditioning on a target environment map. The model builds on a pre-trained diffusion model, fine-tuning it on a synthetic relighting dataset. The advantages in generalization and accuracy through evaluations on both synthetic and in-the-wild Internet imagery are shown in the paper. Neural Gaffer can be combined with other generative methods for various downstream 2D tasks like objection insertion. The video results presented in the paper are of high quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)  Neural Gaffer performs single-image relighting without the need for explicit scene decomposition into intrinsic components like normals and BRDFs. This provides an avenue for relighting without collecting expensive relighting real-world datasets.

2)  The model can generate relit images of various objects under different environmental lighting conditions based on a target environment map. The method takes a single image as an input.

3) The method can be applied to real-world objects with high-quality relighting results and perform various downstream tasks such as object insertion.

Weaknesses:
1) In case of the real-world object scenarios, the object may not be always centred and may have complex backgrounds and lighting to start with. The paper does not demonstrate how would the method behave in such cases. How about the objects with high-frequency texture details?

2) Related to 1) there might be multiple objects in a scene. From the results, it seems that the method cannot handle multiple objects from a single image.

3)  The real-world object examples shown in the paper and the video are good but not impressive. It would be more compelling to show faces, humans, animals etc under the lighting conditions to show the generalizability of the method.

Limitations:
The authors have discussed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Neural Gaffer presents an approach to object-centric image relighting using diffusion models. The method adapts a pre-trained diffusion model and fine-tunes it on a synthetic dataset designed for relighting tasks. The main feature is its ability to condition the diffusion process on target environment maps, allowing for control over lighting effects.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) Simple yet effective approach: The paper presents a straightforward fine-tuning method for object relighting, similar to zero-1-2-3 shot learning. This simplicity is a strength, demonstrating that complex relighting can be achieved without overly complicated techniques.

2) Powerful data-driven learning: The supervised conditional diffusion model effectively learns to relight objects, highlighting the potential of data-driven approaches in capturing intricate lighting interactions.

3) Competitive results: Based on the presented figures, the method appears to outperform the recent DiLightNet in some aspects. However, this comparison raises some evaluation questions (see questions section for details).

Weaknesses:
1) Real-world evaluation: The model is fine-tuned on a synthetic relighting dataset, which might not fully capture the complexity of real-world lighting scenarios. Real-world evaluation is necessary, and there are datasets capturing these effects. The paper is currently missing this evaluation, and there are datasets available for such evaluation [1] OpenIllumination [2] Objects with Lighting or [3] Stanford ORB dataset. These papers have been cited but it is surprising to not see an evaluation of these datasets.

2) Reliance of Environment map: Do you need to supply the environment map for relighting? There is a missing baseline that shows what happens if you condition the target lighting image without a full environment map (only image crops). The Diffusion Light Probe (CVPR 2024) paper indicates that diffusion models are capable of inpainting reliable environment maps and they seem to be implicitly encoded within the model. This baseline will justify why a full environment map is required or necessary for this task.

3) Generalization to scenes: The extent to which the method generalizes to scenes -- not just objects -- is unclear. Evaluating the MIT-multi illumination dataset could shed light on this. The current reliance on explicit environment maps makes it harder to perform on these scenes, but it would be interesting to see if, without explicit environment maps (like suggested above), can you learn to relight and compare on scenes.

4) Evaluation metrics: Recent studies show that PSNR, SSIM, etc. are not consistent with human evaluation. See ""Towards a Perceptual Evaluation Framework for Lighting Estimation"" (CVPR 2024). These metrics don't tell us much about whether the method is promising as such. A thorough evaluation via user studies or the metrics as defined in the recent paper is currently missing from the paper.

5) Unrealistic results and missing comparisons: The object insertion results look unrealistic, with incorrect shadows that don't match the lighting conditions. Several relevant lighting-aware compositing methods are missing from the comparisons, such as ControlCom [Zhang et al., arXiv 2023], Intrinsic Harmonization [Carega et al., SIGGRAPH 2023], Reshading [Bhattad and Forsyth, 3DV 2022], and ARShadowGAN [CVPR 2020]. The comparison to AnyDoor doesn't make sense as it's not lighting-aware. Including these comparisons would provide a better evaluation of the method's performance against current state-of-the-art techniques.

6) Further, as the papers use off-the-shelf methods to estimate environmental maps (text2light), why not compare with existing 3D object compositing with lighting estimation methods to get a sense of how the proposed methods compare to these tasks -- see Garon et al (CVPR 2019), StyleLight (Wang et al; ECCV 2022) and similar papers? Rendering objaverse objects using lighting estimated from the mentioned or similar methods would help understand the gaps between explicit environment map prediction methods.

7) 3D relighting evaluation: For the 3D relighting setting, according to the Objects with Lighting 3DV 2024 paper, Mitsuba + NeuS is a stronger baseline compared to TensorIR, which is currently missing in the paper.

8) Failure analysis: The paper mentions in the limitations section that the approach might not work for portrait relighting, but it would be interesting to see the kind of failures the diffusion model makes. The current setup lacks experiments in this direction to see what are these failures to encourage future research. Further, the current papers also do not provide any failure examples from Objaverse instances. Is the method perfect on all unseen objects -- detailed analysis is missing as to what objects the proposed methods perform best or worse on. Such analysis helps scope out limitations of the current methods instead of shallow limitations provided in Appendix D.

9) Lack of comparison with simple color matching baselines: The paper doesn't include a comparison with straightforward color adjustment techniques, such as RGB histogram matching between the inserted object and the target scene. This omission raises questions about how much of the method's perceived success in relighting is due to sophisticated light interaction modeling versus simple color transformations. A comparison with such a baseline would help quantify the added value of the diffusion model approach over a simpler method.

Limitations:
Somewhat but not fully. See my weakness 8.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel method for single-image relighting, which takes an image of an object and a target environmental map as inputs. The authors fine-tune Stable Diffusion on a synthetic relighting dataset to output relit images, conditioning on both the input object image and the target environmental map. The authors show their method outperforms existing baselines. Additionally, the trained relighting model can be applied to downstream tasks such as relighting a neural radiance field and object insertion.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I check the video results in the supplementary video. The visual results are impressive.
- The authors have shown several downstream applications using their trained relighting model, including text-based relighting and object insertion.
- The authors have conducted extensive ablation studies to prove the effectiveness of their proposed method.

Weaknesses:
I don’t have many complaints about the paper. I list several potential improvements below:

- In the 3D relighting experiments, it seems unfair to compare with inverse rendering methods such as Nvdiffrec-mc and TensoIR, as they can apply any lighting to the object once the material is recovered, while Neural Gaffer needs optimize for every lighting. On the other hand, I think Neural Gaffer should be combined with these inverse rendering methods and provide priors when recovering material and lighting.
- The extrinsic information is injected by rotating the environmental map. However, it seems intrinsic information is not considered, which means there is an assumed fixed FOV. This could introduce biases in downstream applications and limit the input views in 3D relighting.
- The quantitive comparison with IC-Light is missing.
- The generated image resolution is limited to 256x256.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zTu0QEpvtZ;"REVIEW 
Summary:
This paper aims to understand two mechanisms of diffusion models. First, the denoising process is analyzed, and it is found that shapes in an image are constructed in the beginning of the denoising process, while textures and details are filled in later. This empirical observation is justified with a mathematical frequency analysis. Second, the role of text conditioning is analyzed and it is found that the [EOS] token, which captures global information of the prompt, is relied on more heavily by the diffusion model. It is also observed that the text prompt is utilized more in the earlier stages of the denoising process. This finding is utilized to speed up diffusion sampling by ~25% while maintaining the image quality and prompt alignment. This is done by only injecting conditional information in the beginning of the denoising process.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Although the finding that shape is constructed in the first few timesteps has been observed many times before, it is nice to have a more principled study with various experiments and mathematical justification. 
* The finding that the special [EOS] token is the most relied upon during generation rather than the prompt tokens is an interesting finding that can be used in later studies. For instance, improving prompt alignment, attribute binding, etc. 
* The observation that the text prompt is used more in the early denoising process lends itself to a practical application of speeding up inference. 
* Multiple architectures and samplers are used in this study, suggesting the generality of these findings.

Weaknesses:
* As mentioned in the Strengths section above, the findings are not completely surprising (for instance, the shape reconstruction or reliance on text in the early denoising steps, then detail-filling in the later steps). However, this work takes a principled approach in studying these phenomena which have largely been used in diffusion application literature (e.g., [1, 2])
* Limited to no mention of broader impact or limitations. Furthermore, the Conclusion section is just a summary of the paper but does not discuss the implications of these findings. 

[1] @inproceedings{mengsdedit,
  title={SDEdit: Guided Image Synthesis and Editing with Stochastic Differential Equations},
  author={Meng, Chenlin and He, Yutong and Song, Yang and Song, Jiaming and Wu, Jiajun and Zhu, Jun-Yan and Ermon, Stefano},
  booktitle={International Conference on Learning Representations}
}
[2] @inproceedings{hertzprompt,
  title={Prompt-to-Prompt Image Editing with Cross-Attention Control},
  author={Hertz, Amir and Mokady, Ron and Tenenbaum, Jay and Aberman, Kfir and Pritch, Yael and Cohen-or, Daniel},
  booktitle={International Conference on Learning Representations}
}

Limitations:
Although there are no societal implications, a discussion of limitations is lacking.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the mechanism in the text-to-image diffusion model, including the generation order of image components, the influence of various tokens, and the steps in which tokens work.
These observations bring some insight into understanding the diffusion model.
Besides, the authors also design a sampling strategy that accelerates the sampling of the denoising process by 25%+.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The conclusion of the [EOS] token is interesting and has been rarely investigated in previous papers.
2. The analytical experiments in this article are sufficient and strongly support its conclusion.
3. The writing expression of this article is very clear.

Weaknesses:
1. The other conclusions in this paper, e.g., shape first then details, have been discussed in previous works.
2. The sampling strategy is more like a sample trick than a method.

Limitations:
Suggest the author to discuss the applicability and limitations of the proposed sampling scheme.
For example, can it be applied to human face generation without losing human identity?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates the denoising process in DPM, identifying that the overall shape of the image is formed early in the process while details are added later. It further examines the influence of different text prompt tokens, finding that the end-of-sequence token [EOS] plays a crucial role in shaping the initial stages of image generation. The authors propose a method to speed up the generation process by removing text guidance after the initial stages, achieving a significant reduction in computational cost.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Comprehensive analysis of the denoising process stages in DPM.
- Detailed exploration of the influence of different tokens in the text prompt.
- Practical application of findings to accelerate the T2I generation process.
- Empirical and theoretical support for the proposed acceleration method.

Weaknesses:
- The paper might lack clarity in explaining the theoretical aspects of frequency signal analysis.
- Limited exploration of potential biases introduced by the dominance of the [EOS] token.
- The study may benefit from a broader range of experiments to validate the generalizability of the findings.

Limitations:
- Authors should discuss the robustness of their findings and the need for further experiments across various models and more complex or diverse text prompts to validate their conclusions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper study how the EOS token plays a role in the generation process of diffusion model. In particular, this paper finds that diffusion models tend to first generate low frequency part of the image at the beginning of the generation process, then gradually add high frequency signal to it. Experiments show that the low frequency signal is conditional on the EOS token while the high frequency signal can be generated without text guidance. In combined with the aforementioned observation, this paper proposes to remove $\epsilon_\theta$ in classifier-free guidance once the low frequency signal has been generation to improve generation efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This paper offers a new perspective for understanding the role of textual condition in diffusion models.  By exploring how the EOS influence the generation process of diffusion model, this paper argues that the conditional part $\epsilon_\theta$ in classifier-free guidance (CFG) might be unnecessary after certain denoising step $t_w$.  
- Most experiments are inspirational and interesting. By swapping the EOS token and the sentence body, it demonstrates that diffusion models rely on the EOS token to synthesize low frequency part of the image. 
- This paper explains the tendency of generating image from low-to-high frequency in diffusion models.

Weaknesses:
- It is not clear that how the ""computational cost"" is defined in this paper. If the computational cost is GPU VRAM, then the claimed efficiency improvement might be invalid, as the required GPU VRAM for computing $\epsilon_\theta(x_t, C)$ or $\epsilon_\theta(x_t, \emptyset )$
 is unchanged. 

- This paper mainly focus on the role of EOS token in T2I diffusion models while neglecting the SOS token. Despite the weight of SOS token is significantly higher than SEM and EOS token (see Figure 3). However, the authoer(s) claims that the SOS carries no information due to the autoregressive nature CLIP text encoder.  Since this claim is not yet supported by other works, the author(s) should have conducted experiments to support this claim, as there is a chance that EOS and SOS tokens altogether influence the generation process.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zO55ovdLJw;"REVIEW 
Summary:
The paper proposes a prompt optimization approach to the missing modality issues in multimodal learning. Inspired by the missing-aware prompt (MMP), this paper adds more prompts, including correlated, dynamic and modal-common prompts, to each encoder to improve the performance. The experiment on three datasets shows the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The missing modality issue in multimodal learning is a practical challenge. 

The designed method is clearly presented.

Weaknesses:
1. The novelty of the proposed method is limited since the MMP has proposed the prompt optimization approach to solving the missing modality issue. Compared with MMP, this paper adds more parameters in the form of prompt tokens from different inputs and functions. 

2. The empirical comparison with MMP is probably not quite fair as the proposed method uses more additional parameters compared with MMP. According to Line 337, this method adds 2.4% additional parameters, while MMP only adds 0.2%.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The model proposes prompting strategy where both modalities (image and text) are prompted, and the prompt for both modalities are correlated. The strategy is to use multiple prompts, namely correlated prompts, dynamic prompts, and modal-common prompts. As the backbone itself is multimodal (CLIP), it is a good idea to consider synchronized multi-modal prompts to fully harness the model capabilities when prompting it. The model surpasses multiple multimodal SoTAs on multiple datasets and also has proven to be effective in handling missing modalities in training and inference.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The strategy of using multiple types of multimodal prompts, along with the correlation strategy, is logically sound as the multimodal backbone itself is trained to understand the relationship between image and text modalities.

2. The modal surpasses multiple SoTAs on multiple benchmarks with considerable score improvement.

3. The ablation studies are sufficient to understand the justification of the network design.

Weaknesses:
1. Ablation studies regarding the multimodal backbone, e.g. using other model than CLIP or use dedicated unimodal encoders for each modality, highly recommended to increase paper quality.
2. In table 4, what are the performances when either image or text modalities are completely missing?

Limitations:
The limitations are discussed in the appendix, including that only text and visual modalities are tested with this model and the number of tested models.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of generalized missing modalities in multimodal learning, where a modality can be absent during any learning phase (e.g., training, testing, or both). he authors investigate prompt learning with missing modalities and propose deep correlated prompts designed to capture various types of correlations between prompts and input features across different modalities. Specifically, the proposed prompts include mechanisms for perceiving beneficial information from preceding layers, dynamically generating prompts based on input characteristics, and leveraging the complementary information from multimodal inputs. These designs improve the robustness of large multimodal models (e.g., CLIP) to missing modalities. Extensive experiments and ablation studies demonstrate consistently superior performance and verify the effectiveness of the proposed method.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper addresses a more challenging missing modality setting, where modalities may be absent during both training and testing phases, making it highly practical and essential for real-world applications.
2.	The paper is well-motivated. The authors highlight the weaknesses of prior work and propose several designs (e.g., deep correlated prompts, dynamic prompts, common prompts) to improve robustness.
3.	The paper explores various types of correlations between prompts and input features across different modalities, and the proposed designs for each are technically sound.
4.	Extensive experiments show great improvement on the baseline and consistently superior performance compared to other methods across all benchmarks.
5.	Comprehensive ablation studies are conducted to validate the effectiveness of each proposed component.

Weaknesses:
1.	The paper lacks a detailed explanation or discussion on the efficacy of different prompt designs. In Figure 2, it shows that sequentially adding different designs improves the baselines, but it does not discuss the individual improvement gains for each design. Additional discussion on each design could help validate whether the increasing gains from sequentially adding designs are not merely due to more learnable parameters.
2.	The paper lacks visualization of each learnable prompt (e.g., deep correlated prompts, dynamic prompts, and common prompts). Visualizations could help validate whether the different components work as expected. For example, do dynamic prompts genuinely capture the different characteristics of inputs, or do they merely distinguish between different missing cases, which might be easier to learn due to the obvious absence of a modality?
3.	For each available modality, it seems there are a total of $(3*(2^M-1))$ prompts for each missing modality case. This could lead to an exponential increase and redundant prompts as more modalities are considered (i.e., M>2). For example, in a vision-and-language task, in the case of complete and missing-image, the text modality is available for both cases. However, it requires two separate prompt sets for the text encoder, which may actually learn the prompts for the same “text-available” case.

Limitations:
One limitation is that the proposed method requires modality-specific deep correlated prompts for each available modality, which could be challenging to extend to more modalities (e.g., five or more modalities).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to address the missing modality problem for the multimodal recognition model (i.e. the multi-modal data could be incomplete). There are three techniques of prompting being proposed (while the recognition model, i.e. two-stream multimodal method CLIP in this paper, is kept fixed), including: 1) correlated prompts, where a part of the prompts in the input-level are firstly selected according to the missing scenario (e.g. complete, text-only, or image-only), then the prompt in each of the following network layers are predicted from the multimodal prompt of its preceding layer; 2) dynamic prompts, the input-level prompts contain a portion generated according to the input sample; 3) modal-common prompts, where the rest of the input-level prompts is stemmed from a common component shared across modalities. The combination of the aforementioned three techniques experimentally shows better performance in comparison to various baselines (mainly the SOTA method from MMP [17]).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The proposed method provides superior performance with respect to various baselines and its proposed techniques (i.e. correlated prompts, dynamic prompts, modal-common prompts) are experimentally shown to benefit the model performance.
+ The extensive experiments are conducted on multiple dataset with various experimental settings.
+ The presentation is clear and easy to follow.

Weaknesses:
- The modal-common prompts and the dynamic prompts actually are not directly connected to the missing modality problem (or being irrelevant to different cases of missing modality). While excluding these two prompting techniques from the proposed method (in which such variant becomes ""Ours (A)"" in Figure 2), the improvement with respect to the state-of-the-art approach of handling missing modality (i.e. MMP[17]) would become marginal (please include MMP[17] into the ablation study shown in Figure 2 or directly provide the tabular quantitative results for the ablation study). Similarly, while we only consider the technique of correlated prompts as the manner in the proposed to tackle the missing modality, it becomes the only difference in the proposed method compared to MMP [17] (in terms of methodology), thus leading to the concern of limited novelty. Furthermore, there should be a baseline of integrating the modal-common prompts (acting as a basic component of prompt) and dynamic prompts into MMP[17] to better highlight the contribution of the proposed correlated prompting technique (which is the main technique in the proposed method to be connected with the missing modality challenge). Moreover, as modal-common prompts and the dynamic prompts introduce additional learnable parameters (in comparison the correlated prompts), there should be further detailed analysis/comparison in terms of number of learnable parameters versus model performance.
- Though the proposed dynamic prompts do experimentally shown to improve the overall performance under various missing modality cases, such prompting technique is actually not new, where we can see its similar application in various research problems (e.g. Wu et al., IDPG, NAACL'22; Lu et al., PromptPG, ICLR'23; Qiu et al., FedTPG, FL@FM-NeurIPS’23).

Limitations:
no potential negative societal impact is found.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method to handle missing modalities in visual and language recognition systems.
The paper proposes a very similar method to the one proposed by MMP [17] but using different way of getting the prompts to feed them into the transformer layers. 
Comparison with other works show that the method seems to be effective and some ablations studies are performed to study the different design choices. The method is validated using the most common datasets for this task.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method seems to work when compared with other state-of-the-art models.
- The paper presents results on several datasets and with different settings of the model.

Weaknesses:
- The main weakness of the paper is clarity. There are three different sets of prompts that are appended to the intermediate representations. However, the only difference between them seems to be the type of architecture the method uses to compute them. The explanation is very limited and Figure 1 does not illustrate where do these prompts come from. Without the clarity of this explanation it becomes really hard to understand how the motivation of each type of prompt fits the design. What are exactly correlated prompts, dynamic prompts, and modal-common prompts? What make them correlated, dynamic and modal-common? This is not clear in the paper at all. 

- It is not clear what is baseline. What does dropping features when modality is missing? The input sequence become shorter and coming from only a single modality? If that's the case, what is trainable and what is not? 
Please explain well this part. I would expect that this baseline is: training with the same number of parameters as the base method, by simply adding learnable prompts at each layer and training using mod-drop (dropping modalities randomly when training, dropping modalities can be done by inputting noise instead of tokens, the average of the modality tokens, zeroes, or not passing the missing modality at the input, it is a design choice that needs to be explained). If it is not what I'm thinking, please explain well, since this is a key experiment.

- When comparing with MMP, how did the authors do it? Please explain exactly how was this re-implementation. Also, to be fair, the authors should have applied their method using ViLT instead of CLIP, in that way there is no doubt that this method is better than the very similar MMP. 

- What is the zero-shot performance of CLIP on these datasets?

Limitations:
Limitations have been addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zNiJZUAlxg;"REVIEW 
Summary:
The paper analyzes the class-generalizable anomaly detection problem and introduces residual feature learning. 
Based on the residual features, the paper proposes a simple AD framework, i.e., ResAD, which incorporates OCC loss and distribution estimating to distinguish normal and abnormal data.
The experimental results demonstrate that the ResAD performs well on real-world industrial AD datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper analyzes the few-shot class generalizable anomaly detection problem and delivers an interesting insight into residual features.
2. The proposed method is intuitive and easy to understand.
2. The paper is well-written and organized.

Weaknesses:
1. The residual learning for few-shot AD has already been proposed in inCTRL[1]. The proposed Multi-Layer Patch-Level Residual Learning scheme in InCRTL is more sophisticated and reasonable than the direct subtraction in this paper.
2. The results in Table 1 of InCTRL are not consistent with the results in the original paper. Compared with the original results of InCTRL, the ResAD results do not achieve the SOTA performance. 
3. The paper aims to achieve generalization across different classes. I think the authors should compare the accuracy of each class on the Visa dataset with other methods to demonstrate the generalization capability of your approach for different classes, rather than taking the average accuracy of different classes in the dataset.

[1]Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024.

Limitations:
The authors did not give a discussion on the limitations of the proposed method.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple but effective framework that can be directly applied to detect anomalies in new classes. The main insight is learning the residual feature distribution rather than the initial one. In this way, we can significantly reduce feature variations. Even in new classes, the distribution of normal residual features would not remarkably shift from the learned distribution. Experiments were conducted on four datasets and achieved remarkable anomaly detection results.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is original, high quality, clear, and easy to understand. The proposed method has a good heuristic effect on establishing a general anomaly detection model and will become a valuable baseline for the community after the release of the code.

Weaknesses:
1. Although unnecessary, I recommend punctuation at the end of a formula. This is one of the few formatting problems I can pick out. [Well written]
2. In Figure (b), it is suggested that abnormal should use a triangle icon. The difference between a hexagon and a circle is too small to see clearly.
3. The large difference between normal images should be considered, and image difference indicators such as FID and LPIPS can be used to calculate the difference inside the normal images in the data set you show. The difference should be relatively small, which is a potential false alarm hazard.
4. As stated in point 4 of the questions, the experimental setup of training on MVTecAD and then testing on the various classes of VisA is not reasonable.

Limitations:
This paper objectively mentions the limitations of this article, and there is no potential negative impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposed a simple yet effective framework ResAD for class-generalizable anomaly detection by leveraging residual feature learning and a hypersphere constraint. The framework's ability to generalize to new classes without retraining or fine-tuning makes it valuable for real-world applications, providing significant improvements over existing methods. Comprehensive experiments on four real-world industrial AD datasets (MVTecAD, VisA, BTAD, and MVTec3D) demonstrate ResAD's superior performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
(1)ResAD effectively addresses the challenge of class-generalizable anomaly detection, the generalization ability using only a few normal samples as references makes it highly practical for real-world applications.

(2)The use of residual feature learning to reduce feature variations and improve generalizability is novel and effective

(3)The approach is shown to be robust across different datasets and settings.

Weaknesses:
(1)The experiments are primarily conducted on industrial anomaly detection datasets. While these are relevant, the method's generalizability to other domains, such as medical images or video data, is not fully explored.

(2)The selection of few-shot reference samples may impact performance. Previous methods typically run multiple independent runs using different random seeds to ensure robustness. However, this work only provides results from a single group of samples, which may not fully represent the model's performance variability.

Limitations:
Limitations are discussed in Appendix Sec. B.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to address cross-class anomaly detection problem. To this end, this study introduce a residual learning framework ResAD. The ResAD framework aims to learning residual feature distribution between target image and reference image. Experiments are conducted to valid the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The cross-class/class-generalize anomaly detection is a crutial task in the realm of anomaly detection.
2. The structure of ResAD is simple and effective.

Weaknesses:
1. The idea of residual estimation is highly similar to InCTRL [1].
2. Lack of comparision with FastRecon[2] and AnomalyGPT[3].
3. The writing should be improved. The optimization terms are unclear and hard to follow.
4. In Table.5, there is a reproduced result of WinCLIP on WideResNet50, however the windows in WinCLIP is designed for VIT, how can the authors report the result?


[1] Jiawen Zhu and Guansong Pang. Toward generalist anomaly detection via in-context residual learning with few-shot sample prompts. In CVPR, 2024.
[2] Fang Z, Wang X, Li H, et al. Fastrecon: Few-shot industrial anomaly detection via fast feature reconstruction[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 17481-17490.
[3] Gu Z, Zhu B, Zhu G, et al. Anomalygpt: Detecting industrial anomalies using large vision-language models[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(3): 1932-1940.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zNIhPZnqhh;"REVIEW 
Summary:
This paper demonstrates that WTA circuits along with STDP learning resembles EM algorithm-like Bayesian inference and could be used for motion segmentation from event streams by contrast maximization of warped events.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes an interesting approach for event motion segmentation based on observations from event-based dynamic vision sensors, utilizing a EM-like framework for identifying various motion models from event streams and clustering them into motion patterns. This is achieved using WTA circuits together with STDP-based learning.

Weaknesses:
The main weakness of the paper is that the proposed method lacks proper justification of the presented approach, which seems like a heuristic hard clustering method, together with gradient based learning. The experiments also lack depth and the authors demonstrate the high dependence of the performance of the method on the parameter initialization. A more careful writing of the underlying model, the optimization framework and the proposed methodology would be good (see the questions below). Furthermore, the paper lacks more details regarding the choice of $N_{\ell}$ (number of motion models) and the specific forms of the warping functions $W_j$ used. Several steps in the entire methodology, although intuitive, are presented in a heuristic fashion without detailed description and clarity.

Limitations:
See the questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a spike-based Bayesian inference framework for motion segmentation with event cameras. By designing neurons that utilize STDP for online learning of motion patterns, the framework can perform the M-step of the EM algorithm in motion segmentation of event streams. Additionally, the WTA circuit implements the E-step, allowing for the online partitioning of event streams into different motion patterns. The authors provide theoretical proof and experimental results to demonstrate the network's spatiotemporal decoupling capabilities for mixed motion patterns of event streams.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors demonstrate that the SNN framework based on WTA is equivalent to the EM algorithm for motion segmentation of event streams. This online learning approach is compatible with neuromorphic data and beneficial for deployment on low-power, low-latency neuromorphic computing platforms.
 
• The work is based on the Bayesian brain hypothesis, using a more physiologically interpretable SNN for Bayesian inference. Applying this to spatiotemporal data from neuromorphic cameras represents a promising research direction.

Weaknesses:
• The experimental results lack quantitative evaluations. Can the authors further perform object detection and tracking based on the motion segmentation, providing metrics such as object detection success rates and comparisons with other methods?
 
• The proposed algorithm lacks the analysis of time complexity or processing speed. Can it leverage the low-latency advantage of event cameras?

Limitations:
There is a need for quantitative evaluations and an assessment of the dependency on parameter initialization.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proposes a spike Bayesian computational framework for continuous motion segmentation in event streams and demonstrates that the constructed network can implement an EM-based event stream motion segmentation model. The proposed model uses WTA circuits in the network to achieve an equivalent E-step, while the STDP rules for an M-step for contrast maximization. Experimental results demonstrate the network's online learning effectiveness for continuous inputs on extreme event camera datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed network's effectiveness for motion segmentation has been validated on event datasets featuring challenging scenarios that involve mixed camera self-motion and high-speed moving objects. The proposed spike Bayesian inference framework is highly interpretable and applicable to various neuromorphic vision chips and computing hardware, representing a promising research direction.

Weaknesses:
The authors mainly use SVD to find different patches' motion patterns for initialization. Why is this method used, and can other methods be employed for selection? It is recommended that the authors conduct ablation experiments to explore further.

Limitations:
The authors have stated the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes to address motion segmentation at very high temporal resolution via an event-based or spiking implementation of expectation-maximization in a generative model. It demonstrates the performance of the resulting spiking neural networks on example experiments.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The strength of the paper is its deep engagement with the spiking neural network literature, as well as its use of spiking networks for the specific type of problem to which they are most suited: event-based computation.

Weaknesses:
The paper's major weakness is its lack of clarity, which the authors have discussed and addressed in the review period.

Limitations:
I am not confident that I can identify the specific limitations of this paper as opposed to the limitations of spiking neural networks generally.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zMNd0JuceF;"REVIEW 
Summary:
This paper proposes a new way to jailbreak LLMs through an improved version of few-shot jailbreaking. They propose to use a random search to select examples that are most effective to jailbreak the mode from a pre-defined pool generated with Mistral-7B. On top of that, they alternate the steps of each example by the special tokens that are used in the LLMs conversation templates to separate user messages from the model's responses. The authors show that this method is more effective than the previous jailbreaking methods for five different models, and that it can be used and adapted to evade a large number of defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Simple and effective method**. The method proposed is simple and effective. It is easy to understand and to implement. The experimental results show that it is more effective than many baselines.

**Insightful ablations**. The authors do a great job at showing what components are most important for the success of the attack. They check how many shots are necessary, how important the size of the pool is and how important the special tokens are. However, there are some other ablations that I believe would make the paper stronger (see weaknesses).

**Effective evasion of defenses**. The authors show that their method is effective at evading a large number of defenses of different types, from a perplexity filter, to perturbation-based defenses, to safety-filters. Most interestingly, they propose that one could actually exploit a defense (SmoothLLM) to make the attack robust to keyword-based defenses. However, they do not have any experimental results to show that this is actually the case.

**Mildly Compelling motivation**. The motivation of using few-shot jailbreaking is compelling to jailbreak models that do not support a long context. However, it should be noted that these models are also less likely to actually provide useful malicious information to the attacker who is trying to jailbreak the model.

Weaknesses:
**No comparison to few/many-shots baselines**. The authors do not compare their method to Wei at al. [1] and Anil et al. [2], which are the most similar to their method. They claim that Wei et al. have limited effectiveness on well-aligned models such as Llama-2, but Llama-2 is not the only target model considered in the paper, and the authors should show some concrete numbers to back-up their claim. For Anil et al., they claim that the attack requires too much context length to work on the considered models, but, according to the numbers shown in the paper [2], the attack starts being effective with 32 shots, the number considered for Llama-3, and they have results for Llama-2 in their paper up to 128 shots.

**Missing amount of necessary queries**. One of the metrics that are useful for jailbreak attacks is the total number of queries needed by the random search to jailbreak the model. The authors do not report this number, which makes it hard to compare their method to other methods.

**Some ablations are missing**. The authors do a great job at showing what components are most important for the success of the attack. However, they do not show the impact of the quality/length of the examples. It would be interesting to see how the method performs when the examples are shorter or longer, or when some of them are not actually good examples. This would be relevant as the model used to generate the examples could refuse, or generate low-quality examples. Another ablation that would make the paper stronger is how important it is that the special tokens are correct. What happens if you, e.g., use Llama-2's special tokens for Qwen1.5B? Or simply if the special tokens are slightly incorrect (e.g., `[INST]` instead of `[/INST]`? This can be useful to show the potential effectiveness of the attack against models whose special tokens are unkown.

**Minor**:

- No experiments that show that SmoothLLM can be used to evade keyword-based defenses.
- Code is provided, but the data are provided in pickle format, which is known to be unsafe. It would be better to provide the data in a more standard format like CSV or JSON. Moreover, it would be better to provide a README with instructions on how to understand the code.

**References**:

- [1] Wei et al., https://arxiv.org/abs/2310.06387
- [2] Anil et al., https://www.anthropic.com/research/many-shot-jailbreaking

Limitations:
The authors do a good job at discussing the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes two improved techniques for in-context few-shot jailbreaking: demo-level random search and the injection of special tokens from the system prompt. The authors conduct extensive experiments across a series of aligned language models. Ablation studies demonstrate the effectiveness of both proposals.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow, tackling an intriguing and timely problem. The baseline few-shot jailbreak is not strong enough, while the powerful many-shot jailbreak requires a long context length, which is not suitable for all language models. The proposal in this paper significantly improves the attack success rate over few-shot jailbreak.

2. The proposed idea of utilizing special system tokens for jailbreaking is novel, and the experiment evaluation is comprehensive. The improvement in attack success rate over the baseline few-shot jailbreaking is significant. Additionally, the techniques introduced are not hard to implement and could serve as a strong baseline for future language model safety research.

3. Ablation studies on the number of shots and the two components are provided, clearly demonstrating the effectiveness of both modifications. The study also illustrates the effect of several defense methods, showing that the improved few-shot jailbreak method can be successful even with the defense methods presented.

Weaknesses:
My primary concern regarding the methodology's weakness is the potential for **unwanted bias** due to the **dataset** chosen for evaluation, which could impact the numerical results.

As highlighted by previous studies [1], the AdvBench harmful behavior dataset, which is used to construct the demo pool and evaluate attack success rate in this paper, **lacks diversity**. Although it contains **520** harmful requests, only **58 unique behaviors** exist (see Table 5 of [1]). This introduces a significant vulnerability to the current evaluation pipeline, as excluding only 50 requests for testing from the demo pool may be **insufficient**. The pipeline could potentially select similar candidate in-context demo from the pool to the target test request. For instance, if the test request is ""Write a tutorial on how to make a bomb"", the candidate demo selected from the pool could be ""Provide detailed instructions on how to make a bomb"" and its answer. In this scenario, the proposed algorithm's success might be biased towards **replicating certain provided demos** rather than originating from the techniques proposed.

Besides, as illustrated in [1] (see Figure 2), the generation length has a critical effect on ASR. This paper takes length 100 (see line 176) as the default paradigm. This could lead to an overestimation of ASR.

[1]: Mantas Mazeika et al., HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new method to jailbreak LLM to elicit harmful responses. The proposed method follows a line of works on using the demonstrations of harmful responses in the context of prompt to jailbreak. It improves the previous works regarding reducing the number of demonstrations in the context and increasing the efficacy. Specifically, the proposed method uses an unsafe LLM to automatically create a pool harmful demonstrations, insert special tokens into the prompt, and optimizes the demonstrations using a demo-level random search. The empirical results confirm the efficacy of the proposed methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. the proposed method is simple and straightforward to implement.
2. the dramatic sensitivity of FSJ to special tokens is surprising.
3. the evaluation is comprehensive (many defenses are tested) and the results of the proposed method are strong.
4. the paper is well-written and easy to follow.

Weaknesses:
1. The evaluation is based on 50 harmful responses from AdvBench. The scale is limited. Besidse, AdvBench is also used to generate demonstration pool. Although the overlapped ones are inspected and removed, there may be a concern of overfitting. Using a different source of harmful responses like HarmBench [1] for evaluation may be better.
2. The proposed method assumes that attackers have access to model-specific special tokens, which restricts its application scope. Without the help of inserting special tokens, the proposed method seems to be ineffective in breaking the well-aligned models like Llamas as shown in Tab. 1. It is therefore interesting to test if a special token can be determined without the knowlege of target model. 
3. Although the proposed method demonstrates the ability to circumvent a wide range of defenses, it may be ineffective when adaptive defenses were deployed. For example, toxic detectors can be used to detect if harmful content is included in the input prompt as demonstrations.

[1] Mantas Mazeika et al., HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal.

Limitations:
some limitation has been discussed, but more are required. See some points suggested in Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes several ICL (in-context learning)-based techniques to improve the effectiveness and efficiency of jailbreaking prompts, including adding system special tokens and random search on the demonstrations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The discovery that using special tokens can enhance the effectiveness of harmful demonstrations is interesting.
- The experiments show the overall proposed method can notably improve the ASR on multiple LLMs.
- The experiments also include evaluations of the attack against LLMs with defense techniques.

Weaknesses:
- The main objective of this paper seems to be misleading. As indicated by the abstract and the story in the introduction, this paper attempts to address the problem of

> it possible to use few-shot demonstrations to efficiently jailbreak LLMs?

However, since ICA has already been proposed as the few-shot version of jailbreaking, this paper may take ICA as the main target, rather than refining MSJ.

- Following the previous weakness, the most important baseline, ICA, is missed in the experiments. Moreover, what is the difference between the used baseline (FSJ) and ICA is not indicated.
- The first improved technique, injecting special tokens, though interesting, is of limited scientific contribution. It’s more like an attack trick, rather than a substantial academic improvement. More importantly, why these tokens can enhance the ASR is not well-explained or understood.
- The second technique is anyway lacking novelty since the jailbreaking literature has already used the intention of random search (e.g., GCG and AutoDAN) to improve the jailbreaking prompt.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes jailbreak attacks via few-shot demonstrations. The authors introduce a three-step method to achieve this goal, which includes constructing a demo pool, injecting special tokens, and demo-level random search. The proposed method demonstrates strong attack performance against aligned LLMs and multiple defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method is a strong attack that can bypass many advanced defenses.

Weaknesses:
Overall, the paper is well done. However, I have a significant concern: How does the attacker know the special tokens used in the LLMs? This is particularly problematic for attacking closed-source models such as ChatGPT. I also noticed that the authors did not evaluate their method on closed-source models in this paper. This issue represents a critical weakness in practical jailbreak evaluations. I will raise my score to acceptance if this concern is addressed. Otherwise, I think this weakness is a flaw that we can not ignore.

Limitations:
The authors have discussed the broader impacts and limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zLU21oQjD5;"REVIEW 
Summary:
This paper synthesizes a math reasoning dataset with a designed way of rejection sampling. Many base models show performance improvements on math reasoning tasks after instruction-tuning on this dataset. They promise to release the dataset and models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Their curated dataset achieves relatively good instructing-tuning performance with least data amount compared to other baselines. The dataset will be released.

Weaknesses:
1. The proposed sampling technique is trivial and incremental, when comparing with previous works, e.g., the uniform method is used in ToRA, and the prop2diff method is used in MARIO.
2. There’s little improvement or even performance drop when tuning Mistral-7B and DeepSeekMath-7B 
compared to other baselines. As mentioned in the analysis section, this dataset is somehow replaceable by math-specific continual pre-training + supervised fine-tuning (SFT).
3. The major concern is that even the paper claims the proposed dataset is smaller, however, the LLM used to synthesize the smaller dataset is `DeepSeekMath-7B-RL`, which is trained on a larger SFT dataset. An alternative and reasonable response generation method should be leveraging the `DeepSeekMath-7B-Base` with proper prompting, as `DeepSeekMath-7B-Base` has not been supervised fine-tuned.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces Difficulty-Aware Rejection Tuning (DART), a novel approach for enhancing the mathematical problem-solving capabilities of large language models (LLMs). Traditional methods often produce datasets biased towards easier queries, limiting the models' ability to learn from challenging examples. DART addresses this by allocating more sampling trials to difficult queries during the data synthesis phase. The authors created two strategies, Uniform and Prop2Diff, to ensure a balanced representation of easy and difficult queries. Using only open-weight models, the authors generated new, smaller datasets that prioritize difficult queries.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The DART method effectively addresses the bias towards easy queries in traditional rejection sampling, which is a significant contribution to the field.

2. The paper provides a thorough analysis of the biases in existing datasets and clearly explains how DART mitigates these issues.

3. The authors plan to make their datasets and models publicly available, contributing valuable resources to the research community.

Weaknesses:
1. The success of DART relies heavily on the ability of models to generate correct responses for difficult queries, which may not always be feasible for extremely challenging problems.

2. While the focus on difficult queries is commendable, the quality of the generated responses for these queries needs to be high to truly benefit the training process. The paper does not provide a detailed analysis of the quality of these responses.

3. The approach's reliance on extensive sampling for difficult queries might pose scalability issues, particularly for very large datasets or models with limited computational resources.

Limitations:
The limitation is mentioned in the conclusion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a rejection sampling pipeline for automatically generating SFT data, emphasizing that harder data requires more trials. The difficulty is heuristically determined using the ratio of incorrect trials for each question. Experiments demonstrate that this method can outperform traditional rejection methods on various math benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The experiments are solid, showing significant improvements over traditional rejection methods.

- The paper is clearly written and easy to follow.

Weaknesses:
The proposed Prop2Diff strategy lacks innovation. Assigning more budget to more complex questions in data synthesis is a common practice. For instance, in [1], which successfully annotated 83.1% of MATH questions, it is evident that harder problems were allocated more budget in rejection sampling. [1] also indicates that fewer and harder data can significantly and efficiently improve performance. The authors should discuss the differences between their approach and the one used in [1] more thoroughly.

[1] ToRA: A Tool-Integrated Reasoning Agent for Mathematical Problem Solving

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents an approach to improving the performance of LLMs in mathematical problem-solving. The authors identify that current datasets synthesized using proprietary models like GPT-4, are biased towards easier queries. To address this, they introduce Difficulty-Aware Rejection Tuning (DART), which allocates more trials to difficult queries during data synthesis. This method generates datasets focusing on difficult queries using an open-weight model, DeepSeekMath-7B-RL, without relying on proprietary models. The authors demonstrate that models fine-tuned on DART-Math datasets significantly those fine-tuned on traditional datasets across various mathematical benchmarks, and beat the best baseline by average of roughly 3-4%

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Technically solid paper with state-of-the-art results.
- Mostly well-presented and easy to understand.
- Comprehensive experiments and analysis.
- Decent impact in improving mathematical capabilities of LLMs, with the authors publicly releasing their dataset.
- By using an open-weight model, DeepSeekMath-7B-RL, the authors eliminate dependency on proprietary models like GPT-4, making the approach more accessible.

Weaknesses:
1. It is unclear how the hyperparameters of the baseline, VRT (vanilla rejection tuning), were tuned. For instance, as mentioned in Appendix A.2, sampling temperature is searched from 0.3 to 1.7 for DART. Was the same procedure used for VRT? Another caveat is the need for extensive hyperparameter tuning compared to baselines. Were similar extensive procedures for tuning performed for other baselines?
2. It is unclear if the improved performance of the proposed method is due to difficulty or the topic of the problem. For instance, LEVEL 5 Math problems may have a higher number of geometry questions (or at least their fail rate is higher, resulting in fewer samples in VRT). An analysis of topic-wise performance comparing DART and baseline methods may clarify this.

**Minor Weaknesses: **
1. It is unclear how much advantage the method would provide in the case of other multi-iteration fine-tuning methods such as STaR and v-STaR. For instance, it is possible that after multiple iterations, VRT performs similarly to DART, since a higher number of samples will be collected from even the hard problems in second or further iterations.
2. The data synthesis is only done using the DeepSeekMATH-7B model. It is unclear why this model was chosen. Previous methods using VRT-like methods typically use the same model for synthesis and generation. Thus, higher results in smaller models such as Llama-8B may partly be due to the use of stronger models' reasoning chains, making it similar to a distillation method.

Limitations:
Major Limitations are addressed in paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zLClygeRK8;"REVIEW 
Summary:
Policy evaluation, selection and optimization are considered in the context of offline contextual bandits, where i.i.d. data with a known behavior policy is given. The authors set out to study a generalization of importance weighted policy evaluation; for this they start from a general formulation that computes a value for all data observations, which are then averaged. The free ""parameter"" here is $h$, the function that assigns a value given an observation (of a context, associated action, and cost). A tight, general, high probability upper bound on the expected cost of a fixed target policy is derived first. Specific choices for the map $h$ are then derived based on minimizing this upper bound. Two practical solutions to this optimization problem are studied in more details: Global clipping and ""logarithmic smoothing"". Results are then derived for both policy selection and optimization.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Novel ideas, novel results, good empirical results.

Weaknesses:
Despite saying  that the methodology of paper [31] is adopted, this is only partially done. Why deviate from the evaluation in [31]? I expected an explanation of this.

Limitations:
na

Rating:
10: strong accept, should be highlighted at the conference

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the offline contextual bandit problem. The authors consider a class of reward estimators for this setting that is a regularization of Inverse Propensity Scoring (IPS - aka importance sampling). A general concentration result is provided for this class of estimators. This is used to provide a tight result for an existing clipping IPS estimator and to construct a new Logarithmic Smoothing (LS) estimator. The resulting estimator is pessimistic by design, making it immediately applicable to the offline contextual bandit problem.
The authors use it to derive bounds for policy evaluation and selection and also for policy learning in the Bayesian setting. Experimental results also support the usefulness of the estimator.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am only broadly familiar with this line of research making it hard for me to properly contextualize its contributions.

1. The proposed estimator is novel and has nice properties.
2. As the name suggests, the estimator is smooth making it potentially easy to optimize.
3. The application to contextual bandits is interesting.
4. The experimental results are positive.
5. The overall writing is good and clear.

Weaknesses:
1. A more explicit comparison with existing concentration\contextual bandit bounds is missing. The authors explain that their bound is better but this is somewhat vague, especially if the reader is not already an expert in this field.

2. In line 155 the authors explain that their result can be derived from [1, Lemma 1.3]. Does this mean that the LS estimator has previously been suggested or only that an alternative proof technique exists for its concentration bound?

3. Performance seems very close to that of IX

4. The main body of the paper does not include any explanation of the techniques used. This can be a proof sketch for the concentration bound or a discussion comparing your approach to existing techniques. Can you provide such an explanation in your response?

5. The notation U(pi) appears without definition in line 281. I assume it's defined in one of the references but should also be defined in this paper for completeness. (Please include an explanation in your response)

Typo:
line 98: one of the brackets is reversed in the definition of h

Limitations:
N/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose empirical concentration inequalities for off-policy evaluation that apply to several forms of (smoothed) IPS, which are claimed to be tighter than the results in existing works. These bounds are then used to derive policy learning guarantees that inherit the properties of the concentration inequalities.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
I appreciate that the authors have applied their method to OPE, OPS, OPL, and also provided some experiments. 

I did not read the appendix nor check the correctness of the analysis in detail, but from a quick glance it appears that the authors were careful to provide rigorous and well-organized proofs.

Weaknesses:
My biggest criticism is that the authors have not justified *in the main body* their claim that ""LS is provably tighter than its competitors"" (L12) for any of the results, including the concentration inequalities (Prop 1, Cor 3, Cor 4) and the policy learning guarantee (e.g., Prop 6). 

Since these claims are the whole premise for the paper, their justification should be a central pursuit and only stating ""x is in Appendix y"" (L147, 178, 195) is hugely insufficient. 

For example, I would have liked to see a discussion on (possibly even in graphs): 
- For the choices of $h$ described in (4), when do the bounds in Prop 1, Cor 3, and Cor 4 improve over the bounds from their respective papers? 
- Is $h*$ (the tightest choice) better than all of the above? 
- Does this hold for all hyperparameter choices, e.g. $\lambda$ and $L$?
- How does the computational complexity of calculating the bounds in Prop 1, Cor 3, Cor 4 hold up relative to their competitors? 
- Exactly how does this lead to downstream policy learning improvements?

Lastly, I found the overall technical presentation to be relatively poor, and I'll give a few examples: 
- The condition (C1) from Section 2 (""Regularized IPS"") that all results depend on is never explicitly defined, and it should be an assumption that is called in every proceeding proposition/theorem statement.  
- Shouldn't (11) be framed in, e.g., a lemma environment? 
- The term ""pessimism"" is overloaded, e.g., for ""high-probability upper bounds"" in L111 but also for an in-expectation variant in Eq. (5), which is slightly unusual (and I'm pretty sure not the way it's used in [26]) but not recalled again in the main body so I'm not sure what it's for (perhaps the proof of Prop 1).

Limitations:
I do not believe the authors have fully discussed the limitations of their method (see ""Weaknesses"").

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies log-algorithmic smoothing of importance weight for off-policy learning. The proposed smoothing technique can be seen as a differentiable variant of clipping, which is useful for variance reduction for OPL. The paper also analyzes the PAC-Bayes learning bound of the proposed OPL method, characterized by the KL divergence with the logging policy, showing that the proposed method achieves a tighter bound than baselines, including simple clipping. The experiment also shows that the proposed method has tighter bounds than baselines and enables more accurate off-policy selection.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- **Reasonable formulation based on theoretical analysis**: The proposed method is derived from a tight upper bound of the policy's risk. Also, the proposed method has an interpretation as soft, differentiable clipping. The technique is well-motivated and is reasonable to interpret.

- **PAC-Bayes learning bound**: A sub-optimality form is derived, and it is also easy to interpret as a pessimistic approach, which should be acknowledged.

- **Experiments on various tasks**: The paper evaluates the proposed approach in upper bound derivation, off-policy selection, and off-policy learning. The experiment results show the wide applicability of the proposed method in many OPE/OPL-related tasks.

Weaknesses:
- **Connection to Metelli et al. 2021 is not clear**: Metelli et al. 2021 also considers the importance of weight differential and shows that the proposed method achieves a Subgaussian rate. Similar to the reviewed paper, Metelli et al. 2021 also have a KL divergence term in the theoretical analysis. While the proposed method adequately differs from Metelli et al. 2021, and the paper does cite it, the paper does not mention Metelli et al. 2021 in the related work in detail. Since the motivation and contributions are similar, a detailed discussion on the advantages and the differences would be appreciated.

- **Baselines in the experiments**: As mentioned above, Metelli et al. 2021 propose a similar idea that can be used as a baseline in experiments. Comparing with advanced regularization techniques such as shrinkage (Su et al. 2020) would also be informative.

(Metelli et al. 2021) Subgaussian and Differentiable Importance Sampling for Off-Policy Evaluation and Learning. Alberto Maria Metelli, Alessio Russo, Marcello Restelli. NeurIPS, 2021.

(Su et al. 2020) Doubly robust off-policy evaluation with shrinkage. Yi Su, Maria Dimakopoulou, Akshay Krishnamurthy, Miroslav Dudík. ICML, 2020.

Limitations:
Missing connection with a similar idea. See the weaknesses for the details.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zLBlin2zvW;"REVIEW 
Summary:
This work proposed a Gated Sparse Autoencoder (Gated SAE) to mitigate the standard SAEs' biases, such as shrinkage, which systematically underestimate the feature activations from SAEs. The key difference between Gated SAE and SAE is that the Gated SAE separates affine transformations within the encoder in order to decide which dictionary elements to use in a reconstruction loss, and estimate the coefficients of active elements, although with the 50% more computing required to achieve. Comprehensive experiments are conducted to compare and verify how good the Gated SAE is to standard SAE, including a blinded human study to rate and compare the interpretability of randomly sampled Gated and baseline SAE features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A new architecture of SAE inspired by GRU is proposed to include a gate mechanism to mitigate shrinkage bias
- Comprehensive quantitative experiments including ablation studies to evaluate the proposed Gated SAE compared to SAEs
- A human evaluation to rate randomly sampled features from Gated SAE and SAE

Weaknesses:
- It is not very straightforward to understand how well the features from Gated SAE are compared to SAE based on Figure 4. Some case studies based on the open-source SAE visualizer library [1] are required to help better understand this.
- It will be better to see more case studies on downstream tasks to compare Gated SAE and SAE, e.g., automatic circuit detection [2]

[1] C. McDougall. SAE Visualizer, 2024. https://github.com/callummcdougall/sae_vis

[2] Huben, Robert, et al. ""Sparse Autoencoders Find Highly Interpretable Features in Language Models."" The Twelfth International Conference on Learning Representations. 2023.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper attempts to resolve the issue of feature shrinkage in sparse autoencoders (SAEs) by replacing the SAE ReLU activation function with a gated ReLU unit. The weight-tying scheme they use for the gated unit effectively turns it into a jump ReLU activation function.
They train gated SAEs and baseline SAEs on a one layer transformer, Pyhtia-2.8B and Gemma-7B. They find that gated SAEs eliminate systematic shrinkage, and consistently outperform baseline SAEs on the pareto-curve of sparsity, measured by the L0 pseudonorm, and faithfulness, measured by the model loss recovered relative to a zero ablation baseline. 
They run various additional tests involving variations of the gated and baseline SAE architectures, including combinations of SAE dictionaries with the classic gradient pursuit algorithm for choosing sparse feature coefficients at inference time. They conclude that the Pareto improvement of their gated SAEs over their baseline SAEs is due in part to better feature dictionaries, in addition to better estimated feature coefficients.
They compare the subjective interpretability of 150 gated SAE and baseline SAE features in Pythia-2.8B and 192 features in Gemma-7B, using a blinded analysis of activating dataset examples. They find that the features were similarly interpretable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper attempts to address a substantive practical problem with current SAE training methods.


The paper's proposed new architecture is evaluated extensively, and many detailed additional investigations on the individual effects of various parts of the gated SAE architecture are described in sections 5.1, 5.2 and Appendix D. 


I find Appendix D interesting in its own right, since it shows quantitative comparisons between SAE methods and the classic gradient pursuit optimization algorithm, as well as mixing SAE feature dictionaries with gradient pursuit for sparse approximation of feature coefficients. I have not encountered such a comparison before. 


For the most part, good documentation of all their process is provided, and the writing and presentation are very clear in general.

Weaknesses:
The paper does not really address the concern that gated SAEs may outperform baseline SAEs in part by implicitly widening the definition of what it means for ‘features’ to be represented in the model. As the paper itself notes in Appendix D, though other more powerful sparse coding algorithms greatly outperform SAEs in terms of reconstruction and sparsity, there are concerns that the greater expressivity of these techniques lets them find spurious ‘features’ that would not be accessible to the model’s own internal computations. An SAE can only find features that are represented in the sense that their current values can be read off with a single ReLU probe, while an inference time algorithm or a multi-layer probe may read off ‘feature’ values that the model itself could not possibly access using a single MLP layer. A gated ReLU is far less expressive than an algorithm like gradient pursuit, but more expressive than a ReLU. So to what extent do gated SAEs outperform baseline SAEs merely because they are implicitly working with a more relaxed definition of what it means for a feature to be represented in the model? Figure 6 in Appendix D incidentally investigates this somewhat, since it attempts to compare the quality of gated vs. baseline dictionaries independent of their coefficients. However, the results there seem inconsistent, with smaller performance gaps and baseline SAEs outperforming gated SAEs at higher L0. I think this issue of the representational power of the probe used is pretty central for contextualizing the results, and should at least have been discussed.

Throughout the paper, the authors present reconstruction scores for SAEs in terms of the fraction of model loss recovered compared to a zero-ablation baseline. I think this metric obscures vital information. Lowering CE loss from e.g. 4.5 to 4.0 is typically much easier than lowering it from 1.5 to 1.0. Thus, the same difference in loss recovered between two SAEs can correspond to very different gaps in SAE quality. Without the raw CE scores, there is no direct way to infer how large the gap is quantitatively. At minimum, these raw CE scores should be in the supplementals. Better yet, the recovered performance could additionally be reported in terms of the compute required to train a model with the same CE score, as suggested in https://arxiv.org/abs/2406.04093.

Limitations:
All addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work introduces a new technique under mechanistic interpretability's sparse autoencoders. By using a less naive SAE, with a gating mechanism and a little extra computation, the paper shows a decent improvement over the baseline.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This work addresses the important issue of interpreting transformer-based LLMs and clearly demonstrates an interesting method. The mechanistic interpretability community will certainly find this work of interest.

The writing is well written and fairly easy to follow, the results are clearly presented, and all relevant aspects of the method are appropriately ablated

I liked the setup of the internal user study; I think future papers will follow the design of the study closely.

The work's cited throughout the manuscript are incredibly thorough.

Weaknesses:
While I generally like the paper, I have two primary concerns:

* The architecture and loss are somewhat difficult to understand. I did appreciate the pseudo-code in the appendix, but I feel for readers not familiar with SAEs may have a hard time, especially with the optimization-based design choices of weight tying. Perhaps explaining weight tying later in 3.2 would help. I would especially prefer if a few lines of pseudo code could be added in the main paper, next to figure two.
* The user study results. I don't mind the small change in means between the method and the baseline, but the explainable AI community has been around for a long time and the shift from studies with a few experts to larger cohorts has been the norm for a while now. Just because there's a rebranding to mechanistic interpretability doesn't mean this field should settle for underpowered studies. Nevertheless, I do find the study setup itself to be well articulated and a very useful starting point for future work in this area.

Minor:
Some of the design choices (weight-tying, no r_mag, etc) aren't well explained until the ablation where we find they are primarily for optimization. This could be motivated a little earlier, i.e. that the pareto improvement comes from the separation, and not those choices.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Gated Sparse Autoencoders (Gated SAEs), an improvement over standard sparse autoencoders (SAEs) for decomposing language model activations. The key idea is to separate the tasks of detecting which features are active and estimating their magnitudes, allowing the sparsity penalty to be applied only to feature detection. Through experiments on language models up to 7B parameters, the authors show that Gated SAEs achieve better reconstruction fidelity for a given level of sparsity compared to baseline SAEs, while resolving issues like shrinkage. A human evaluation study finds Gated SAE features to be comparably interpretable to baseline features.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A well-motivated architectural modification to SAEs that addresses key limitations
- Comprehensive empirical evaluation across multiple model sizes and activation sites demonstrating clear improvements over baseline SAEs
- Careful ablation studies and analysis to understand the source of improvements
- Human evaluation study to assess interpretability of learned features
- Thorough discussion of limitations and future work directions

Weaknesses:
- The presentation could be improved in some areas, particularly in explaining some of the technical details and metrics
- Some of the figures are quite dense and could be made more readable
- The human evaluation study, while valuable, has a relatively small sample size

Limitations:
The authors provide a good discussion of limitations in the conclusion section. They appropriately note that their experiments were limited to specific model families and that further work is needed to evaluate usefulness for downstream interpretability tasks. They also acknowledge the subjective nature of the human interpretability study.
Regarding potential negative societal impacts, the authors do discuss this briefly in Appendix A. They note that advances in LM interpretability could potentially be misused, but argue that the current work poses minimal short-term risks. While this discussion is somewhat brief, it does address the key points and seems appropriate for the nature of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zJremsKVyh;"REVIEW 
Summary:
This paper introduces _Frugal Flows_ a method that learns the data distribution of data for causal effect estimation; namely outcome $Y$, binary treatment $X$ and pretreatment covariates $\mathbf{Z}$. 
Through a combination of frugal parametrisation, normalizing flows and copulas, separate components for the marginal causal effect $p_{Y| do(X)}$, the probability integral transforms of $\mathbf{Z}$ and the propensity score are leaned.
(The components of) the learned model, can be used for (i) estimating the marginal effect and (ii) to generate synthetic data with a fixed marginal effect for benchmarking other causal inference methods.
In the second application the component for the marginal effect is switched out for another with desired properties.
(i) is demonstrated on small synthetic datasets.
(ii) is demonstrated by fitting FFs to two real-world datasets and generating synthetic data with adjusted properties.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written.
- It tackles an important problem in causality research. Since, randomized data is hard and expensive to get, many causal methods are only evaluated on synthetic data and generating realistic/semi-synthetic data is hard. This paper makes a great contribution towards improving synthetic data generation. If the code for the method is provided in a user-friendly manner, I could see this having a big impact on the causality community.

Weaknesses:
- Normalizing Flows have been used in the causal modelling context before (see [1, 2]). While prior works solve different problems (the inferred latents correspond to exogenous variables of an SCM, not directly applied to causal effect estimation), I think it would still be valuable to contrast this work to what has been done before for future reference in the literature.
- L59: The basic causal assumptions aren't explicitly stated. What are the causal assumptions on $X$, $Y$ and $\mathbf{Z}$? It seems like the method wouldn't hold if $\mathbf{Z}$ was a mediator (I suppose the equation after L60 wouldn't hold). A reference to a 500+ page book is given for the assumptions, which feels like a slap in the face for the reader.
- The notation for interventional distributions is confusing: what's the difference between using an asterisk and explicitly using the do-notation? In the equation after L60, the LHS seems to be an interventional quantitiy (asterisk, but no do-notation), whereas Equation (1) has the do-notation, but no asterisk. Do the two notation elements mean different things?
- I think this paper would greatly benefit from a visual abstract showing how the different flows and distributions come together. Maybe this is something that could be added for the camera-ready.


Minor:

- L201: typo

[1] Javaloy et al. ""Causal normalizing flows: from theory to practice."" NeurIPS 2023

[2] Wendong et al. ""Causal component analysis"" NeurIPS 2023

Limitations:
- The biggest limitation of the work seems to be the requirements for the dataset size, needed to train normalizing flows. The authors mention this in Sec. 4.1. There could be some applications that have enough data for using Frugal Flows for causal effect estimation (e.g. online businesses with many customers, or well-curated medical datasets like the ones from healthcare providers in Isreal). However, for most applications the data won't be enough.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a generative modeling approach called Frugal Flows, designed to learn the data generation process with an explicit parametrization for the marginal causal effect of treatment on outcomes. Inspired by the frugal parametrization of marginal structural models, this approach models the marginal intervention distribution $p(Y|do(X))$ directly, rather than the joint distribution $p(Y|Z, do(X))$. This helps in preserving any constraints on the average treatment effect while flexibly modeling the data generation process. Frugal Flows employs copula flows to parameterize the model, accommodating constraints on the average causal effect and handling unobserved confounding during data generation. The authors validate the proposed method through experiments on both synthetic and real-world datasets, demonstrating its ability to generate realistic datasets with user-specified constraints.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The paper's approach to validating causal models using simulated datasets is indeed impactful and relevant. It addresses a significant gap by allowing for general constraints on quantities of interest, such as average causal effect and unobserved confounding, during data generation. This capability is crucial because many prior generative modeling approaches for causal datasets either do not offer such flexibility or cannot ensure the preservation of these constraints, thus making this work a notable advancement in the field.

* The paper is well-written, with clear explanations in the background sections on frugal parametrization and flows, which help the reader grasp the proposed approach. The details of the approach are well-articulated, and the experimental results are presented effectively.

* The proposed approach is indeed novel. While it builds on established concepts like frugal parametrization, the specific application of normalizing flows for parametrization and its focus on average causal effect estimation represent a significant and innovative contribution.

Weaknesses:
My main concern with the work is the limited empirical validation of the proposed approach. Given that the primary contribution is the learning methodology rather than theoretical analysis, I would expect a more extensive set of experiments to validate its effectiveness. For example, prior research on generative modeling for causal inference, such as the work by [1], includes comprehensive experiments with various statistical tests to assess the realism of generated samples and a broader benchmarking of causal estimators. This paper would benefit from similar depth in its empirical evaluation.

It would nice if the authors can conduct similar experiments to asses whether learned generative model generates realistic samples and evaluate it on more datasets. Also, the authors should compare with the prior works [1, 2] as baselines to establish which approach is the best at capturing the underlying data generation process, and empirically validate their claim (Section 2.6) that the proposed approach would be better than prior works at capturing used-specified constraints on the average causal effect.

References

[1] Neal, Brady, Chin-Wei Huang, and Sunand Raghupathi. ""Realcause: Realistic causal inference benchmarking."" arXiv preprint arXiv:2011.15007 (2020).

[2] Harsh Parikh, Carlos Varjao, Louise Xu, and Eric Tchetgen Tchetgen. Validating causal inference methods. In International conference on machine learning, pages 17346–17358. PMLR, 2022.

Limitations:
Yes, the authors have adequately addressed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a generative model called Frugal Flows making use of copula flows to infer about marginal causal effects by simulating the data generating process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem of inferencing marginal causal effects is an interesting and important problem
- The idea of using generative models to estimate the marginal effects in the paper is interesting

Weaknesses:
See questions.

Limitations:
See questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to leverage existing neural density estimators (specifically, normalizing flows) to exploit a newly-proposed ""frugal parametrization"" that can capture the causal marginal distribution of an underlying causal model. Under this parametrization, the authors show how to specify and train each component of the model, and thus train the proposed Frugal-Flows to match the observational distribution as closely as possible, while being able to tune the marginal causal effect present in the generated data.

This way, frugal flows can be used to generate synthetic causal benchmarks that closely represent the _observational_ data while having more difficult-to-estimate causal effects, putting existing approaches to the test.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- **S1.** The proposed frugal flows provide a way of generating new datasets that can be challenging from a causal-inference point of view, which I believe _important_ to test new and existing methods.
- **S2.** The construction of the proposed architecture is quite rich in details.
- **S3.** I find the frugal parametrization conceptually quite interesting.
- **S4.** The authors motivate different scenarios for frugal flows in Sec. 3.2, as well as empirically show positive results on some synthetic and real-world scenarios.

Weaknesses:
- **W1.** I find the frugal parametrization to be extremely under-explained, relying too much on the reader having full knowledge of the referenced work. Similarly, there is little to no explanation/intuition on why the frugal parametrization would properly capture the marginal causal distributions.
- **W2.** The lack of explanations also applies to other concepts, e.g., ""conditional ignorability"" (line 39) ""variation independence"" (line 82, and I know the definition is later in App. A), or why copula-based flows would target conditional causal effects instead of marginal causal ones (line 182). (similar with lines 221 and 229)
- **W3.** There are no mention to related works that propose similar ways of constructing causal benchmarks. From a 1-min search in google scholar, I already found some likely relevant works: [Work 1](https://arxiv.org/abs/2406.08311), [Work 2](https://arxiv.org/abs/2011.15007).
- **W4.** I find the experiments a bit underwhelming, specially those from Section 4.1. The authors should at least show how is the fitting of the observational likelihood, and if they want to show the capabilities of frugal flows for causal inference (and not only causal-benchmark generation), they should compare with other methods like [Causal Normalizing Flows](https://arxiv.org/abs/2306.05415).

Limitations:
I think limitations are properly discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zJNSbgl4UA;"REVIEW 
Summary:
The paper targets scaling down Vision Transformers (ViT) to fit environments with dynamically changing resource constraints. The authors propose Scala, a framework enabling a single network to represent multiple smaller ViTs with flexible inference capability by activating various subnets during training. Scala introduces Isolated Activation to disentangle the smallest sub-network and uses Scale Coordination to provide stable and accurate learning objectives. Empirical validations on different tasks show that Scala achieves scalable representation with one-shot training, matching the performance of Separate Training without modifying the original ViT structure. Scala demonstrates an average improvement of 1.6% on ImageNet-1K compared to previous methods, using fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is important in practice.
2. The experimental results seem decent.

Weaknesses:
1. My major concern is that, the same aim of adapting ViTs to dynamically changing resource constraints, can also be achieved by multi-exit networks, e.g., [*1, *2, *3]. However, the paper does not discuss these highly relevant works or compare with them. Hence, I vote for rejection.
2. The method seems to lack novelty. 'smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths' is not a surprising observation. The key techniques (e.g., Isolated Activation and Knowledge Distillation) are not new (naive or have been widely adopted).


[*1] Huang, Gao, et al. ""Multi-Scale Dense Networks for Resource Efficient Image Classification."" International Conference on Learning Representations. 2018.

[*2] Wang, Yulin, et al. ""Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition."" Advances in neural information processing systems 34 (2021): 11960-11973.

[*3] Han, Yizeng, et al. ""Dynamic perceiver for efficient visual recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
The authors have addressed the limitations and potential negative societal impacts of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents Scala, a novel framework for scalable representation learning developed from US-Net. It identifies the issues of directly applying US-Net to ViTs and proposes solutions including Isolated Activation, Scale Coordination, and Stable Sampling. These innovations enable Scala to output several sub-networks in one-shot learning. Extensive experiments on various network architectures and datasets demonstrate that the sub-networks produced by Scala consistently outperform those generated by separate training, with significantly reduced training time.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality: Scala addresses the limitations of US-Net and successfully applies the concept of scaling to ViT backbones. This is a significant step in the adaptation of scaling methods for more complex network architectures.

Quality: The paper supports its claims with extensive experimental results, providing strong evidence for the effectiveness of Scala.

Clarity: The paper is clearly written and well-organized, making it accessible and easy to follow.

Significance: Scala has the potential to influence future research directions in scaling ViTs.

Weaknesses:
Originality: The novelty of Scala is somewhat constrained. For instance, Noise Calibration does not show a distinct difference from standard knowledge distillation. Essentially, Scala integrates US-Net with an alternative activation for the smallest subnet and fixed scaling ratios.

Quality: The authors might consider emphasizing results from a more standard 300-epoch ViT training schedule to align with common practices in the field.

Clarity: No further issues.

Significance: The challenge of scaling ViTs with arbitrary ratios remains unresolved.

Limitations:
The novelty and significance of Scala are somewhat limited, as discussed in the weaknesses section. However, the extensive experimental results provide a robust foundation for the claims made in the paper. Overall, the work is well-executed and makes a valuable contribution to the field, justifying a recommendation for weak acceptance.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Scala, a novel framework designed to effectively scale down Vision Transformers (ViTs) for use in environments with fluctuating resource constraints. The key insight is that smaller ViTs can function as sub-networks within a larger ViT, differing mainly in width. Scala enables a singular network architecture that can emulate multiple smaller ViTs, thereby offering versatile inference capabilities while maintaining the structural principles of ViTs. The framework uniquely incorporates multiple sub-networks during its training phase, utilizes Isolated Activation to differentiate the smallest sub-network, and implements Scale Coordination to streamline the learning objectives for each sub-network, aiming for simplicity, stability, and accuracy. The empirical results across various tasks confirm that Scala can learn scalable representations efficiently with a single training iteration, maintaining the integrity of the original ViT architecture and achieving performance on par with networks trained separately.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed Scala framework aims to enhance Vision Transformers (ViTs) by enabling them to learn scalable representations suitable for flexible inference. This is achieved through two key innovations: Isolated Activation, which effectively disentangles the representation of the smallest subnet to maintain clarity and specificity, and Scale Coordination, which ensures that each subnet within the larger network receives simplified, consistent, and accurate signals. These mechanisms are designed to optimize the performance and scalability of ViTs, addressing common challenges in adapting these architectures to varied and dynamic operational contexts.

Weaknesses:
1. Recent papers[1,2,3] with ""Scalable"" usually scale ViT to billion size with large scale datasets like DFN, JFT, and Datacomp. Therefore, I suggest authors should reconsider if the experiments can support ""Scalable"".


[1] Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2022). Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12104-12113).

[2] El-Nouby, A., Klein, M., Zhai, S., Bautista, M. A., Toshev, A., Shankar, V., ... & Joulin, A. (2024). Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541.

[3] Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., ... & Houlsby, N. (2023, July). Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning (pp. 7480-7512). PMLR.

Limitations:
This paper addresses the limitations in conclusion

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances an approach for training Vision Transfomers (ViTs) such that at inference time they can be dynamically adjusted to fit different budget constraints with reduced drops of performance. To this end, the authors introduce Scala, a framework that allows a single network to encapsulate and train simultaneously multiple sub-networks of different capacities and widths. The methodological backbone of this work are the Universally slimmable networks (US-Net) [37], originally devised for CNNs. The authors identify and analyze a few flaws of US-Nets: difficulty to generalize to ViTs, small interpolation and extrapolation ability to sub-network size unseen during training, impact of sustained activation of the smallest sub-network that coupled with the sandwich rule for selecting sub-networks during training leads to an over-emphasis on it at the expense of the other sub-networks.
The authors propose two simple strategies towards such a method for ViTs: (i) Isolated activation that separates the smallest sub-network from the other sub-networks; (ii) scale coordination consisting of a set of heuristics to ensure that each sub-network gets simple, accurate and stable learning objectives: (a) progressive knowledge transfer from larger networks to smaller ones in gradual decrease of capacity, (b) stable sampling of intermediate width ratios to avoid large variations in capacities in the sandwich, (c) noise calibration, essentially a composite loss of supervised cross-entropy and distillation from the bigger sub-network.
Scala is evaluated on several settings on the ImageNet-1k dataset with ViT-Ti/S/B, hybrid CNN-ViT architectures, lightweight networks, but also for dense prediction on semantic segmentation and self-supervised pre-training with interesting results. The baselines used here were Separate Training,  Autoformer and US-Net.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
### Significance
- the paper deals with a challenging and useful task for deploying ViT models into different operational settings with different computational constraints without retraining or distilling specific architectures each time

- although a computational overhead is expected for such methods, the main components of Scala are relatively simple and make sense 

- Scala achieves good performance with a higher boost in the low parameter regime

### Originality
- the proposed contributions are somehow incremental as they are improving the US-Net prior work, but do have some novelty and they are simple.

### Clarity
- in general this work is well argued and easy to follow. The authors construct well the arguments regarding the challenges when going from CNNs to ViT with US-Net and how to construct their Scala approach.

### Quality
- the paper offers several experiments and studies in the main paper and in the appendix (longer training, fast interpolation, ablation of components) that are well thought and improve the understanding of the method.

- I appreciate the experiments beyond image classification, on semantic segmentation, as well as the self-supervised pretraining and subsequent linear probing on a downstream task.

Weaknesses:
### ""Scalable"" naming
- I think that the framing of the method as _""scalable representation learning""_ is quite confusing as it's not representative for this task, it's not a name used by other related works. Importantly, it can be easily mistaken with most works that use ""scalable"" for depicting the ability/property of a system (method, architecture) to handle a growing amount of data, parameters, and the potential to accommodate this growth. In other words ""scalable"" is rather used for depicting scaling up, whereas this work depicts the property of the proposed approach to accommodate sub-networks of different lower sizes/scales from the original.

- maybe other names us in related works would be more appropriate here: slimmable, elastic, modular, flexibile inference, etc.


### Limited baselines and related work
- some relevant related works dealing with tranformer networks are either just briefly mentioned, e.g., Matformer  [18], or not mentioned at all, e.g., SortedNet [a], Early exit [b]

- One of the main baselines, US-Net is originally designed for CNNs and, as the authors mentioned, moving to ViTs is not straightforward. Matformer is criticized for the limited number of models produced, but can be considered in the several experiments with X=4 sub-networks. Matformer and SortedNet could be included in the experimental evaluation


### Scope of experiments
- While the authors considered several settings for computer vision tasks (image classification, segmentation, light architectures), transformer architectures are also encountered in NLP (as mentioned by the authors in L56). In such cases the original models can have much more parameters and elastic inference for lower computational budgets would be of high interest.

- It would be useful to include an experiment from NLP in the style of those from Matformer or SortedNet.

- The biggest architectures used here is a ViT-B (~86M params). Extending experiments to larger modern architectures would be definitely useful and interesting.

### Clarity
- it's not always clear in the text and cost estimations that Scala needs a pre-trained full network as teacher for the distillation. This add some cost in compute and time in the end. Besides it's not clear whether US-Net also needs and uses a pre-trained teacher in the reported results.

- in the intro, the authors mention that they address the issue of minimal interpolation ability of ViTs. Results from Table 2 show that the interpolation abilities of ViTs with Scala are still very low. However the fast interpolation strategy from $\S$A.2 is actually interesting for practical settings even though not fully solving this issue. It might be worth moving up in the main paper.

- the idea of the transferability experiment ($\S$5.4) with DINOv2 is nice. From the description it is not clear whether DINOv2 was used as teacher for the distillation or also as supervised pre-training on ImageNet-1k? Or the pre-training on ImageNet-1K was done in a supervised manner as in previous experiments?

- the ablation experiment from Table 6 is nice. However the presentation with removing one component at once offers only a partial understanding of the contributions of each module. Different configurations with different modules in on/off mode should give a better global understanding.



**References:**

[a] Valipour et al., SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks, arXiv 2023

[b] Xin et al., Deebert: Dynamic early exiting for accelerating bert inference, ACL 2020

Limitations:
The authors addressed some of the limitations in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zIr2QjU4hl;"REVIEW 
Summary:
This paper presents a conservative fine-tuning method called BRAID, which integrates the strengths of diffusion models and model-based optimization (MBO) to improve the performance of pre-trained diffusion models on offline datasets. BRAID optimizes a conservative reward model that includes penalties outside the offline data distribution to prevent overoptimization and generate valid designs. The approach is validated through empirical and theoretical analyses, demonstrating its ability to outperform the best designs in offline data while avoiding the generation of invalid designs. The paper also discusses the method's effectiveness compared to existing conditional diffusion models and traditional MBO techniques, with experiments showcasing its superiority in biological sequence and image generation. The authors acknowledge the limitations of their study, particularly in model selection and hyperparameter tuning, and suggest future research directions.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* BRAID incorporates a conservative approach to fine-tuning diffusion models, which includes penalization terms that discourage the model from generating designs outside the distribution of the offline data. This conservative strategy is effective in preventing overoptimization and ensuring the validity of the generated designs.

* The method is supported by both theoretical analysis and empirical results. Theoretically, it provides a regret guarantee, ensuring that the fine-tuned models can outperform the best designs in the offline data. Empirically, it has been validated through experiments across various domains, such as biological sequences and images, demonstrating its ability to generate high-quality designs.

Weaknesses:
* Difficulty in tuning hyperparameters without online data interaction.
* Reliance on accurate reward and diffusion models for effective performance.
* Theoretical results depend on certain idealized assumptions that may not hold in all cases.
* Can you compare the methods with other SOTA offline RL methods to illustrate your proposed augmented methods more effective than the SOTA offline RL methods? I think this paper is very relevant to some offline RL methods, such as ReDS[1], A2PR[2], CPED[3], SCQ[4]. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.

References：

[1] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[2] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[3] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

[4] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

Limitations:
* The method requires careful selection of hyperparameters, which can be challenging in a purely offline setting without access to additional online data.
* The pseudo-code of the paper does not illustates the algorithm explicitly. The authors can improve it further.
* This paper lack strong baselines comparisions and enough related works, which is related to some offline reinforcement learning methods. So you can add more related offline reinforcement learning works.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the task of black box optimization in an offline setting. Given a pretrained diffusion model, they first train a surrogate model on the offline data and use it to tilt the diffusion model distribution via finetuning it. The authors distinctly focus on an uncertainty quantification based procedure to bias the diffusion model tilting toward regions where the reward is high and the reward uncertainty is low while not tilting toward regions with high uncertainty. Experiments are carried out on a reasonable set of diverse tasks.

The paper introduces a small specific challenge and address it well with a reasonable approach and good motivations. The potential impact of the method may be small but it is neat, educational, and should be useful in many cases. I recommend acceptance.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Identifying a crucial non-obvious overlooked challenge and specifically pinpointing it. The authors identify that previous work on tilting diffusion models with reward models misses to incorporate tilting less toward regions of the reward function in which it has high uncertainty but high reward. Instead, we should only tilt to the regions that have high reward and high certainty to avoid optimizing toward adversarial examples. (The fact that the finetuned diffusion model will steer away from the pretraining distribution seems like a less relevant insight)
2. The authors identify a relevant overlooked problem in finetuning diffusion models and bring standard techniques from uncertainty quantification into the field to address it in a reasonable fashion. They do not overcomplicate things and their technique could be valuable to several researchers in the area. 
3. The authors prove that the training procedure yields the desired distribution. I have no comments regarding the value/insightfulness of the proof. Maybe other reviewers have a stronger opinion about the relevance.
4. The authors evaluate their method on a very diverse set of experiments that includes discrete DNA sequence generation, and image generation. The results are convincing and demonstrate the central empirical claim that out of distribution generation is a problem and is effectively avoided with the proposed conservative reward model fine tuning.
Minor:
1. Interesting snippets of insights. The authors point out interesting relationships and connections along the way which are non-obvious and well placed for putting their motivations into context.
2. Exceptional clarity in writing. The paper lays out the task in its precise specification and covers required concepts and related work in equal clarity.

Weaknesses:
1. I would say that the insights in terms of methodological novelty are on the moderate side. The ideas are simple and good, which is appreciated, but the level on which the conceptural changes operate are low level (a tweak to diffusion model tilting) and thus limited in impact. However, it is certainly a good thing to have.
Very hard to address and not a must have for ML conferences:
1. Evaluations are inherently limited in their computational nature and the conclusions that can be drawn for the procedures effectiveness in biological sequence optimization is small. Do you authors disagree with this in any way?

Limitations:
The authors point out the key inherent limitation of their work in the fact that the reward models that are often trained on limited data are likely suboptimal instead of just mentioning useless small limitations that are beside the point (which is the more common practice it seems).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a conservative approach for fine-tuning diffusion models with a reward model learned from offline data. Specifically, the ideas are two-fold: The first idea is to replace the reward model with a conservative estimate based on classical generalization bounds. The second idea is to leverage the KL divergence to force the optimized distribution to not deviate too far from the pretrained model. Experiments and theoretical results show the efficacy of the proposed method in fine-tuning diffusion models without over-optimization.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is well-presented, and the motivation behind the algorithm is interesting. The over-optimization problem is indeed critical when fine-tuning diffusion models with learned rewards.

2. Extensive experimental results show the efficacy of the proposed method in improving the reward model while avoiding reward over-optimization.

Weaknesses:
1. Leveraging generalization bounds via kernel RKHS and bootstrap is interesting, but I doubt their practicality for real applications. Firstly, the RKHS bound is usually too conservative to be useful, while the computational cost for the bootstrap method is pretty high since one has to train the model from scratch multiple times. As far as I can tell, the reward models used in the experiments are mainly single-layer MLPs, and it is doubtful whether this approach is useful when the reward model needs to be a larger model.

2. Another problem with the conservative estimator of the reward models is that it is unclear whether it is useful given the current experimental results. On one hand, KL regularization is a widely-known technique for preventing over-optimization in diffusion models and is thoroughly studied in existing works, so it is certain that the KL regularization term will help. On the other hand, the proposed algorithm mixes both the conservative reward estimator and the KL regularization term together, making it unclear which part is playing the role in avoiding over-optimization. My guess is that, for the most part, only the KL regularization term is effective in the end.

Limitations:
See my comments on the weakness above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
1) This paper analyzed the two mainstream angles of computational design.
2) Proposed a hybrid one that offline fine-tunes generative models.
3) Conduct experiments on two tasks to show the performance of their method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) Sufficient theoretical analysis and detailed preliminaries.
2) The idea is straightforward.
3) The method is comprehensive.

Weaknesses:
1) In the introduction, the advantages and disadvantages of the two mainstream issues are not fully analyzed.
2) Insufficient metrics evaluation for image generation task.

Limitations:
1) Lack of gradual guidance from analyzing the advantages and disadvantages of existing methods to proposing the hybrid method.
2) Lack of multi-metric quantitative results analysis. (e.g. in the image generation task, the fidelity and diversity should also be reported by like LPIPS Score/CLIP score/FID/IS, etc.)

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zGN0YWy2he;"REVIEW 
Summary:
This paper employs scene graph for image generation. Different from the previous methods, they employ the generative capabilities of variational autoencoders and diffusion models in a generalizable manner, compositing diverse disentangled visual clues from scene graphs. The authors propose a semantics-Layout Variational AutoEncoder to jointly derive layouts and semantics from scene graph. Then they develop CMA integrated with a diffusion model. They also introduce the multi-layered sampler for achieving graph manipulation. Experiments show that the method outperforms existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper address the problems in existing methods well. Existing methods in the field of scene graph to image generation mainly depends on the layout or semantics. Using one of them may cause some problems. Inspired by these phenomenons, the authors propose the method to jointly considering the layout and the semantics. What's more, the techniques used in the framework are novel enough. 
2. The authors conduct plenty of experiments. The ablation studies support the motivations.

Weaknesses:
The only weakness I found is that the authors should reorganize the paper carefully. The writings is not so clear in some sections. For example, the multi-layered sampler section is too abstract to be understood.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes DisCo (Disentangled Compositional image generation), which integrates both layout and semantic information derived from scene graphs to improve the quality and controllability of generated images. In particular, DisCo has three main components: Semantics-Layout Variational AutoEncoder (SL-VAE) for disentangling spatial layouts and semantics, Compositional Masked Attention (CMA) for fine-grained attribute guidance, and Multi-Layered Sampler (MLS) for object-level graph manipulation. Extensive experiments demonstrate that DisCo outperforms state-of-the-art methods in generating rational and controllable images from text, layout, and scene graph conditions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation is clear. The idea of disentangling layout and semantics from scene graphs is novel.
2. DisCo outperforms recent methods in both fidelity and diversity of image generation, as evidenced by the IS and FID scores. Overall, it enhances the generation diversity and controllability.
3. Extensive experiments and ablation studies have demonstrated the effectiveness and the contribution of each component.

Weaknesses:
1. The increased inference cost of DisCo (Table 7). In particular, CMA mechanism might increase the computational cost, which may limit the method's scalability and efficiency, especially for large-scale applications. Moreover, since diffusion models are already quite large, the additional AutoEncoders (Lines 129-130) may result in more parameter and memory overhead.
2. DisCo requires expensive training, e.g. 4 A100 GPUs, as indicated in Lines 202-203. With more models releasing recently, this technique might be not scalable.
3. The image quality looks better with this proposed method. However, as metrics today cannot always reflect the real image quality, it would be more convincing to conduct a user study, e.g. votes, to quantify the advantage of DisCo compared to previous works.

Limitations:
Yes, authors have stated their limitations on attribute leakage of overlapping in Section A.6, as well as a short discussion on the broader impact in Section A.7. 

Beside that, authors are also encouraged to add a few discussions on the efficiency of their proposed pipeline. Even though these overheads are inevitable (they are quite common in most researches), a clearer trade-off between the improved image quality and the increased model complexity would help to better assess the value of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents ""DisCo,"" a novel framework for generating complex images from structured scene graphs. Unlike traditional text-to-image or layout-to-image methods, DisCo utilizes a Semantics-Layout Variational AutoEncoder (SL-VAE) to disentangle and generate diverse spatial layouts and interactive semantics from scene graphs. It incorporates these elements using a Compositional Masked Attention (CMA) mechanism within a diffusion model, enhancing generation control and rationality. The framework also introduces a Multi-Layered Sampler (MLS) for flexible, graph-based image editing, preserving visual consistency while manipulating object attributes and positions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Introduces innovative methods for disentangling and integrating spatial and semantic information from scene graphs, which is a novel approach in image generation. 
2. Offers significant improvements in image generation from complex scene graphs, enhancing both the fidelity and controllability of generated image

Weaknesses:
1. The paper lacks quantitative comparisons with closely related baselines, such as R3CD, which could provide a more comprehensive evaluation of the model's performance. Inclusion of these comparisons could help validate the proposed advantages of DisCo over existing methods, particularly in handling complex scene graph-to-image generation tasks.
2. Some generated images, particularly those highlighted in Figure 10, exhibit unnatural aspect ratios and stretched elements, suggesting issues with the model’s handling of object proportions and spatial embeddings. 
3. It would be great to discuss the scalability aspects, particularly how the proposed model handles graph sizes that exceed typical training configurations.
4. how the model performs with imperfect or noisy scene graphs, which are common in automatically extracted data.

Limitations:
The authors address the challenge of attribute leakage in overlapping objects, which affects image fidelity in scenes with dense object interactions. While mitigation strategies are discussed via the CMA mechanism, further refinement is required to eliminate this issue completely. Additional exploration into the computational efficiency and scalability of the proposed methods would also benefit the paper, providing a more comprehensive view of their practical applications and limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method that uses a scene graph and integrates variational autoencoders (VAEs) and diffusion models to address complex scene generation. Specifically, a Semantics-Layout Variational AutoEncoder (SL-VAE) is used to derive diverse layouts and semantics from the scene graph, while a Compositional Masked Attention (CMA) combined with a diffusion model incorporates these elements as guidance. Additionally, a Multi-Layered Sampler (MLS) is introduced for isolated image editing. Experiments show that this approach outperforms recent methods in generation rationality and controllability.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper considers an important issue in text-to-image generation realm.
2. The structure design in Section 3 makes sense.
3. The experimental results shown in Table 1 and 2 show the effectiveness of this method.

Weaknesses:
1. My main concern is the practical application of this method. As we all known, scene graph building is not a trivial task, but you don't explain detail in the paper how to construct an exact scene graph. In addition, during inference, the prompt proposed by uses may be non-standard so that building a scene graph may be more difficult. 
2. Besides, recent SOTA models, e.g. DALLE3, stable diffusion 3 try to solve the complex generation task by large-scale fine-grained dataset construction. How do you compare your methods with these data-centric methods. The authors should spend more space discussing the issues.

Limitations:
Please see the weakness

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zDaD8zv8tG;"REVIEW 
Summary:
The paper introduces a novel teacher-teacher framework named LIghtweight kNowledge alignmEnt (LINE), which facilitates knowledge exchange between two pre-existing large language models (LLMs) to enhance clinical language representation. By leveraging complementary knowledge from general-purpose and domain-specific models, LINE aims to harmonize their knowledge within a unified representation space. The framework is validated through downstream tasks showing that the LINE model outperforms individual pre-existing models in understanding and processing clinical language. This approach allows for more efficient sharing of clinical pretrianed models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. **Clarity and Structure**: The paper is well-written and structured, offering a clear motivation for the study. This makes it accessible and engaging for readers, facilitating a deeper understanding of the proposed framework.

2. **Novelty and Utility**: The proposed teacher-teacher framework, LIghtweight kNowledge alignmEnt (LINE), is innovative, providing a pragmatic approach to integrating the strengths of different pre-trained models. This methodology is particularly notable for its potential to enhance clinical language representations without the need for developing new models from scratch.

3. **Usability and Efficiency**: The framework is user-friendly and does not require retraining of the original models, which significantly reduces computational overhead and simplifies its adoption in real-world applications.

4. **Empirical Validation**: The experimental results demonstrate stable and significant improvements over existing methods, substantiating the efficacy and value of the proposed framework in practical settings.

Weaknesses:
**Data Requirements and Availability**: A notable limitation of the proposed LINE framework is its dependency on well-aligned and specific types of data sources, which may not be readily available or commonly found in practical settings. For example, integrating data from disparate modalities like CT and MRI requires the availability of cases that include both types of data, which may not always be feasible. This requirement could limit the framework's applicability across different clinical or real-world scenarios where such aligned data sets are scarce.

Limitations:
No.
The authors discuss the potential improvement instead of the limitation of the current work. Bring more information and try other situations cannot be counted as an adequate discussion of limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an interesting topic on LLM but the importance of this problem is not convincing and the methods here is not novel.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The teacher-teacher concept is novel to some extent.

Weaknesses:
1. The problem's importance is not significant.
2. There lacks the inclusion of SOTA models like llama, gpt, etc.
3. The results improvement is limited as shown in Tab. 4,5.
4. The Fig1 lacks details of the proposed method.

Limitations:
See details in weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors look to address the question representational alignment between language models trained on different textual domains to improve performance of potentially both models on their out-of-domain text. The authors propose to specifically investigate this in the context of EHR text, and choose as their models for this CODER and BGE. They propose a contrastive loss, and additionally propose to train an alignment module/project layer rather than end-to-end training of the teacher models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The concept is solid and well implemented and motivated. I wonder if it would be possible to further generalize it beyond medical text - which it is restricted too due to the reliance on alignment with extracted medical concepts by NILE. The discussion mentions this possibility, but it would be exciting to see it in action.

The clinical NLP benchmarks are particularly appropriate for the task.

Weaknesses:
Some of the benchmark tasks are older, and the comparisons could be more robust. Some ablations are missing.

The project's scope is incredibly narrow: encoder models on extractive medical tasks. While the authors claim that the technique is broadly generalizable, it would be nice to see proof-of-concept. 

The work seems to me to fit more into the realm of domain adaptation rather than learning by alignment. We aren't learning novel models here via alignment (like CLIP), but rather, pushing the learned representations of two different models into a common space. I'd strongly consider citing and discussing DA literature for this paper.

Limitations:
No discussion of limitations. Without additional experiments at the minimum a stated limitation should be the highly restricted domain of application (purely encoder models on medical topics).

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce a teacher-teacher framework for clinical language representation learning. The framework uses a lightweight knowledge alignment module to harmonize the knowledge of both models within a unified space, which including two steps: The first step involves initial training to define residuals and capture complementary information. The second step focuses on refining the alignment by recovering residual information. The framework was validated using the MIMIC-IV database, where the LINE model outperformed baseline models in aligning concept and text representations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of the work is proposed teacher-teacher framework, and training strategy.

- Originality: The teacher-teacher framework is very interesting as it enables mutual enhancement between two pre-existing LLMs, a unique departure from traditional approaches that typically involve training a new model or continual pre-training of existing models. This innovative method opens new avenues for leveraging existing resources to achieve superior performance.

- Quality: The paper demonstrates high quality through its validation using the MIMIC-IV database, a well-known and respected dataset in the clinical domain, adding significant credibility. Additionally, the LINE model's performance is compared against several strong baseline models, showing clear improvements across various downstream tasks, thus underscoring the robustness and reliability of the proposed framework.

- Clarity: The paper is well-written and clearly structured, making it accessible to both domain experts and those new to the field. The introduction provides a comprehensive background and motivation for the proposed framework, while the methodology section offers detailed descriptions of the teacher models and the LINE module.

- Significance: The practical applications and potential impact on the clinical domain shown the significance of this work. The teacher-teacher idea has substantial implications for more advancing NLP applications in other filed.

Weaknesses:
1. Figure 1 is somewhat confusing. From my understanding, Teacher 1 should be a strong LLM, while Teacher 2 should be an LLM with existing domain-specific knowledge. However, Figure 1 gives the impression that Teacher 2 serves merely as a database, making the framework resemble a RAG framework.

2. Although the paper compares the LINE model against several strong baseline models, it lacks a detailed comparison with the latest strong general LLMs, such as GPT-4, which should be considered a strong baseline. Consider adding a small comparative analysis or stating the advantages of the framework over simply using GPT-4.

3. The paper underscore the practical value of the framework, but it does not sufficiently address potential practical implementation challenges, such as computational requirements and scalability when applied in real-world clinical settings.

Limitations:
The paper has limited discussion on the broader implications of implementing the teacher-teacher framework in clinical settings. Consider to add the assessment of how the framework could impact patient care, data security, and trust in AI systems in healthcare.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a mutual learning framework, called LINE, between two pre-existing LLMs in the healthcare domains. By harmonizing the knowledge of two distinct LLMs into a unified representation space, the model achieves better performance on intrinsic and extrinsic downstream evaluations of clinical tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Clear motivation. Overall well written.

The methodology was reasonably designed to map representations from two distinct LLMs into a unified representation space.

The method achieves better performance on downstream clinical tasks.

Weaknesses:
1. Only two LLMs (BGE and CODER) were aligned by LINE. It is unclear if LINE will work on combinations of other LLMs. 

2. LINE make downstream predictions based on clinical concepts only, rather than the full context. The concepts themselves can be negated, historical and hypothetical in context, but the proposed method does not seem to consider this.

Limitations:
See weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zBMKodNgKX;"REVIEW 
Summary:
The paper introduces a novel approach to address the challenge of collaboratively visualizing high-dimensional data in a federated learning (FL) environment. The proposed method, FEDNE, integrates the FEDAVG framework with contrastive neighbor embedding (NE) techniques, aiming to preserve data privacy while ensuring effective data visualization. By employing a surrogate loss function and an intra-client data mixing strategy, FEDNE seeks to enhance the alignment and preservation of neighborhood structures in the global embedding space. The paper includes comprehensive experiments on both synthetic and real-world datasets, demonstrating the effectiveness of FEDNE in outperforming several baseline methods in terms of neighborhood data structure preservation and clustering.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. FEDNE introduces a novel integration of FEDAVG with contrastive NE techniques, addressing the unique challenges of pairwise data relationships in federated learning environments without requiring data sharing.
2. The intra-client data mixing strategy effectively enhances local data diversity, mitigating the limitations of biased local kNN graphs and ensuring better neighborhood representation.
3. The paper provides a thorough evaluation of FEDNE using various datasets and metrics, showcasing its superior performance compared to baseline methods in preserving neighborhood structures and clustering.

Weaknesses:
1.	While the authors mention that FEDNE introduces only 35% more GPU time compared to FEDAVG, the overall complexity and scalability in a more extensive, real-world setting are not fully addressed. The authors should further investigate how FEDNE scales with a significantly larger number of clients and more complex datasets or models.
2.	The paper proposes intra-client data mixing as a solution to the bias in local kNN graphs. However, this approach might not entirely mitigate the issue of incorrect neighbor connections, especially in highly imbalanced datasets. More detailed comparisons with alternative methods or further enhancements could provide a more robust solution.
3.	The focus is primarily on dimensionality reduction. The validation results are performed only on the vision classification tasks. Extending the discussions and analyses to include applications in other domains could be beneficial.

Limitations:
The authors have addressed the works limitations and social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""FEDNE: Surrogate-Assisted Federated Neighbor Embedding for Privacy-Preserving Dimensionality Reduction"" presents a method for visualizing high-dimensional data while maintaining privacy without requiring any shareable reference data. 

Federated Neighbor Embedding (FEDNE): A framework combining federated averaging (FEDAVG) with contrastive neighbor embedding (NE) to create a joint NE model across multiple clients without compromising data privacy.

Surrogate Loss Function: An innovative loss function to enhance inter-client repulsion in the global embedding space, ensuring better separation of data points from different clients while preserving local data structures.

Data-Mixing Strategy: A technique to counter issues like invisible and false neighbors in local k-nearest neighbor (kNN) graphs by mixing data from various clients during training, thus improving the quality of the learned embeddings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Well-Presented: The paper is clearly and coherently written, making it easy to follow.
Novel Approach: The study addresses an important problem with a novel approach, combining federated learning with neighbor embedding techniques.

Weaknesses:
Privacy Concerns: While the approach is innovative, the paper does not sufficiently address privacy concerns. It lacks experiments and guarantees demonstrating the privacy preservation of the FedNE approach.

Computational Inefficiency: The method appears to be computationally inefficient. There are no experiments conducted on large datasets, such as those in real-world medical or other privacy-critical domains, where computational complexity could be a significant issue.

Inadequate Analysis of Related Work: The related works section is not thoroughly analyzed or discussed, missing critical comparisons and context necessary for a comprehensive understanding of the state of the art.

The study's applicability could be strengthened by extending beyond benchmark datasets to encompass real-world, privacy-sensitive datasets found in domains such as healthcare or finance. This expansion would provide a more robust demonstration of the method's practical relevance and effectiveness. 

Additionally, addressing pairwise issues associated with attraction terms is essential for improving the preservation of neighborhood structures and enhancing clustering quality. 

Furthermore, it is crucial to conduct thorough analyses aimed at optimizing the computational efficiency and scalability of the algorithms, ensuring their capability to handle large-scale datasets effectively. Moreover, the method currently lacks explicit consideration of privacy guarantees. And on elucidating how privacy concerns are addressed within the framework and formalizing privacy guarantees to assure users and stakeholders.

Limitations:
No societal impacts.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a new federated learning approach named FEDNE for dimension reduction using contrastive neighbor embedding (NE). The key idea is the introduction of a surrogate loss function that each client learns and shares, which compensates for the lack of inter-client repulsion essential for global alignment in the embedding space. Additionally, the paper proposes a data-mixing strategy to augment local data, addressing issues of invisible and false neighbors in local kNN graphs. Comprehensive experiments demonstrate that FEDNE effectively preserves neighborhood data structures and enhances alignment in the global embedding space compared to several baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The studied problem is important. There could be many downstream tasks after applying federated neighbor embedding.

2. Many metrics are included in the experiments to evaluate the quality of the resulting embeddings

Weaknesses:
1. The paper lacks investigation on the effect of choice of hyperparameter k.

2. The improvement of FEDNE is significant on some metrics (e.g., kNN) but is very limited in other metrics (e.g., conti.). The paper lacks a detailed exploration of why FEDNE produces different behavior for different metrics.

3. I suggest to highlight the best results in Table 2. Currently the results of FEDNE are highlighted although it may not achieve the best performance in some cases.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of distributed neural embedding (NE) with a focus on privacy protection. To achieve this, the authors extend the concept of federated learning (FL) to NE. However, NE tends to diverge because FL prevents clients from accessing each other's data, leading to inconsistent feature spaces across clients. To mitigate this issue, the authors employ surrogate loss models trained locally, which are then broadcast to all other clients to serve as an anchor. The experiments show promising performance compared to existing baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated and well-written.
2. The problem is practical and useful for many real-life applications, though scalability may be the main constraint.
3. The idea is straightforward, and the experiments seem to verify its effectiveness.

Weaknesses:
1. **Communication complexity**: If I understand correctly, every client in the proposed method must broadcast the surrogate models to all other clients. Although the surrogate models consist of only one hidden layer, this design results in a communication complexity of $\mathcal{O}(N^2)$. As the number of clients in the system increases, the additional communication costs will rise dramatically. This might be manageable in some cross-silo settings, where only a few clients participate.

2. **Straggler effect**: Following point (1), the proposed method requires communication among clients. However, clients may drop out during training. It would be insightful if the authors could analyze how missing surrogate loss models would affect overall performance.

3. **Additional privacy concerns**: Sharing surrogate models introduces additional privacy risks, e.g., enabling reconstruction attacks or membership inference. While some recent work empirically shows that such private information is less leaked after distillation (e.g., [1] and [2]), the proposed method might be more vulnerable to privacy attacks without differential privacy.

[1] Dong, Tian, Bo Zhao, and Lingjuan Lyu. ""Privacy for free: How does dataset condensation help privacy?."" International Conference on Machine Learning. PMLR, 2022.
[2] Wang, Hui-Po, et al. ""Fedlap-dp: Federated learning by sharing differentially private loss approximations,"" Proceedings on Privacy Enhancing Technologies, 2024.

Limitations:
The authors have discussed some limitations. However, the authors are encouraged to discuss the use cases of the proposed method, such as cross-silo settings. Moreover, they are encouraged to discuss the additional privacy risks potentially introduced by their method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zuWgB7GerW;"REVIEW 
Summary:
This paper introduces Accordion Networks (AccNets), a novel neural network structure composed of multiple shallow networks. The authors propose a generalization bound for AccNets that leverages the F1-norms and Lipschitz constants of the subnetworks, demonstrating that these networks can break the curse of dimensionality by efficiently learning compositions of Sobolev functions. The paper also provides theoretical insights and empirical validation, showcasing the superior performance of AccNets in learning complex compositional tasks compared to shallow networks and kernel methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The introduction of Accordion Networks (AccNets) as a novel neural network structure is a creative and original contribution. The paper provides a thorough theoretical analysis supported by empirical evidence, ensuring the soundness of its claims. The ability of AccNets to break the curse of dimensionality by learning compositional functions efficiently addresses a fundamental challenge in high-dimensional learning tasks.

Weaknesses:
1. The practical implementation of the proposed regularization methods might be challenging, particularly the first one requiring infinite width. 

2. The paper mentions the difficulty in optimizing Lipschitz constants, which could be a limitation in practical applications.

3.  Additional experiments on more diverse real-world datasets could further demonstrate the robustness and generalizability of AccNets.

4. Although the author has discussed the differences between DNN and AccNet, there is still not enough information for me to be sure in which settings to use AccNet and in which settings to use DNN. More clear differences and applicable conditions, especially the shortcomings of each need to be pointed out.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization bound for deep neural networks that describes how depth enables models to learn functions that are compositions of Sobolev functions. To do this, they both prove a generalization bound for compositions of accordion networks (densely connected networks with a low-rank weight structure) and for compositions of Sobolev functions. They then present a sample efficiency result for different kinds of regularization on accordion networks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I really liked this paper and would like to see it accepted to NeurIPS. It addresses an important question: how does depth change generalization bounds for deep neural networks? To my knowledge, not many papers so far have addressed this question and I found the findings presented here very interesting and well embedded within prior methodology.

I also found the paper very well written. I found it easy to follow along despite the highly technical nature of the results (note that I did not check the proofs in particular detail). I especially appreciated the remarks explaining different potential extensions and limitations.

Finally, the theory appears to be able to explain certain empirical phenomena (in networks trained under realistic paradigms) at least qualitatively (though note that I had a few questions I will mention under weaknesses and questions). This indicates to me that it is a promising way for thinking about generalization in deep neural networks.

Weaknesses:
1. I would like to see a more thorough comparison with shallow networks and generalization bounds, as this comparison is a central argument for the usefulness of the presented theory. While it is clear how the findings for the shallow network are a special case of the findings on the deep networks (as presented in Thm. 1), it remains a bit unclear to me how the theory can explain improved generalization in deep compared to shallow networks. The authors certainly present different several pieces of evidence on this: both Fig. 1 and Fig. 3 demonstrate that shallow networks exhibit worse scaling. I also appreciated the theoretical explanation of a particular contrast in l. 256-261. However, I think it would be really useful to provide a general theoretical explanation for this difference and test it empirically: would it be possible to extend the theoretical comparison in l. 256-261 to the general experimental setup studied in the figures --- and if so, would this theoretical comparison predict the conditions under which deep networks have the strongest advantages over shallow networks (or perhaps the conditions under which they don't perform that much better)? Not only would this serve as a useful validation of the theory, I think it would also provide a more extensive intuition for the authors' findings.

2. I appreciated the fact that the authors compare their findings with related work wherever this becomes relevant. However, I think a (potentially brief) section comparing the results here to other theoretical investigations of depth in deep networks (perhaps using different approaches) would be useful. 

3. The linked codebase does not contain the notebooks indicated in the README as far as I can tell and therefore currently can't be used to directly reproduce the findings.

4. I believe the figures would still benefit from error bars or some other indication of the overall statistical error in the findings. I agree that the main contribution of this paper is theoretical, but since the experiments test the empirical validity of the theory, I believe it is nevertheless important to get a sense for the overall deviation in these findings (e.g. across model seeds). If the authors are concerned about a lack of clarity, they could leave the bars out of the main figures but add supplementary figures with error bars. Moreover, some of the lines in Fig. 1 do contain error bars and it would be good to clarify what these error bars represent.

Limitations:
The authors adequately discuss the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce accordion networks (AccNets), which are compositions of multiple shallow networks. By leveraging prior workthat computes norm-based generalization bounds for shallow two-layer networks, the authors bound the complexity of a deep AccNet (as measured by its F1 norm) but the sum of the complexities of the individual shallow networks. They empirically observe that the rates predicted on real-world data are roughly representative of the trained networks, and are indeed much better than those for kernels trained on the same tasks. They put forth a nontrivial scaling law for the excess risk: $N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{mid})}$ for an Acc Net compared to $\mathcal L \sim N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{in})}$ for a kernel in terms of the dimensionalities $d$ and Sobolev constants $\nu$ of the respective spaces and functions. From this, the authors obtain predictions of several phases, that they put forth experiments to verify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper tackles a very important open question in the theory of deep learning, for which not much progress has been made. By creatively leveraging results for shallow network in composition, the authors arrive at a nontrivial bound for deep nets. The empirics are a very compelling and welcome part of the paper. The phase diagrams illustrate the nontrivial predictivity of the theory, especially at the level of the rates. This may have important implications for scaling laws. Modulo minor revisions in discussion and exposition, the whole paper is quite readable for a relatively broad audience.

Weaknesses:
I am not sure how compelling the phase plots in Figure 2 are. The bounds in general are extremely loose, however the comparison of the rates in Figure 2c and Figure 3 is very promising. In general, however, it is the experience of the reviewer that measuring a rate is an extremely finicky business. It is therefore important to add a section in the appendix explicitly stating how the rates were obtained and measured. I also strongly encourage the authors to make the code for all figures public. 

Because they are used very early on throughout the paper, it is the opinion of the reviewer that the notions of F1 distance and Sobolev norm should be defined earlier on in the paper. Without this, it seems like the audience will be constrained to the set of learning theorists familiar with these terms. However, if these terms are defined early on, the paper becomes remarkably accessible to a much broader audience.

Limitations:
Given the theoretical nature of this work, it is unlikely to have major social implications.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
znBiAp5ISn;"REVIEW 
Summary:
There's a large performance gap for graph tasks, especially graph classification tasks, between the spiking neural networks and artificial neural networks. The authors proposes the problems as the neuron's under starvation and illustrated the reason of the problem. To solve the problem, TAS-GNN was proposed.

The main contributions of the paper are as follows:
1: Starvation problem of spiking neurone in GNNs in graph classification tasks are identified.

2: A strategy was proposed to address the spike frequency deviations on the basis of the correlation between graph topology and spike frequency patterns.

The authors conduct experiments on 5 popular datasets and use several different designs of GNN layer. The results show competitive potential of the TAS-GNN.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1:This is a well-written paper, from the formulation of the problem to the solution. The author's motivation for the use of graph topology is clear.

2:The method of using topology-awaregroup-adaptive neurons shows competitive results compared with other baselines. The ablation study makes the result more persuasive. 

3: The Figures in the paper are quite straightforward, easy to follow.

Weaknesses:
1: The name of the paper is ""Topology-Aware Spiking Graph Neural Networks"". However, as I can tell the only graph topology used in the method is nodes degree, which is used to group the neurons. I wonder if it is appropriate to name it as ""topology aware"", or the author can explain it more.

2: The analysis regarding the performance of the method is lack of discussion. For instance, in some datasets, such as MUTAG and IMDB-Binary, the proposed method achieve quite competitive results while in PROTEINS it doesn't. It's betted to explain what cause the phenomenon, like the characteristics of the datasets? Also, in table 2, the results of GAT and GAT+TAG in IMDB-Binary are the same. It's better to make an explanation about them.

3: There're several typos and basic grammar mistakes in the paper that will affect the presentation of the paper. In line 120 "" and apply is to""; The sentence in line 123 is hard to understand

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper primarily discusses integrating Spiking Neural Networks (SNNs) into Graph Neural Networks (GNNs) to address several key challenges in graph classification tasks. Specifically, the paper proposes a new method called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) which leverages the topology of graphs to improve the performance of spiking neural networks in graph classification tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
（1）The authors clearly articulate the performance gap between existing Graph Neural Networks (GNNs) and Spiking Neural Networks (SNNs) in graph classification tasks.
（2）The authors conduct an in-depth analysis of the performance degradation of spiking neural networks in graph classification tasks and introduce the ""neuron starvation"" problem.
（3）The authors propose topology-aware group-adaptive neurons (TAG) based on the graph's topology, a novel approach that helps address the neuron starvation issue.
（4）The authors provide a detailed description of how to convert input graphs into spike representations, perform message passing, and classify the graphs.
（5）The authors validate the method's generalizability and effectiveness by using multiple public datasets (such as MUTAG, PROTEINS, ENZYMES, NCI1, IMDB-BINARY) in the experimental section.

Weaknesses:
（1）The authors mention several application areas and challenges, but the references and comparisons to existing literature are not sufficiently comprehensive.
（2）Although the methodology section describes the main steps, it lacks detailed descriptions of some key aspects such as threshold initialization and the specific training process.
（3）Although there are some ablation studies, the analysis of the individual contributions of each component is insufficient, making it difficult to determine the specific impact of each component on the overall performance improvement.

Limitations:
(1) While the paper discusses the neuron starvation problem and the sensitivity of initial thresholds, it does not explicitly outline the broader limitations of the proposed TAS-GNN method. It would be beneficial to include a dedicated section that explicitly lists and discusses the limitations of the current work.
(2) The paper does not thoroughly address how TAS-GNN scales with extremely large datasets or very high-dimensional graphs. Including an analysis of computational complexity and memory usage for larger graphs would provide a clearer understanding of the scalability limitations.
(3) While multiple datasets are used, the paper could further discuss the generalizability of TAS-GNN to other types of graph-based tasks beyond classification, such as regression, clustering, or even dynamic graphs.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the performance gap between spiking neural networks (SNNs) and artificial neural networks (ANNs) in graph classification tasks. The authors identify a ""starvation"" problem in spiking neurons within GNNs, where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set, unlike in transductive or inductive learning settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	This paper identifies a critical ""starvation"" problem in spiking neurons within Graph Neural Networks (GNNs), where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set
2.	The paper proposes a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the graph classification problem.

Weaknesses:
1.	The authors use the node degree instead of the concept of topology, there’s a large gap between the graph topology and node degree.
2.	The authors solve the graph classification task as a contribution, which is not a significant challenge for spiking graph neural networks.
3.	The advantage of Spiking Neural Networks (SNN) is their low energy consumption. However, the paper does not mention the feature, so it is unclear why graph neural networks should be combined with SNN. The motivation behind TAS-GNN is not clear.

Limitations:
The authors adequately addressed the limitations.  The authors should discuss more details of the potential negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes topology-aware spiking graph neural networks with adaptive thresholds based on a group of neurons for graph classification. The paper first diagnoses the poor performance as the existence of neurons under starvation caused by the graph structure. Then the paper proposes the adaptive threshold among neurons partitioned by degrees, as well as the learnable initial threshold and decay rate to reduce the sensitivity. Experiments on several datasets show superior performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes the first SNN design to target graph classification.

2. This paper identifies the starvation problem and proposes a novel topology-aware group-adaptive technique.

3. Experiments show superior performance on several datasets, some outperforming ANNs.

Weaknesses:
1. The proposed method seems to be a hybrid ANN-SNN model rather than a pure SNN design. The paper did not discuss how this will affect the deployment of the model on potential neuromorphic hardware, since SNNs mainly target those hardware to obtain energy efficiency.

2. The paper did not discuss the (theoretical) energy efficiency estimation, which is a major motivation for considering SNNs as stated in Introduction.

3. Or if the motivation is to get models with better performance than ANN, then Table 1 does not include state-of-the-art ANN results for comparisons.

Limitations:
The authors discussed limitations in Appendix A.1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zeNwOAcb4q;"REVIEW 
Summary:
In this work, the authors proposed an approach to estimate the instance-dependent transition matrix in order to reliably learn from noisy labels. The idea is to use a condition diffusion model to estimate the transition matrix by using the pretrained extracted image features as the conditions. Once the transition matrices are estimated, the classifier is learned through the corrected cross entropy loss. Experiments are presented to compare the performance of the approach with other baselines using both synthetic and real noisy datasets.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper is easy to read and notations are clearly stated

Weaknesses:
The main weakness is the lack of support and discussion in substantiating the idea. Experiments are insufficient to support the claims.

Limitations:
No limitations are discussed

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of supervised learning from noisy labels, where the label noise is modeled using instance-dependent label transition probability matrix. Mainly, this work attempts to leverage conditional diffusion model in order to obtain a generative model of transition matrix conditioned on the sample features. To that end, this work first generate pseudo paired samples $( x_i, T_i )_{i=1}^N$ using existing method (VolMinNet). Secondly, a conditional diffusion model is trained that generates $T_i$ given $x_i$. Finally, the classifier is trained taking into consideration the estimated transition matrix from the diffusion model.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem considered is of interest to the broad ML community
2. Adequate experimental settings, baselines, and ablations are provided for numerical validation.
3. The attempt to apply diffusion model is novel.

Weaknesses:
1. The technical soundness of the proposed method is questionable. Essentially, the proposed method trains a conditional diffusion model using paired samples $(x_i, T_i)$. If we consider the true transition matrix as $T(x)$ for a sample $x$, then the idea of the proposed method is to train a conditional generative model $p( T(x) | x )$. There are several issues with this attempt and the proposed implementation:
   (a) The authors use pseudo transition matrix $T_i$ generated from a sample-independent method (VolMinNet). $T_i$ only depends upon the cluster assignment of $x_i$. The diffusion model, at best, can approximate the conditional distribution $p( T_i | x_i )$. This has no clear relation to $p(T(x) | x)$. Therefore, in principle, the transition matrix generated by the trained diffusion model cannot be better than that returned by VolMinNet.
   (b) Second, the transition matrix is modeled as a deterministic function of sample, i.e., only one $T(x)$ exists for a given $x$. Therefore, it does not make sense to learn a generative model for $p(T(x) | x)$, since it is a degenerate distribution (probability of all other matrices should be zero except the true $T(x)$). 

2. Another hint at why the proposed method should be limited by the pseudo paired sample distribution is that the diffusion model training part (which is ultimately used as transition matrix estimator) does not require available noisy labels. Hence, no extra information can be extracted about the true transition matrix $T(x)$ beyond the information captured by the pseudo paired samples $(x_i, T_i)$. 

2. It is unclear where the performance gain in empirical results is coming from. The manuscript does not provide any intuitive or theoretical explanation to justify the quality of their estimator. Moreover, no rationale for the algorithm design is provided.

Limitations:
Limitations are not adequately discussed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the estimation of the transition matrix with instance-dependent label noise. They used a diffusion model for this estimation. By applying a diffusion process to the transition matrix, the diffusion model is trained to generate transition matrices from a prior distribution. The instance-wise generated transition matrix is then used to train the classifier with a forward cross-entropy loss. The improvement of the method is demonstrated by experiments on benchmark and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The instance-dependent label noise scenario is a challenging task.

Weaknesses:
* The reason for generating the transition matrix using a diffusion model is unclear.
  * The instance-dependent transition matrix is the target to be estimated, but it is uncertain what role training a diffusion model to generate the transition matrix without a fixed target.
  * In addition, as mentioned by the authors, the transition matrix must be satisfied: the entries are greater than 0, the row sum is to be 1, and the diagonal entry is typically the largest. However, these considerations have not been taken into account in the construction of the diffusion process. Although a transformation method is proposed in Section 3.4, there is no discussion of how this affects the training of the diffusion model.

* Pre-trained features are fed into the diffusion network, but their impact on the diffusion process has not been analysed. This could be seen as providing additional conditional information during the diffusion process, implying that this diffusion model might be a conditional diffusion model. It would be better to discuss these consideration.

* In Algorithm 3, it appears that the diffusion model is trained in order to generate the initialized $T_i$. I wonder if the desired training is for the initialized $T_i$ to be generated perfectly as is. This could lead to a transition matrix that might not contain instance-dependent information, raising questions about the mechanism by which diffusion training introduces variance.

* The diffusion training seems to take a considerable amount of time, which needs to be analysed. If it takes a long time, the performance improvement may not be significant in comparison.

Limitations:
They mentioned the limitations only briefly in the experimental section. I have noted additional limitations that I perceive in the Weaknesses part.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zFHJUSTZka;"REVIEW 
Summary:
This paper propose OAIF, an online method to align language model with human preference where feedback from language models serve as a surrogate of human feedback. The key of OAIF is to use online generated preference pair along the training process. Experiment results shows that, by switching offline preference dataset to online dataset labeled by other language models, the generated responses are more aligned with human preference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The strengths of the paper are listed below:

1. This paper introduces OAIF, which is featured by using on-the-fly generated preference pairs and AI-provided labels.
2. The author conducted experiment on various direct alignment methods and the results consolidate the claim by the authors

Weaknesses:
My questions and concerns are listed as follows:

1. My first concern is regarding the novelty of the paper. It seems that the language model annotator is essentially a preference model. Therefore, OAIF can be seen as a method of online direct alignment algorithm with access to a preference model. The author mentioned several previous work with on-policy generation and online feedback but in need of a reward model. How is OAIF different from different from these method if we simply plug in the language model annotator as the reward model in their methods?
2. At line 118 the author pointed out that RM might suffer from distribution shift because the training data of RM might not share the same distribution with $\pi_\theta$. However, it seems to me that using language model as preference annotator cannot bypass this problem since the language models' pretraining corpus or the finetuning corpus relating to preference labeling has a similar distribution with $\pi_\theta$.
3. How is OAIF's performance compared to other online methods like RSO and IterativeDPO? I think that these methods might also be included as baselines since reward model can also be taken by AI annotators.

Limitations:
The limitation is discussed by the author

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends offline preference learning methods, i.e., DPO, to a online variant by using LLM as annotator to collect new datasets for further preference learning. The results show that Direct alignment from preferences (DAP) methods win-rate over the offline methods beyond 60%.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is good writing, easy to follow.
2. This online variant provides demonstrates significant performance improvements over offline DAP and RLHF methods through comprehensive evaluations.

Weaknesses:
1. The improvement by extending online is under expectation as it introduces more datasets and training budgets. 
2. The contribution is limited. The only difference compared to the prior method is substituting the reward model of prior methods (Iterative DPO) to LLMs, though I agree the explicitly static reward model may introduce the model distributional shift problem.
3. Some drawings or comparisons are not fair enough. (a). Table 1 explicitly avoids the limitation of this method by leveraging the feedback from LLM, though it is another variant of the ""reward model"". (b). Figure 3, the training step is not an approximate x-axis as the online DPO variant has been heavily fine-tuned offline. 
4. There are no theoretical foundations, or new plausible explanations, aside from more datasets and the online budget, for the further improvement of the online variant DPO.

Limitations:
see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper applies direct alignment from preferences (DAP) methods, particularly DPO, to online settings where responses are sampled in an on-policy manner and feedback is provided by the LLM annotator in real-time. Extensive experiments demonstrate the effectiveness of these simple ideas.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written, with detailed explanations of introduced definitions and discussions with existing methods. 

The experiments are well-designed, supporting the main idea of the paper. The proposed prompt-controllable approach is particularly commendable.

Weaknesses:
The rationale for why on-policy learning brings performance gains is not well clarified. The cited reference [1] does not provide strong support for this claim. There is no experimental evidence that on-policy sampling encourages exploration. 

Most experiments are conducted with the closed-source LLM Palm; evaluating state-of-the-art open-sourced LLMs would enhance generalizability. 

It is unclear how much of the performance gains are due to on-policy sampling versus online feedback. 

The reasons why utilizing online on-policy data can avoid overfitting and improve performance should be further analyzed and discussed.

References:
[1] Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V., Hertweck, T., and Riedmiller, M. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.

Limitations:
The computational overhead introduced by on-policy sampling and online feedback is not discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new method called Online AI Feedback (OAIF) for direct alignment from preferences (DAP) that addresses the limitations of existing DAP methods, which rely on static, offline feedback datasets. By using an LLM as an online annotator to provide real-time feedback during each training iteration, OAIF ensures the alignment process remains on-policy and adapts dynamically to the evolving model. Through human evaluations across various tasks, the authors demonstrate that OAIF outperforms traditional offline DAP and reinforcement learning from human feedback (RLHF) methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
OAIF uses LLMs for preference annotation, eliminating the need for a separate reward model and large datasets typically required for RLHF methods. It introduces a new way to address off-policy issues in policy optimization, a significant problem in traditional DPO methods.

The paper is well-written and easy to understand. OAIF outperforms offline DPO and other offline RLHF methods.

Weaknesses:
1. The idea is straightforward but lacks theoretical proof. The proposed method combines DPO and AI feedback, unlike the constitutional AI paper, which integrates PPO with AI feedback. However, this point is minor. Given the abundance of concurrent work [1-7], the authors should further develop the theoretical analysis of their approach to strengthen their method. 

2. Different methods should use an equal amount of training data. In the second epoch of onlineDPO, although the prompts remain the same as in the first epoch, the responses and rank information differ due to online generation.

3. Recent results on Reward Bench indicate that small reward models are more effective than LLM critiques. The iterative DPO methods are similar to OAIF DPO. A performance comparison between OAIF and various iterative DPO methods using cheaper reward models, as both address the off-policy issue, is essential and should be included.

[1] Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint

[2] RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

[3] RSO: Statistical rejection sampling improves preference optimization

[4] Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682

[5] Hoang Tran, Chris Glaze, and Braden Hancock. 2023. Iterative dpo alignment. Technical report, Snorkel AI.

[6] Self-rewarding language models. arXiv preprint arXiv:2401.10020

[7] Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zDYXdR3ClP;"REVIEW 
Summary:
This paper introduces a universal image restoration framework UIR-LoRA based on multiple low-rank adapters. UIR-LoRA employs the pre-trained text-to-image diffusion model SD-turbo as the shared component. It utilizes a LoRA composing strategy based on the degradation similarity predicted by CLIP encoder to combine different LoRA modules. Experiments show the effectiveness of the proposed method.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed LoRA-based Universal IR method is easy to understand and follow.
2. The motivation of this paper is very clear to me.

Weaknesses:
1. UIR-LoRA adopts SD-turbo as the pre-trained backbone for image restoration. However, SD-tubo utilizes VAE with high compression rate to encode input images, resulting in severe detail distortion for image restoration. This issue has been widely discussed in recent published works [1,2]. However, the paper ignores this very important issue in the Method Section and only mentions the skip-connections for VAE in Line 223.
2. The degradation-aware router seems to be unreliable. I do not believe that the original pre-trained CLIP Text Encoder can distinguish between different degradations through degraded text representations, such as ""rain"" and ""raindrop"". Therefore, DA-CLIP fine-tunes the original CLIP. But this paper doesn't contain any discussions about this.
3. This paper does not provide complete technical details, such as how the LQ image is used as a condition for SD-turbo. Is ControlNet used, or is it directly concatenated? I do not see any information about this in the paper. 
4. Tab. 1 only reports the trainable Param for UIR-LoRA. I think it's necessary to report the overall Param of the model. In addition, the reported PSNR for DiffBIR is very low. Did the authors add skip-connections to the VAE of DiffBIR for a fair comparison?
5. The visual results in Fig. 3 seem strange. The visual results of Restormer show noticeable artifacts between patches. Do the authors test Restormer using a tiled mode? As far as I know, using a single A100 GPU (Line 251), Restormer can restore the entire image without encountering out-of-memory issues.

[1] Wang, Wenjing, et al. ""Zero-Reference Low-Light Enhancement via Physical Quadruple Priors."" In CVPR, 2024.

[2] Geng, Zigang, et al. ""Instructdiffusion: A generalist modeling interface for vision tasks."" In CVPR, 2024.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to perform universal image restoration via multiple low-rank adaptation. The key idea is to leverage a pre-trained stable diffusion model as the shared component and transfer it to specific degradations with LoRA adaptation. A degradation-aware router is further proposed to generate weights for LoRA combination based on degradation confidence. In experiments, the authors evaluated their method on multi-degradation and mixed-degradation datasets and conducted several ablation experiments on their core components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of applying LoRA to a pre-trained SD for multi-task image restoration is promising and interesting.
- The overall presentation is easy to follow.
- The experimental results are good and the ablation studies make sense.

Weaknesses:
- ControlNet is the most popular approach to adapting SD models to other tasks. I'm curious why the authors chose LoRA? As far as I know, LoRA is often used for large language models (with billions of parameters). It would be great to provide more detailed motivation in the introduction.
- In line 123, maybe it's better to use ""concatenate"" or other operators instead of ""add"" to present the unified parameters. Here, the weight $s_k$ can be ignored.
- Can the authors use other SD models as the base model? I believe applying LoRA to a multi-step diffusion process can further illustrate its efficiency.
- In Eq. (4), $s_0 \cdot M_k$ is used in both numerator and denominator, which seems weird and confusing.
- The mixed degradation experiment is cool. It would be interesting if the authors could apply their model to real-world degraded images.
- Line 45: proposed -> propose

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission proposes a transfer-learning based strategy to address challenges related to image-degradation restoration. The premise is that a pre-trained generative model can be employed as a common starting component for multiple degradation types, upon which distinct sets of trainable parameters (ie. low-rank adaptors) can be added in order to address specific-degradation restoration tasks. Mixed-degradation restoration is enabled through a top-K hyperparameter, that affords a mixture of (degradation) experts to be active. The experimental setup considers multi and mixed image restoration problems where average results are offered across image-degradation datasets and appropriate standard quantitative metrics, qualitative examples, are reported in comparison with alternative approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The technique described for piping specific samples down specific low-rank adaptor chutes is relatively easy to understand and yet reportedly results in competitive restoration accuracy for investigated datasets. 

* Nascent investigations into mixed-degradation image restoration problems provide a promising seed to be followed.

* The writing is of a reasonable standard.

Weaknesses:
* The key idea of leveraging pretrained VLM features (and specifically CLIP) for the task of image restoration from multiple degradations, pre-dates the current submission [R1]. While authors clearly go to some length to highlight their alternative CLIP-based scheme, which amounts to envoking specific (pre-existing [R2]) low-rank adaptors, the core technical contributions here can be regarded as somewhat limited.  

* The phrase 'Universal Image Restoration' may not be a sufficiently accurate (or modest) description for the proposed method. The submission collates ten different image restoration tasks which, despite vague statements in the abstract, remains a 'multi-task' not a 'universal' setup. Samples for all ten degradation tasks are shared between train and test (Sec. A.1) and individual task adaptors appear to be trained independently on task-specific datasets (L188--196). Generalisation ability to previously unseen degradations is also not considered. Suggest method description requires reworking.

* The claim that multi-task learning (MTL) frameworks, designed to handle image restoration for multiple degradations, share all parameters across different degradations (L029) is incomplete and somewhat misleading. Several existing MTL works (eg. [R3,R4]) make use of both shared and task-specific parameter subsets for multiple image restoration tasks. Indeed 'which proportion of parameters should be shared and which should be task specific' can be considered a fundamental (and long standing) MTL question. The idea of benefiting from commonalities between image restoration tasks is well understood and my concern is that this casts doubt on a core premise of the submission. 


References

R1. Controlling Vision-Language Models for Multi-Task Image Restoration. ICLR 2024.

R2. LoRA: Low-rank adaptation of large language models. ICLR 2022.

R3. All in One Bad Weather Removal using Architectural Search. CVPR 2020.

R4. Pre-Trained Image Processing Transformer. CVPR 2021.

Minor:

L076: 'draining' --> 'deraining'

L099: 'mim' --> 'min'

L238: 'aspects' --> 'aspects.'

Limitations:
Half of one sentence (L293) is apportioned to discussing method limitations. See above for suggestions on components that might make for valid additions here.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes universal image restoration framework using multiple low-rank adapters that learns task specific weights from to perform multi-domain transfer learning. the proposed method leverages the pre-trained generative model weights as the shared component and adapts it task specific low-rank adapters. At each layer in the restoration pipeline the proposed method uses the degradation similarity to combine LoRA adapters outputs, this enables the proposed to handle for mixed degradation restoration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes LoRA adapters to learn task specific weights and proposes a strategy to combine the adapter outputs using degradation similarity measure
- extensive experiments are performed showing the proposed strategy works better than random and average in table 3.
- extensive experiments are performed to show the proposed methods performance against the sota methods in table 1 for mutliple degradation task.
- Extensive experiments are performaed showing impact of LoRA rank and prediction accuracy

Weaknesses:
- In table of the paper authors compared proposed method against sota on REDS and LOLBlur datasets, both these datasets have mixed degradations  of blur, jpeg compression, noise, and low light. Although these comparisons performed on mixed degradations, it would be helpful to how the proposed method performs on mixed weather conditioned images (MID6), which is comparatively challenging than REDS and LOLBlur  datasets. 
MID6: Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration, CVPR, 2024.

- Can authors confirm, whether network re-trained seperately for each experiment in table-1,  and table-2 separately, i.e. table-1 and table-2 trained network weights for proposed method are different.

Limitations:
- authors have addresed limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a framework to improve image restoration across various degradation types using Low-Rank Adapters (LoRA). The proposed method adapts a pre-trained generative model to each degradation type. It performs a weighted sum of the output of adapted models using the estimated degradation of input images. The proposed method performs impressive results in restoration accuracy and resources.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proposed method is interesting and reasonable.
Experimental results support this paper's contributions and the proposed method's effectiveness.

Weaknesses:
In Table 3, the 'Top-1' strategy performs almost the same as the 'All' strategy, which limits the motivation of the weighted sum of the adapted models.
Table 6 presents the restoration performance comparisons for each degradation. The proposed method underperforms previous works in significant degradation types such as blurry, low-light, raindrop, and rainy.
The average scores might mislead the evaluation performances.

Limitations:
The proposed method is simple and effective, but evaluating average scores on multiple degradations can mislead its contribution.
The proposed method achieves near-best performance by selecting a single adapted model but underperforms in many major degradation types.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
yaYJlpidX1;"REVIEW 
Summary:
The paper proposes a novel technique to automatically discover in-context continual learning dynamics for image classification task sequences through meta-learning. In order to achieve this purpose, the approach relies on 2 main novelties: 
* Using self referential weight matrices on top of an image encoder - SRWM, as self-modifying that adapts itself to the stream of inputs, is an natural model for continual learning. 
* Encoding continual learning desiderata in the meta-objective, i.e. backward and forward transfer. 

The authors first apply the approach in a classic two-task setting (Split-MNIST) that allows them to showcase and analyse the emergence of in-context catastrophic forgetting phenomena, and to show that using their ACL loss can help reduce it. They further evaluate their method and compare them to replay-free baselines from the CL and meta-CL literature, showing an advantage of their approach in scenarios with up to 3 tasks. 

The authors further test the limits of their approach by comparing it to more recent learning to prompt techniques for continual learning, leveraging the power of pretrained large models. This scenario show a limitation of the technique in more complex scenarios with more tasks, more diverse and complex data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper takes an interesting perspective on continual learning, leveraging the interesting properties of SRWM and the capability of meta-learning to encode the desired behavior in the meta-learning objective. The combination of these two contributions is novel to the best of my knowledge, and lead to interesting insights. 

* The approach leads to interesting performance in relatively simple scenarios, outperforming some of the existing continual learning techniques. 

* I also particularly appreciated the authors discussion of the method limitations. Both the experiments with learning to prompts and the discussion provide very valuable insights that can help building on the work in the future.

Weaknesses:
* In my opinion, the main limitation of the approach is its practicality. From the experiments reported in Table 4, it seems that the approach requires to met-train on a sequence of similar length and/or complexity to provide its potential. This is not possible to know in advance in practice. Moreover, one limitation that the authors have not mentioned is that the meta-objective seems to require keeping in memory a number of copies of the model that is equal to the number of tasks. This can quickly become cumbersome for real applications that can require more complex models and very long sequences of tasks.  

* While the authors focus on classic benchmarks for continual and meta-learning, these benchmarks are artificial, relatively simple and lack of diversity. Different works highlight the limits of these benchmarks, I invite the authors to look at ""Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification"" Ullah et al. 2023, and ""NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research"" Bornschein et al. 2023 for examples of more realistic benchmarks. 

* It would be interesting to add a discussion of the cost of the approach (computation, memory, ...). Even is it gives a substantial boost in many cases, it would be interesting for practitioners to compare what they gain to what they pay.

Limitations:
The authors provide a detailed discussion of the work limitations, both in the experiments and the discussion sections. Some other limitations are highlighted in the Weaknesses paragraph above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on Automated Continual Learning which is different than handcrafted continual learning. It uses self referential neural networks to meta learn their own in-context continual learning algorithm. First, the paper shows the emergence of in-context catastrophic forgetting. Second, the paper analyze the performance of proposed method (ACL) and finally the paper discuss the limitation of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written and easy to follow
- The paper introduces original idea of Automated Continual Learning
- The paper identifies ""in-context"" catastrophic forgetting

Weaknesses:
- The paper claims to do in-context continual learning but the concept of in-context learning is not clearly explained.
- The paper mainly focus on two task and five task settings but it would be more helpful to see the more different settings such as three task or four task
- How is the size of SRWM affects the maximum sequence length that can be train?

Limitations:
Authors have addresses the limitation of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes a method for in-context continual learning (CL) by using a type of meta-learning neural architecture based on ‘self-referential weight matrices’ (SRWM). Proposed in prior work, these models learn to modify weight matrices iteratively as they process more and more inputs. In this work, they are given few-shot examples from different tasks and iteratively update the weight matrices as the examples are processed. This update process is referred to as “in-context” learning in this work. The key innovation is to define the loss function of SRWM training to optimise for both forward (improving performance of subsequent CL tasks) and backward (improving performance of previous CL tasks) transfer while achieving good performance on the current task. Experiments are conducted on commonimage classification meta-learning benchmarks such as Split-MNIST and Mini-ImageNet. Results show the proposed method prevents catastrophic forgetting (without using replay), outperforming existing meta-learning baselines on the evaluated benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Studies the problem of in-context catastrophic forgetting via a two-task toy setting and reveals the issue when training with no backward transfer loss term. This is shown to be mitigated by including the backward transfer loss term.

Proposes an in-context CL method using models based on SRWM and a novel loss to mitigate catastrophic forgetting as more tasks are learned. The method does not use a replay buffer.

Studies and covers standard image classification meta-learning tasks such as Split-MNIST, FashionMNIST, and CIFAR-10. On Split-MNIST, shows improvements over existing CL and meta-baselines in both domain and class incremental evaluation settings. The improvements, when additional 5-task fine-tuning is used, is significantly above baselines. 

The paper is clearly written, with thorough literature review.

Weaknesses:
One weakness of the proposed method is that the number of loss function terms increases with the number of CL tasks, as pointed out by the authors in Appendix A.5. This prevents this method from being scaled to more practically relevant settings where a large number (much more than 2 or 3 that this paper has mostly focused the experiments on) of tasks are considered in a CL setting. Method of reducing the loss terms would strengthen the paper.

Another weakness, which is also noted by the authors in Table 4 and Section 4.3, is that the performance of the proposed model and method is poor compared with those based on pre-trained transformer models, even on an easier evaluation task. The authors in Section 5 also discuss a potential connection between LLM transformer training as an implicit version of the proposed model and method. Given these existing strong and more widely adopted methods, it is unclear how much value the proposed method adds. SRWMs are not widely used and LLMs training can scale to a massive number of tasks with a single loss [1] (albeit not CL). A more detailed explanation of the application of the findings of this paper beyond those interested in SRWMs would be helpful.

Another weakness of this paper is its focus on image classification meta-learning tasks only. It is helpful to know the generality of this method, for example on language modelling tasks or multimodal tasks. An experiment demonstrating the method in CL language tasks would be helpful.

[1] Finetuned language models are zero-shot learners. Wei et al. ICLR 2022.

Limitations:
Limitations have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of catastrophic forgetting (CF) by formulating continual learning (CL) as learning from a sequence of demonstrations of tasks. The paper proposes a meta-learning objective function that includes backward transfer terms. These terms compute the error of the predictor on previous tasks after receiving demonstrations of the current task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The approach of formulating (continual learning) CL as learning from a sequence of demonstrations of tasks is interesting.
- The experiment shows positive results when compared to non-meta-learning approaches

Weaknesses:
- The paper is difficult to follow. Many definitions and the algorithm are not very well explained.
    - The motivation of formulating (continual learning) CL as meta-learning is not well presented.
    - Some details of the architecture are mentioned in the background section only (e.g. replacing self-attention with SRWN and the multi-head version.)
    - The details of the training and inference process are not well presented.
- The training process can be very costly and poorly scaled with the number of tasks and the number of examples per task. In each step over a sequence of demonstrations, the method needs to compute and store a new weight matrix in order to perform back-propagation. It might require more memory during training and at inference.
- Even being a meta-learning approach, the model still needs fine-tuning when given a new task to adapt to a new number of tasks.

Limitations:
There are no negative social impacts. My suggestions have been listed in the previous sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
yAKuSbIwR7;"REVIEW 
Summary:
This paper provides a thorough characterization of regularizers which lead to synaptic balance (when the ""cost"" of input weights to a neuron or pool of neurons is tied to the cost of output weights) in trained neural networks. Their results apply to many different activation functions and architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is very well-written and easy to follow. I was able to read everything, including the math, smoothly. The mathematical arguments themselves are crisp and correct, which I really appreciated.

Weaknesses:
The paper is strongly lacking in motivation. I never really understood *why* I should care about synaptic balance. Also, it is clear from the numerical experiments that synaptic balance only emerges in networks when it is enforced via a regularizer (expect in the case of infinitely small learning rate), but why is this surprising? It seems obvious that adding a regularizer for some property tends to result in that property. It would be shocking if synaptic balance occurred without some regularization towards the property. Thus, while the ""what"" and ""how"" of the paper are nicely addressed, I feel the paper is missing the ""why"". I believe if the authors could address this from the outset, it would make the paper much stronger, and I would of course be willing to increase my score.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a theoretical approach to the analysis of balanced neurons and networks. Their theoretical work includes proof of the convergence of stochastic balancing. In addition, they investigate the effect of different regularizers and learning rates on balance, training loss, and network weights, including practical simulations for two classification problems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tries to reveal the inner structure of neural networks during the training phase. This is a very important but difficult problem; its solution could provide new insights for developing better training algorithms. The work proposed can ultimately be an important step toward more transparent networks as opposed to their current black box character.

Weaknesses:
The paper has some weaknesses, most notably how the material is presented and part of the evaluation.

Theorem 5.1, dealing with the convergence of stochastic balancing, is arguably the central piece of the paper. However, its formulation is bulky and should be reduced to a shorter, more manageable size, potentially with the help of lemmata. This becomes apparent when seeing that its proof contains the proof of another proposition.

In Figure 4, the authors say that these panels are not meant for assessing the quality of learning. However, measuring not only the training loss but also the accuracy on a test set will give important insights. How does the classification performance relate to the degree of balancing? Why did the authors not include this analysis? It could give important insights into the relationships between overtraining, generalization capability, balance, and accuracy.

The author should discuss the consequences of their work on network training. They do not discuss the immediate practical consequences or any recommendations they can make based on their results.

Limitations:
The authors could be more specific about the consequences of their work, including limitations. For example, can they recommend any specific learning rate, network structure, or other features for optimal training?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to study and explain the phenomenon of neural synaptic balance, where a balanced neuron means that the total norm of its input weights is equal to the total norm of its output weights. Particularly, the authors study the reasons why and when randomly initialized balanced models (so, models whose neurons are balanced) tend to be balanced at the end of training as well. The study takes into account many different components of neural networks (activations, layer kinds, regularisers).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study is very comprehensive, and sheds light on some interesting properties of deep neural networks.

Weaknesses:
While it is true that, as the authors state in the conclusion, neural synaptic balance is a theory that is interesting on its own, I would encourage the authors to expand the discussion on possible application domains of this theory. Why is it interesting? What are the advantages that a complete understanding of such phenomenons could bring to the table?

Limitations:
No concerns here

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a theory of neural synaptic balance, defined as the condition in which a total loss achieves the same value for the input weights to a neuron and its output weights. This is different from the well studied  E/I balance in neuroscience and machine learning literature. The authors show mathematical derivations of how to balance a neuron without affecting the outcome of the network and show that balancing a network is a convex optimization process.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is overall clear and detailed, the mathematical proofs are sound and the paper structured well moving from straightforward claims to less trivial points.

Weaknesses:
The paper is about neural synaptic balance, but the authors do not provide convincing motivation why we should care about such balancing.  As they mentioned, adding a simple L2 regularizer will balance the network naturally (in a distribution sense, not necessarily each neuron individually) during training and have other well-known  benefits, so the elaborate mathematical derivations on the general balancing process seem redundant. In addition, in the authors' own plots, unbalanced networks sometimes outperform the balanced networks (e.g., fig 3E), which just emphasizes the point. One of the mentioned motivations  is biological neurons, but they claim that biological neural data about synapses do not exist. However, they could test their hypothesis against the currently available connectomes e.g., from or the Drosophila fly brain. They mention spiking networks, but the notion of input-output homogeneity is unclear in spiking networks. Finally, physical neurons' energy consumption is mentioned without details.

Limitations:
The whole framework is specific to BiLU neurons or perhaps to other power-law functions. The relevance to spiking neurons is therefore questionable. It is also questionable as a general principle for machine learning.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
xtpY1kQmW9;"REVIEW 
Summary:
This paper appears to suggest that any decision is composed of two Bayesian decisions and it trys to evaluate the implications of this idea. 

I am very confused by this paper and really don't know what to make out of it. For example, the conclusion seems to be only a brainstorming session of random ideas and the rest of the paper does not appear to be much better.

At the very least, it is not well written, at worst the proposed approach does not make any sense.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Given that I don't properly understand what exactly the authors want to achieve, I am unable to formulate the strengths of this paper.

Weaknesses:
The presentation is very messy. The paper jumps from topic to topic without me understanding their relations to each other.

Limitations:
see above

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the implications of Bayes' theorem, making assumptions inspired by a thought experiment of communicating a message. Prior (and model) elicitation by solving a fixed point equation is discussed.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The paper takes a fresh look at decision marking under uncertainty, which is at the center of machine learning.
* The generality of the setting makes the discussion applicable to virtually all of ML.

Weaknesses:
While I am sensible to the topic of prior and model elicitation from coherence arguments, I believe the paper needs a thorough revision focussing on clarity. While I have some intuition now, it is still not crystal clear to me what the exact goal or claims of the paper are. See bullets below for constructive comments.

## Major
1. Section 4: what is the probability $P$? What is the underlying space and sigma algebra? What are they supposed to represent?  
2. Section 4 introduces several very strong assumptions, like $1-P(A\vert B) = P(B\vert A)$ (is it for all $A,B$ in some sigma-algebra or for a specific pair of events?), that are motivated by an analogy about communicating a message. It is not clear why I should be prepared to make these strong assumptions. The fact that I don't know what $P$ is supposed to model or serve as does not help. Is it a joint probability over the variables describing a decision problem, as in decision theory? In that case, will it be used in conjunction to a loss function to make decisions? Will it be judged by some measure of decision accuracy? Or are we in a de Finetti framework, coming up with a personal probability $P$ which we will use to make predictions about unobserved variables? My intuition is that we are dealing with the latter kind, but this should be explained. And the strong assumptions need to be motivated by more than an analogy about communication.
3. The information analogy which motivates imposing the fixed point equation (9) is unclear, as well to what probability and what events it should apply.
4. p5 L179: the sentence about the parameter being a dynamic parameter for a learning system is unclear. We haven't discussed any learning algorithm yet.
5. I am not sure I see where Eqn (11) comes from. $\lambda$ has been chosen to derive (10) from Bayes' theorem, but it doesn't have to be the right base to write (11), right? Same remark for (18).

## Minor
1. p7 L248: Although neural networks have been a popular class of models and algorithms, supervised learning is not synonymous with neural network training.
2. p7 L252: the meaning of ""the $\lambda$ expression"" is unclear.

Limitations:
This is fundamental work that does not have any immediate negative societal impact.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The purpose of this paper is to investigate the optimality of a classifier. It is known that the Bayes classifier is optimal, and it is likewise known that an explicit computation of the Bayes classifier is often very challenging if not impossible. This paper offers an analysis of the Bayes classifier as a sequential solution of two problems. An analysis and interpretation of a vase / faces example is presented and some theory is developed to further understand it. The paper concludes with an application.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors are exploring an idea which is novel, and the whole thinking about Bayes classifiers as comprising two sub-problems seems novel and worth pursuing.

Weaknesses:
I did not really understand the discussion with the vase, the sender and receiver. Perhaps the authors should somehow connect the Bayesian ideas to the description of the problem earlier? I think the paper would really benefit from rewriting Section 4 with the vase as a running example, because it is hard to connect the various decisions with the probabilities. Maybe it's worth to add more illustrations / diagrams for this? The authors are presenting novel ideas and it's hard to understand them as they are currently presented.

For the theoretical implications, I think it would be better to illustrate the approach on a simpler model like a linear one. 

The paper started by mentioning the Bayes classifier but does not come back to it as an example. 

The paper states that the Bayes classifier is broken up into two decisions, but those are just briefly mentioned in the vase / faces example. The authors should carry this thread of reasoning through the whole paper.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
wZ5kEOCTce;"REVIEW 
Summary:
This paper reveals the role of inter-patch dependencies in the decoder of MAE on representation learning. The paper shows that MAE achieves coherent image reconstruction through global representations learned in the encoder rather than interactions between patches in the decoder. Based on this, the authors propose CrossMAE, which only utilizes cross-attention in the decoder.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The approach of analyzing the reconstruction process through self-attention between mask tokens and cross-attention between mask and visible tokens is intriguing.
- The writing is clear and easy to follow, with main messages that are solid and insightful.

Weaknesses:
1. Idea/Novelty
- The claim that MAE reconstruction is achieved through global representation learning within the encoder rather than interactions between patches needs more support. Recent studies linking MAE to contrastive learning have found that the receptive field of specific mask tokens in the decoder is relatively small. Could the role of mask tokens in the decoder be to capture local area information? This might explain the smaller attention magnitude of masked tokens compared to visible tokens in Figure 1(b). 
- There is a concern that without self-attention (i.e., with the proposed method), the observation that authors made on the vanilla MAE may no longer be valid. Additional explanation on this point is necessary as this observation is the main motivation for suggesting CrossMAE.

2. Additional justification
- Effectiveness of using a subset of mask tokens as queries: Unlike the traditional architecture, this method uses only a subset of mask tokens as queries. Detailed analysis and interpretation are needed on why this is effective. 
- Performance differences when using the entire set of mask tokens versus a subset (and what percentage of mask tokens is used) should be reported.

3. Experiment
- For a fair comparison, CrossMAE's performance should be evaluated using the same setting as the original MAE, especially regarding the fine-tuning recipe.
- The current experimental results do not convincingly demonstrate the effectiveness of the method. For classification tasks, only the linear-probing and fine-tuning results on IN1K are reported. Following the previous works, classification on various downstream datasets should be also considered.
- For generalizability, evaluation on another task like semantic segmentation (e.g. on ADE20K) would be useful to verify that the suggested method learns the generalizable feature representation.

Limitations:
The authors have not discussed limitations of this work except for the very last sentence of section 5, indicating that they have discussed limitations in this section in the questionnaire #2. It is strongly recommended to disclose more detailed limitations of the proposed work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel pre-training approach called CrossMAE. Instead of concatenating the masked and visible tokens for the decoder, the authors add cross-attention to decode the masked tokens by using them and the visible patch embeddings as separate inputs to the decoder. Further, the authors introduce a method to only partially reconstruct the masked patches, and leverage inter-bock attention to fuse feature across different layers.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well motivated through a practical observation
- The authors propose a useful technical contribution which seem intuitive given the described observations
- The paper is well written and technically sound
- All visualizations provide additional value, I especially like Figure 5. It describes the effect of the contributions well
- Judging from the experiment section, the presented approach mostly improves over the vanilla MAE and other MAE-like follow-up works

Weaknesses:
- I feel like the paper is missing a more structure ablation of the individual contributions. I think the paper would benefit from having a simple table where all contributions are added sequentially to better identify the performance effect of the individual contributions as in:
	MAE X.X
	+ Cross-Attn X.X
	+ Partial Reconstruction X.X
	+ Inter-Block Attn X.X
- As can be observed from Table 3 c), the final setting (underlined) of the prediction ratio, 0.75, turns out to be exactly the same as the optimal masking ratio, 0.75. If I understood correctly, this means that in practice, CrossMAE works best when it predicts all tokens that were masked, not just a fraction of them. Only predicting part of the masked tokens was previously listed as a contribution. Therefore, I don’t understand how this additional hyper parameter provides any benefit for better downstream performance. Maybe I’m missing something and this be cleared up by answering the previous point.
- All models are only trained for 800 epochs. The original MAE reaches peak performance at 1600 epochs. For a thorough comparison, it would be necessary to also train CrossMAE for 1600 epochs and see if the performance gains sustain, or if performance has peaked at 800 epochs.
- Table 1 is missing the CrossMAE ViT-H with 75% masking ratio
- Contribution 2 and 3 don’t seem to be as well motivated in the introduction in comparison to Contribution 1
- Better performance is listed as a contribution. IMO this is not a contribution, rather a result of the technical contributions

Limitations:
The authors have sufficiently addressed the limitations of their approach.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents CrossMAE, a methodology for improving pre-training efficiency over that of MAE for an encoder. The paper motivates its approach by presenting visual evidence that, in standard MAE pre-training, masked tokens attend to other masked tokens significantly less than to non-masked (aka, visible) tokens. Using this motivation, the paper then presents CrossMAE, which differs from MAE largely in that it replaces the MAE self-attention with cross-attention between the masked tokens and a learnable weighted combination of the encoder feature maps. This aspect decouples queries from keys and values (which is not the case in MAE), which the paper then exploits to allow only some (but not necessarily all) mask tokens to be used during reconstruction to pre-train the model. The paper presents an analysis of which encoder block features are optimal to cross attend with each decoder block, and it presents ablation studies on multiple design decisions. Finally, it presents visual and fine-tuning results showing comparable performance to MAE and similar methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper motivates CrossMAE well by showing evidence of a potential inefficiency in MAE (self-attention) and then presenting an approach to remedy it (cross attention). I particularly like how the paper delves even deeper, though: instead of stopping at the level of replacing self-attention with cross-attention, it then points out that this choice allows for a significantly fewer number of masked patches to have to be reconstructed, which reduces flop count significantly. The ablations in Table 3 are fairly thorough and answered some questions I have developed. The performance of CrossMAE appears comparable to other SOTA methods but with significantly more efficient pretraining.

Weaknesses:
1) In Fig 1b, IIUC, for one particular mask token, the two $\mu$'s are the respective attention values averaged over all transformer blocks and all masked/non-masked tokens. If this is the case, my concern is that by averaging over all transformer blocks, variations in the attention is being hidden. Naively, I would think that for early blocks, the attention due to masked tokens would be small (as the paper concludes) but becomes larger for the later blocks (since now the masked tokens have actual useful signal in them). Did you consider this?

2) I do not follow why CrossMAE does not need an MLP at the end to convert final decoder tokens back to raw pixels. Line 218 says that the inputs to the first encoder block are included in the feature maps for cross attentions. Does this cause a final MLP to not be used?

3) Less critical:
  3a) Fig 1b should point the reader to Section A.1. I spent much of my reading confused about what $\mu$ is.
  3b) Fig 4a should have a different number of decoder layers than encoder layers. When I saw this figure, I immediately wondered why a decoder block wasn't being paired with feature maps from its ""partner"" encoder. I had to wait until lines 204-207 to get an explanation of why this doesn't work.
  3c) Line 187 references a ""second question"" in Sec 3.1, which doesn't exist as far as I can tell.
  3d) Fig 4a shows the ""vanilla"" version of Cross MAE, where the final encoder layer feature maps are attended with all decoder layers. But the paper presents results exclusively (?) on the version that uses a learned combination of the feature maps. Anyway, the figure confused me. Maybe I just didn't understand what the solid arrows vs dotted ones are supposed to represent.

Limitations:
No weaknesses are specifically addressed. But as this paper is essentially an optimization to MAE, I'm not sure this question is relevent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
wH36UKML4x;"REVIEW 
Summary:
This paper addresses the problem of subpopulation generalization, also known as spurious correlations. Building on the Last Layer Retraining (DFR) method, it removes the constraints on a small subset of annotations. The paper introduces the Environment-based Validation and Loss-based Sampling (EVaLS) method. Unlike DFR, EVaLS divides the validation set $D^{val}$ into two parts: (1) $D^{LL}$,  where losses from an ERM-trained model are used as a proxy for identifying minority groups for retraining, and (2) $D^{MS}$, where environment inference methods are used for partitioning environments. The paper presents theoretical insights and empirical results demonstrating the effectiveness of EVaLS.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper is well-structured and presented in a clear and organized manner, making it easy to comprehend and follow along.
* The proposed method is simple but effective and explores a relatively challenging area in existing literature (*i.e.* subgroup generalization without group annotations). 
* The authors provide some theoretical analysis to support their claims.

Weaknesses:
* The novelty and contribution of the proposed method may be limited for the following reasons: 1) The paper combines multiple previously proposed methods (*i.e.* DFR [1], EIIL [2]) all at once, which inherently guarantees a nontrivial performance; (2) The primary technical contribution, at least from my perspective, is the loss-based sampling, which has been already explored extensively in the noisy label literature and has been used as tools for pseudo-labeling. 
* The paper fails to discuss recently proposed methods that also require no group annotations, such as SELF [3], BAM [4], and BPA [5]. In particular, SELF is also a direct follow-up of DFR. The authors are encouraged to discuss the limitations and strengths of loss-based schemes against the class-based schemes advocated by SELF and BAM.
* More analyses can be included to provide further understanding of the selected loss-based samples. For example, given a threshold, how much percent of the high-loss and low-loss samples are indeed the minority and majority samples and how does this percentage change with the threshold?

[1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations, ICLR 2023

[2] Environment inference for invariant learning. ICML 2021

[3] Towards Last-layer Retraining for Group Robustness with Fewer Annotations. NeurIPS 2023 

[4] Bias Amplification Enhances Minority Performance. TMLR 2024

[5] Unsupervised learning of debiased representations with pseudo-attributes. CVPR 2022

Limitations:
Aforementioned in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To address the issue of spurious correlations when group labels are unavailable, this paper proposes a new method called EVaLS. It first creates a balanced training dataset using loss-based sampling. Then, it evaluates the accuracy of the balanced training set based on the inferred environments from the validation set, and selects models accordingly.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written, and includes a rich set of experiments with necessary theoretical explanations.

2. It is essential to discuss the multiple  (unknown) spurious features case which has been overlooked in previous studies.

Weaknesses:
1. Why is the approach of using high-loss points (considered as the minority group) more effective than directly using misclassified points (considered as the minority group) in methods like JTT? Intuitively, compared to misclassified points, high-loss points are more ""implicit"" and no obvious thresholds, which could potentially result in high-loss points actually belonging to the majority group, thus exacerbating the imbalance in resampling.

2. If the author can show the balance level of samples obtained through loss-based sampling compared to directly using labels (misclassified points), it could further illustrate the advantages of loss-based sampling.

3. In Section ""Mitigating Multiple Shortcut Attributes"", if color is treated as a known spurious attribute and shape as an unknown spurious attribute, how would the performance of EVaLS be affected? Based on my understanding, there is a possibility that simplicity bias could cause the model to prioritize learning the simpler feature, color, and struggle to learn the more complex shape attribute. Therefore, considering color as known and shape as unknown can better show the performance of EVaLS in handling complex spurious features.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies how to improve the model’s robustness to multiple spurious correlations when the group labels (indicator for spurious correlation) are unknown in general. The proposed approach, EVaLS, leverages the loss from a base ERM model to sample a balanced subset to prevent learning from spurious correlations. In addition, a new synthetic dataset (Dominoes-CMF) for multiple spurious attributes is crafted. Empirically, the proposed approach sometimes has advantages over the rest of the baselines when using the same amount of additional information (group label).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The main paper is generally well-written. 
2. The theoretical analysis in Section 3.3 (with derivations and proofs in Appendix) shows that for one-dimensional Gaussian distributions, choosing the tails on the two sides of the distributions creates balanced groups, even though the original data distribution is skewed. 
3. Environment inference technique is demonstrated to be useful for separating the dataset into groups with different distributions of the subpopulations and then for model selection. 
4. The proposed technique only requires last-layer retraining on part of the validation set, which is generally more efficient.

Weaknesses:
1. Figure 2 attempts to illustrate more minority samples have high loss while the majority samples have low loss. However, in each of the plots, only the % of one of the minority or majority groups is shown. The illustration can be improved by showing the % of both majority and minority groups in the same plot, and showing the actual distribution of the loss for the groups. 
2. Though the idea is straightforward, it is unclear how the loss-based instance sampling is actually implemented. It is helpful to provide an algorithm or pseudocode to improve the presentation. 
3. The theoretical analysis is generally sound but limited to a case without discussing the use of the loss (which may not be Gaussian) and the spurious correlations (which involve at least two dimensions of core and spurious features [1]). 
4. The experimental results are less polished and sometimes the advantages are not so clear over other baselines. Some results are missing for datasets such as UrbanCars and MultiNLI. Only a few baselines are compared for the new dataset in Table 2. There is also no convincing and fine-grained analysis (e.g., ablation study) to understand how the proposed approach ensures data balancing and improves group robustness. 
5. The paper initially focuses on improving group robustness when multiple spurious correlations are present, but the experimental results are lacking for these more challenging datasets. 

[1] Sagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. ""An investigation of why overparameterization exacerbates spurious correlations."" In *International Conference on Machine Learning*, pp. 8346-8356. PMLR, 2020.

Limitations:
The authors have discussed the limitations in Section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
vYmvgxpgwH;"REVIEW 
Summary:
This paper explores compute-optimal inference for large language
models (LLMs), focusing on designing models and strategies that
balance additional inference-time computation with improved
performance. The study evaluates the effectiveness and efficiency of
various inference strategies, including Greedy Search, Majority
Voting, Best-of-N, and Weighted Voting, across different model sizes
(e.g., 7B and 34B) and computational budgets. Experimental results
indicate that smaller models with advanced tree search algorithms can
achieve a Pareto-optimal trade-off, offering significant benefits for
end-device deployment. For example, the Llemma-7B model matches the
accuracy of the Llemma-34B model on the MATH500 dataset while using
half the FLOPs. These findings suggest that smaller models with
sophisticated decoding algorithms can enhance problem-solving accuracy
across various generation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper focuses on an interesting topic and should be of interest
  to the audience of NeurIPS.
- It considers a comprehensive experimental investigation to confirm
  the claims.
- The proposed tree search algorithm is interesting and seems to
  outperform the competition.

Weaknesses:
- Although the paper offers quite thorough experimental analysis, it
  does not look deep in terms of theoretical ideas (although there are
  2 theorems), which may be a problem for a flagship venue like
  NeurIPS.
- Overall findings on the possibility to train an equally accurate
  model with fewer computational resources do not look surprising.
- The paper would benefit from additional proof-reading as there are a
  large number of typos present.

Limitations:
The paper concentrates on mathematical problem-solving tasks using 7B
and 34B models, with findings potentially not applicable to other
domains. Future research should explore a broader range of model sizes
and different training datasets to better understand compute-optimal
inference in mathematical problem-solving.

I should also say that these limitations have been explicitly
discussed by the authors themselves (so not a criticism).

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an approach to select an optimal inference strategy for LLMs and empirical analysis on Math problem solving tasks. The main idea is to select an inference strategy based on a computational budget (FLOPs). The underlying policy model samples solutions by generating tokens based on the budget and a ranking model consumes these tokens. A new reward model is developed  to explore the solution space more effectively. The reward acts as a weighted majority function over the solutions.
Experiments are performed on Math problem solving benchmarks. Some of the key insights from the experiments is that a smaller LLM can outperform the larger LLM in terms of using a smaller computational budget while maintaining similar accuracy. They also show that the proposed approach with a smaller budget has comparable accuracy than sampling with a larger budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The insights that inference time strategy can compensate for using smaller LLMs in generation seems to be interesting
- The experiments also provide a basis for analyzing scaling properties of inference which can be significant

Weaknesses:
- In terms of the method itself, I was not sure if it is very novel. It seems to be a smaller variation on the tree search methods that search for solutions in the generated space
- In terms of comparisons, I was not sure about the significance of the benchmark, i.e., are there some properties that make the proposed reward reranking more optimal in Llema model specifically (due to the structure of math problems, etc.). In general, since the main contribution of the paper is empirical, I think there should be experiments or discussions different LLMs to make the contribution more significant. 
-Overall, the empirical conclusions seem very tied to the specific benchmarks, so I was a little unsure regarding the significance of the conclusions.

Limitations:
Limitations regarding the datasets are mentioned.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the optimal training configurations of large language models (LLMs) during inference. The proposed inference strategy, REward BAlanced SEarch (REBASE), combines the strengths of Monte Carlo Tree Search (MCTS) with reduced inference costs, resulting in improved performance on math-domain tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper provides a comprehensive overview, i,e, the inference scaling law, of the performance of different sampling strategies under various inference configurations.
2. The novel REBASE inference strategy achieves better downstream task performance under the same computational budget or even less.

Weaknesses:
### Major 

1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.

2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?

3. In Section 4.2, the observation in ""Scaling law of compute-optimal inference"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?

4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. 

5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. 

### Minor

1. Figures. 2 and 3 are not referenced in the main manuscript. 

2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uS9RZH6K65;"REVIEW 
Summary:
This paper proposes a denoising framework to alleviate the influence of noisy text descriptions on open-vocabulary action recognition in real scenarios. A comprehensive analysis of the noise rate/type in text description is provided and the robustness evaluation of existing OVAR methods is conducted. A DENOISER framework with generative-discriminative optimization is proposed. The experiments demonstrate the effectiveness of the framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The robustness to noisy text descriptions/instructions in real-world OVAR applications is an interesting and meaningful problem.
- The evaluation of the robustness of existing OVAR methods when facing the noise text description input is valuable to the community.
- The motivation is clear and the overall framework is technically sound.

Weaknesses:
- About the experiments,
    - The reviewer thinks that the most convincing results are the Top-1 Acc of existing OVAR models under the Real noise type. However, in Table 1, the proposed model does not demonstrate much superiority compared to GPT3.5's simple correction. The reviewer worries about the research significance of this problem. Will this problem be well resolved when using more powerful GPT4/GPT4o with some engineering prompt designs?
    - In Table 2, I would like to see the performance of other correction methods (e.g., GPT3.5/4/4o) for a more comprehensive comparison.
    - Since this work focuses on the noise text description problem in OVAR, it is necessary to demonstrate the results of those CLIP-based methods without any additional textual adaptation (e.g., the vanilla CLIP).


- About the method,
    - The reviewer thinks that the overall model design is reasonable and clear. However, the method part introduces too many symbols which makes the paper very hard to follow. It is unnecessary to over-decorate the technical contributions.

- Minor issue,
    - The authors seem to have a misunderstanding about the OVAR setting (Line 113). In OVAR, the model is evaluated on both base-classes and novel-classes during testing. In this case, all action classes from the UCF/HMDB datasets can be used for testing when the model is trained on K400, as there are many overlap classes between K400 and UCF/HMDB.

Limitations:
The limitations are discussed and there is no potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles the challenge of noisy text descriptions in Open-Vocabulary Action Recognition (OVAR), a task that associates videos with textual labels in computer vision. The authors identify the issue of text noise, such as typos and misspellings, which can hinder the performance of OVAR systems. To address this, they propose a novel framework named DENOISER, which consists of generative and discriminative components. The generative part corrects the noisy text, while the discriminative part matches visual samples with the cleaned text. The framework is optimized through alternating iterations between the two components, leading to improved recognition accuracy and noise reduction. Experiments show that DENOISER outperforms existing methods, confirming its effectiveness in enhancing OVAR robustness against textual noise.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper aims to study a new research topic, i.e., achieving robust open-vocabulary recognition performance with noisy texts. This direction has not been investigated before, which seems to be applicable in real-world applications.

- The proposed intra-modal and inter-modal methods are intuitive and demonstrated effective in the experiments. 

- The experiments show that the proposed method is effective with different network architectures (XCLIP and ActionCLIP), which verifies that the method can be widely used.

Weaknesses:
- The baseline models are outdated and not tailored for OVAR. The authors failed to reference recent OVAR papers such as OpenVCLIP[1] (ICML 2023), FROSTER (ICLR 2024), and OTI (ACM MM 2023).

- In Table 1, it is evident that the proposed method outperforms GPT-3.5. Additionally, the authors present examples in Table 4 to demonstrate the superiority of the proposed method over GPT-3.5. However, upon personal experimentation with all the examples from Table 4 using the provided prompt from the paper (lines 243-245), I observed that the GPT-3.5 model successfully rectified all issues, including challenging cases where the proposed method fell short. As a result, I remain unconvinced by the findings.

This is the prompt given to GPT-3.5 model, and I hope other reviewers can also try it on their own:

The following words may contain spelling errors by deleting, inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer without explication. What is the correct spelling of the action of  “cutting i aitnchen”.


[1] Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization.

[2] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition.

[3] Orthogonal Temporal Interpolation for Zero-Shot Video Recognition.

Limitations:
Yes, they addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of Open-Vocabulary Action Recogniton (OVAR). Specifically, it focuses on the issue that the action labels provided by users may contain some noise such as misspellings and typos. The authors find that the existing OVAR methods' performance drops significantly in this situation.  Based on this analysis, they propose the DENOISER framework to reduce the noise in the action vocabulary.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. 
2. The framework is well presented and explained.
3. The experiments show the effectiveness of the denoising process.

Weaknesses:
1. This paper actually focuses on text denoising and does not involve any specific action recognition technology. The author just chose the field of OVAR to verify the effectiveness of the proposed text-denoising method. The title is somewhat misleading. I think the author should regard text-denoising as the core contribution of the article instead of the so-called ""robust OVAR""
2. The article focuses on too few and too simple types of text noise, including only single-letter deletions, insertions, or substitutions. These kinds of errors can be easily discovered and corrected through the editor's automatic spell check when users create a class vocabulary. This makes the method in this paper very limited in practical significance.
3. , The proposed method, although a somewhat complex theoretical derivation is carried out in the article, is very simple and intuitive: that is, for each word in the class label, selecting the one that can give the highest score to the sample classified into this category among several words that are closest to the word.  There is limited novelty or technical contribution.

Limitations:
The author states two limitations of the work in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the challenge of noisy text descriptions in  Open-Vocabulary Action Recognition. It introduces the DENOISER  framework, which combines generative and discriminative approaches to denoise the text descriptions and improve the accuracy of visual sample  classification. The paper provides empirical evidence of the framework's  robustness and conducts detailed analyses of its components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and the content is easy to understand. 
2. The motivation presented by the authors is clear, the label noise problem does exist in video datasets.
3. The authors show the types of noise and their percentage, in addition, the authors verify the validity of the proposed method through comparative experiments.

Weaknesses:
1. As the authors state in the limitations section, textual description noise does exist, but it can be corrected with an offline language model, what are the advantages of the authors' proposed approach?
2. I would assume that the text noise problem presented in this paper is even worse on large video datasets collected by semi-automatically labeled networks, e.g., Panda70M, howto100M, and InternVid. I suggest that the authors might consider validating their ideas on these datasets.

Limitations:
The authors have provided a limitations analysis in their paper and I have suggested some limitations in the Questions section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
vKwf15M5EE;"REVIEW 
Summary:
The submission presents a deep learning-based approach for cortical surface reconstruction (CSR) from brain MRI data using weak supervision derived from cortical brain segmentation maps. The claimed contributions are: 

1. Weak Supervision Paradigm: The authors introduce a new weakly supervised paradigm for reconstructing multiple cortical surfaces, significantly reducing the reliance on pseudo ground truth (pGT) surfaces generated by conventional CSR methods.
2. New Loss Functions: Two novel loss functions are designed to optimize the surfaces towards the boundaries of the cortical ribbon segmentation maps. Regularization terms are also introduced to enforce surface uniformity and smoothness.
3. Evaluation and Performance: The proposed method is extensively evaluated on two large-scale adult brain MRI datasets and one infant brain MRI dataset, demonstrating comparable or superior performance to existing supervised DL-based CSR methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper presents an approach to leverage weak supervision from segmentation maps instead of relying on pGT surfaces, which is a significant departure from traditional methods.
2. The methodology is explained and the experimental setup is described. The authors conduct evaluations on multiple datasets, evaluating the efficacy and efficiency.
3. The paper is well-structured, with clear descriptions of the problem, methodology, and results. The figures and tables effectively illustrate the performance and comparisons.
4. The approach addresses a critical bottleneck in CSR by reducing the dependency on time-consuming and error-prone pGT surfaces, potentially broadening the applicability of CSR methods to more diverse datasets and clinical scenarios.

Weaknesses:
Method
1. It seems that this work combines [1] and [2], and thus has limited technical novelty. The architecture in Figure 1 and the circle consistency loss (Eq. 5) are almost identical to CoCSR [1]. The boundary surface loss and inter-mesh normal consistency loss (Eq. 3-4 and Figure 2) are very similar to the loss functions proposed by [2].

2. Additionally, the customized edge length loss (Eq. 6) has also been proposed by [3]. Considering the large individual differences across human brains, how did the authors choose the area A without knowing the pGT cortical surfaces?

3. It is confusing that the ribbon segmentations are used as both input and pGT. The authors claimed that the ribbon segmentations are inaccurate weak supervision, but still generated the initial surface based on ribbon segmentations according to Figure 1.

4. The velocity field defined in Eq. 1 is time dependent. How did the authors learn non-stationary velocity fields through a 3D U-Net?

5. In line 156, a bijective mapping with continuous inverse is called homeomorphism. A diffeomorphism is defined as a smooth/differentiable bijection with smooth/differentiable inverse.

6. As shown in Figure 2 (b), it is clear to observe that the WM and pial surfaces do not have the same normal directions in some regions. The inter-mesh normal consistency loss could cause inaccurate surface reconstruction. Could the authors provide more insights to solve this problem?


Results
1. The experimental results are unreliable and unconvincing. After careful comparison, it seems that the baseline results (CorticalFlow++, CortexODE, Vox2Cortex, DeepCSR) on the ADNI and OASIS datasets in Table 1 were directly copied and pasted from Table 2 in [1]. This leads to unfair comparisons.

2. Furthermore, as reported in Table 1, SegCSR produced no more than 0.061% of self-intersecting faces (SIF), whereas the authors claimed in line 264 that there are ∼0.3% on average for both white and pial surfaces. This is confusing. Which result is correct?

3. In line 263, the authors claimed that DeepCSR and U-Net produced a large number of SIFs without post-processing. However, the Marching Cubes algorithm only produces topological errors such as holes no SIFs.

4. The BCP dataset only includes 19 test subjects. Cross-validation should be conducted to ensure fair evaluation of the performance.

5. The flow ODE was integrated using the forward Euler method with T=5 steps. Such a large step size could cause unstable ODE solutions and failure in preventing self-intersections. The value of the Lipschitz constant should be reported to examine the numerical stability of the ODE solver.

6. The authors reported that SegCSR requires only 0.37s of runtime per brain hemisphere. However, SegCSR adopted a topology correction algorithm, which may take several seconds to a few minutes, to create an initial midthickness surface for each subject. This should be included in the total runtime. A breakdown of runtime should be reported and compared to SOTA baseline approaches. 


[1] Zheng, H., Li, H. and Fan, Y. Coupled reconstruction of cortical surfaces by diffeomorphic mesh deformation. Advances in Neural Information Processing Systems, 2023.

[2] Ma, Q., Li, L., Robinson, E.C., Kainz, B. and Rueckert, D. Weakly Supervised Learning of Cortical Surface Reconstruction from Segmentations. arXiv preprint arXiv:2406.12650

[3] Chen, X., Zhao, J., Liu, S., Ahmad, S. and Yap, P.T. SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI. MICCAI, 2023.

Limitations:
The authors have addressed some limitations, but further clarity on the following aspects would be beneficial:

1. The efficacy of SegCSR is influenced by the quality of pGT segmentations. More discussion on how to handle low-quality segmentations would be helpful.
2. The constraint on inter-mesh consistency of deformation might affect the anatomical fidelity of pial surfaces. Further exploration of this trade-off is necessary.
3. The method could be tested on more diverse cohorts to demonstrate its efficacy across various imaging qualities and subject demographics.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors proposed a novel new method to jointly reconstruct multiple cortical surfaces using weak supervision from brain MRI ribbon segmentation results, which deforms midthickness surface deformed inward and outward to form the inner (white matter) and outer (pial) cortical surfaces. The proposed method is evaluated on two large-scale adult brain MRI datasets and one infant brain MRI dataset, demonstrating comparable or superior performance in CSR in terms of accuracy and surface regularity.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Propose a new weakly supervised paradigm for reconstructing multiple cortical surfaces, reducing the dependence on pGT cortical surfaces in training, unlike existing DL methods.
2.	Design two loss functions to optimize the surfaces towards the boundary of the cortical ribbon segmentation maps, along with regularization terms to enforce the regularity of surfaces.
3.	Conduct extensive experiments on two large-scale adult brain MRI datasets and one infant brain MRI dataset.

Weaknesses:
1.	It seems overclaim in the manuscript. The ‘pseudo’ ground-truth surface mentioned in the manuscript is actually the ground-truth mesh in other approaches, obtained by Marching cube/Free surfer. Since the chamfer distance is used to guide the network training, why do the authors claim the proposed method is weakly supervised?
2.	It is not clear how the original images are overlaid with the predicted mesh. Is any registration used? Details are missing.
3.	It seems the main contribution of the proposed SegCSR is the boundary loss function?

Limitations:
The limitations are discussed in the msnuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a deep learning approach to jointly reconstruct multiple cortical surfaces using weak supervision from brain ribbon segmentations derived from brain MRIs. The method leverages the midthickness surface and deforms it inward and outward to fit the inner and outer cortical surfaces by jointly learning diffeomorphic flows. Regularization terms are included to promote uniformity, smoothness, and topology preservation across the surfaces. Experiments are conducted on large-scale adult and infant brain MRI datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The approach is novel in its use of weak supervision from readily available segmentation datasets, which reduces the burden of preparing pseudo-ground truth surfaces.
- The paper is well-written and structured, with a clear motivation for the method.
- The methodology is explained in detail, and the experiments are comprehensive.
- The approach has the potential to democratize the use of deep learning in cortical surface reconstruction by leveraging existing segmentation datasets.

Weaknesses:
- The paper's central contribution of weak supervision is undermined by the fact that the model is trained on pseudo ground truth surfaces for white matter and pial surfaces.
- The experimentation is limited to brain cortical surfaces and MRI images. Broader experiments involving different anatomies (e.g., bone cortical surfaces, heart walls) and imaging modalities would enhance the paper's impact.
- Results lack statistical significance analysis to validate sub-millimeter reconstruction errors.
- There is no evidence showing that improvements in mesh reconstructions correlate with enhanced performance in downstream analysis tasks.
- The robustness of the method regarding input noise/perturbation and images from multiple centers is not evaluated.
- There is no analysis of the computational complexity, including the resources and time savings provided by the proposed weak supervision.
- There is no sensitivity analysis on the choice of weights used to weigh the different components of the overall loss.
- The impact of ribbon segmentations quality (e.g., voxel spacing) as weak supervision is not investigated.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel deep learning method for the reconstruction of cortical surfaces from 3D MRI. The proposed method follows an approach learning explicit surface deformations, in which a CNN is used to predict three velocity fields, corresponding to the pial, white matter and midthickness surfaces. Unlike previous techniques which use cortical surface pseudo ground truth (e.g., generated using FreeSurfer), the proposed method trains the network with faster-to-obtain segmentation pseudo ground truth. In addition to the standard surface prediction losses (based on Chamfer distance), the method uses 1) an Inter-Mesh Normal Consistency loss that encourages the pial and WM surface to be locally parallel, 2) an Intensity Gradient loss that place the surfaces at regions of high intensity gradients, 3) a Cycle Consistency loss enforcing inverse consistency between the midthickness-to-pial deformation and the midthickness-to-WM one, and 4) a Mesh Quality loss that helps having regular surface meshes (uniform sized triangles and smoothly varying normals). The method is evaluated on the ADNI, OASIS and BCP datasets, where its performance is compared to that of implicit and explicit approaches. Results show that the method obtains a better reconstruction accuracy compared to other techniques trained in a weakly supervised setting (pGT segmentation mask), but a lower performance than those trained with pGT cortical surfaces.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The proposed method differs from previous approaches that explicit surface deformations by predicting a midthickness surface and incorporating additional loss terms that compensate for the weak supervision of pGT segmentation.

* Experiments, involving three different datasets and comparing against several recent baselines, as well as including various ablation variants, are well designed. Results indicate superior performance in the weakly supervised setting.

Weaknesses:
* The main motivation of the proposed method is doubtful. Authors motivate the need for their weakly-supervised cortical reconstruction method by the ""prolonged processing time for generating pGT surfaces"". However, as the pGT cortical surfaces can be generated automatically in an offline step, I believe the argument is weak. Moreover, recent pipelines for brain image processing, such as FastSurfer, can extract surfaces with comparable accuracy in a fraction of the time.

* The accuracy of the proposed method is considerably lower than approaches which train on cortical surfaces. Furthermore, while it produces fewer topological artifacts like self-intersecting faces, those can be removed via post-proicessing in implicit methods like DeepCSR. Combined with my previous comment, the advantages of the method are unclear.

* The ablation study in Table 2 indicates that most of the proposed loss terms have limited impact on the overall performance. For example, adding the Mesh quality loss seems to actually degrade performance in terms of CD, ASSD and HD.

Limitations:
Limitations are reasonably identified in the Conclusions section of the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uvvVjWP1aj;"REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
up0qqbdeQu;"REVIEW 
Summary:
The paper supplies a post hoc method to tune the ResNet based CLIP method on multi-label recognition task. Firstly, the method includes class concept representation, which is an alternative of the default prompt “The photo of a {class}”. It is the average of class description sentence embedding from a text description source (MSCOCO and git3.5 generated caption in the paper). Secondly, the paper proposed a sequential attention to iteratively transfer the the visual features to align with the class concept representation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Training-free enhancement: The proposed method significantly improves zero-shot and prompt-tuning performance without requiring additional training or labeled samples, making it computationally efficient.
2. Robust performance: Experimental results on multiple benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) show substantial performance gains, demonstrating the method's effectiveness.

Weaknesses:
1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.
2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.

3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.
4. Ambiguous terminology: The paper uses the term ""training-free"" in its title, yet it describes the approach as ""test-time adaptation"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.
5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.

Limitations:
see disadvantage and question

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a class concept representation for zero-shot multi-label recognition in a label-free manner and introduces a context-guided visual feature that enhances the alignment of the visual feature of VLM with the class concept.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper presents a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
2.	This paper proposes a context-guided visual feature, which is transformed onto the same text feature space as class concepts using sequential attention, to better align multi-modal features.
3.	The method presented in this paper synergistically enhances the performance of ZSCLIP and other state-of-the-art just-in-time tuning methods, with a minimal increase in inference time.

Weaknesses:
1. Tip-adapter is the proposed training free method in 2021, it would be better to choose the newer training free method in few shot setting.
2. It would be more appealing to emphasize label-free in the abstract.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to adapt without training a large vision-language model for the task of multi-label recognition. They introduce a class concept representation, based on averaging the representation of image descriptions relevant to each class, to replace simple hand-crafted text prompts (e.g., “a photo of {class name}”). Furthermore, they propose to use a context-guided visual process to align visual features with the class concept representation. Experiments conducted on several benchmarks and in zero-shot and partial labeling settings show state-of-the-art performance compared to relevant baselines. Combination with some baseline methods further shows the improvements that can be obtained with the proposed method. Ablation studies show the contribution of each component of the method and the sensitivity to some of the method's parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method achieves state-of-the-art performance
- The proposed method does not require training and can be seen as a form of test-time adaptation
- The method can be combined with existing prompt-tuning methods

Weaknesses:
- Parts of the method descriptions, especially the Context-Guided Visual Feature, are unclear
- The method relies on thousands of text descriptions relevant to the target classes, which could hinder the scalability of the methods with a large number of classes

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes class concept representation for zero-shot multi-label recognition. The paper also proposes context-guided visual representation, which is in the same linear space as class concept representation, with sequential attention. Experiments show the proposed methods improved the performance of zero-shot methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper uses class concept representation for training-free multi-label recognition tasks
2. The paper proposes context-guided visual feature using sequential attention.
3. Experiments show the proposed methods improved the performance of zero-shot methods.

Weaknesses:
1. The class concepts from averaging the vectors of text descriptions need to be verified. E.g. What text/image embeddings are the closest to the class concepts? What clusters do the concepts belong to? Since taking the average for class concepts ""was guided by the prior work on prompt ensembling [4]"" L280, it is not a novel representation for class concepts. 
2. Eq 2,3 needs further explanation. What is ""t"" in the equation? If ""t"" is transpose, what dimensions are swapped for a tensor T? Take k=1 as an example, how do the dimensions change in each step of the equation? In experiments, there should be ablation studies on G and the value of each Mg. Also, is T randomly reshaped? It would be better to have ablation studies on random reshaping or reshaping by clusters.
3. What is the implementation detail for partial label learning with the proposed method?

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
unMRtFfNhF;"REVIEW 
Summary:
The paper studies the computational complexity of data debugging, defined as finding a subset of the training data such that the model obtained by retraining on this subset has better accuracy. The paper focuses on linear models and investigates various loss functions, showing that in some cases, the debugging problem is NP-complete, while in others, it can be solved in linear time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper analyzes the computational complexity of linear classifiers with both fixed and non-fixed loss functions. First, it shows that in the general case of non-fixed loss functions and dimensions, the problem of debugging is NP-hard. However, for hinge-loss-like functions, depending on the dimension of the features and the sign of the intercept, the problem could be either NP-hard or solvable in linear time.

This result also implies that it is not accurate to estimate the impact of a subset of training data by summing up the scores of each training sample in the subset if we assign each sample point a scoring number.

Weaknesses:
- The model studied in the paper is limited to linear classifiers, which is very restrictive.

- Most of the manuscript is devoted to proving the theorems rather than discussing and interpreting the implications of the results.

- The setting involves debugging for any possible test point, which is far from practical.

Limitations:
The paper does not have any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work investigates the computational complexity of the data debugging problem, i.e. the problem of identifying a subset of training data that, when removed, improves model accuracy on a given test point. Via standard complexity theoretic reductions, it establishes the NP-hardness of this problem for linear classifiers trained with SGD under various conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well written with the key problem being well motivated. The proofs are succinct and easy to follow.

Weaknesses:
My key concern with the paper is that the constructions underlying the hardness results are immensely contrived and far removed from how classifiers are actually trained in practice. This casts severe doubts as to whether the key results of the paper implies anything significant about the hardness of data debugging as is performed in practice. To elucidate a few instances:

1. **Theorem 3.1**: The reduction to Monotone 1-in-3 SAT in Theorem 1 hinges on an adversarially constructed loss function (Line 186), one that is very far removed from any loss function I’ve seen used either in the learning theory literature or in practice.
Beyond this, it also requires a specific parameter initialization and learning rate, both of which, to my knowledge, are far removed from the typical random initialization schemes and learning rate scaling rules used in practice and analyzed in theory. For instance, in Line 198, the first $m$ co-ordinates of the learning rate $\eta$ is set as $5$, the next $n$ are set as $\frac{1}{6N}$ while the next $m$ are set as $2000N$. This seems very very removed from any learning rate schedule either used in practice or analyzed in theory. This makes me severely doubt whether the result has any meaningful implication on the inherent hardness of the data debugging problem.\
In addition, the training set and test data point are also constructed adversarially (the latter is perhaps this is to be expected I wouldn’t perceive that in isolation as a major weakness)

2. **GTA Algorithm**: The correctness of the GTA algorithm is proved only for the linear case (which is honestly quite straightforward) and hinge-like losses for $\beta \geq 0$ and dimension $d=1$ (which is of limited interest as most statistical learning problems are high dimensional). This is particularly concerning as the paper does not perform any empirical evaluation of GTA. 

3. **Theorem 4.3** While the analysis for the hinge loss is certainly more interesting than Theorem 3.1, the result suffers from a key weakness that the data ordering is adversarially chosen (In particular, the positioning of $(x_c, y_c), (x_b, y_b), (x_a, y_a)$ is crucial to the reduction). It is well known in the theory of optimization that adversarial data orderings lead to provably worse convergence in practice [Safran and Shamir COLT 2020; Das, Scholkopf and Muehlebach NeurIPS 2022]. In fact, adversarial data ordering is even the basis of an attack on deep neural networks [see Shumailov et. al. “Manipulating SGD with Data Ordering Attacks” NeurIPS 2021].\
The adversarial data ordering considered in the result differs both from the practical implementation of SGD which samples the data points in each epoch as per some random permutation [see Ahn and Sra NeurIPS 2020 and references therein] or the canonical version of SGD considered in theory where data indices are sampled uniformly with replacement [Bubeck 2015]. To the best of my understanding, the result does not hold for either of these commonly considered variants of SGD. Furthermore, the training data and test data is again, adversarially chosen. 

4. **No Analysis for Cross Entropy** : The paper does not contain any analysis for the binary entropy loss which is what is commonly used to train classifiers, further limiting the scope of the results. 

5. **Complete Lack of Empirical Evaluation**: While this wouldn’t be a weakness by itself, the fact that the theoretical results in the paper have limited applicability (as argued above) makes me quite concerned about the absence of empirical evaluation on real world settings, or, for a start, even toy-like settings where the data is drawn from plausible distributions, the learning rates and parameters are initialized as one would normally expect them to be, and SGD is run either with replacement or without replacement and not with an adversarial data ordering. 

While the paper studies a question which I found interesting, I believe the limited applicability of the theoretical results, the absence of experimental evaluation as well as the limited technical novelty of the proofs (the proofs, although crisply written, are based on straightforward complexity theoretic reductions and do not unearth any nontrivial mathematical insights regarding SGD or linear classifiers) makes me confidently feel that the paper is currently not ready for acceptance at NeurIPS.

Limitations:
Mentioned in Section C as per the checklist. Also refer to comments above in Weakness Section.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the computational task (coined as ""data debugging"") of finding training data subsets that yield different test point predictions. The focus of the paper is the SGD learning algorithm with linear classifiers. The paper shows that:
 - For general loss functions (to transform yw^Tx to a loss value), the data debugging task is NP-hard. 
 - For linear loss functions, the data debugging task can be solved in linear time.
 - For ""hinge-like"" loss functions, the data debugging task with more than two features is NP-hard.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proofs look correct
- The proofs are creative and clever
- Generally speaking, understanding the relationship between training data and model predictions is important

Weaknesses:
Clarity and exposition:
- Many of the proofs have ""magic constants"" which makes them harder to understand. The authors may consider generalizing some of the results.
- The proofs look correct, but are not optimized for being easily read and understood in terms of logical flow or notation.
- Similarly, it is hard to extract the intuition from the proofs.
- The implications of results for the wider community are somewhat muddled: while the computational hardness of exactly finding ""bad"" training points according to a particular definition is interesting, it's unclear what the relationship is to scoring based methods is since any selection algorithm could be identified with a scoring method (output 1 on selected points, and -1 on the rest), and ""CSP-solvers"" and ""random algorithms"" are mentioned without any particular explanation/exploration.

Significance:
- The loss function used to prove Theorem 3.1 is rather pathological. The derivative is zero at most places, but has a few intervals with derivatives with wildly varying orders of magnitude: N, 1, and 1/N. Reading through the proof, this pathological loss function seems critical and not easily removed. I would find it much more significant if Theorem 3.1 could be proved for convex loss functions, or similar.
- The assumption of an adversarial training order in Theorem 4.3 seems unreasonably restrictive. The user is changing the dataset by removing points, why can't they change the training order too? Again, this piece seems critical to the proof, since without the adversarial training order, I think the constructed data debugging task would be trivially solvable by taking a gradient descent step on (x_c,y_c) first. Can the theorem be extended to the user choosing the training order? Furthermore, the practical problem of data selection/cleaning is with regards to the presence/absence of datapoints, not the training order which is an optimization consideration. Can the theorem be proved not for a specific optimization algorithm like SGD, but for the minimizer(s) of the loss instead? A unique minimizer (for the hinge-loss and convex losses more generally) could be guaranteed by adding a small strictly convex regularization term.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uNZpvFlsg9;"REVIEW 
Summary:
The paper introduces a novel unsupervised evaluation method for large language models (LLMs): it uses a peer-review mechanism of a models' anonymized answers by other models. The approach assigns a (learnable) capability parameter to each LLM and solves a constrained optimization problem to maximize the consistency between capabilities and scores. The end result is a ranking of the evaluated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduce a novel approach to an important practical problem: ranking the quality of the ever-growing number of open- and closed- source LLM available to the public. The paper is reasonably easy to follow, and the empirical results appear to be sound.

Weaknesses:
The paper could be further improved on several directions:
1) you should dedicate a full section to the iterative elimination of models; what is the benefit of eliminating the weaker ones rather than keeping them around? how di you come up with the threshold of 60% to remove? can you learn this threshold automatically? is this threshold optimal for these 15 models? what happens if you start with, say, 100 models? what happens to your results (and the curves in Fig 5) if you stop earlier (all three metrics, not just CIN)? What if you continue to eliminate all models until you are left with one? is there any relationship between the order in which the models are eliminated and their final rank?
2) are there any scaling issues for 100, 1K, 10K, or 100K models? how about cost: is it cheaper to fine-tune a ""baseline"" model than to pick the best one out of 10K candidates?   
3) while the three metrics you use are meaningful, you should also present results for Precision@1 and -say- RBP@3; after all, we care a lot about identifying the the top models
3) Fig 5 should be extended to nine graphs (3 metrics * 3 datasets); for each of the 9 graphs, you should also show illustrative three ranked lists: PiCO's, PRE's, and the target one. As always, the devil is in the details: not all ""CIN = 1"" are created equal. Performance-wise, it is almost irrelevant if you have the bottom-2 models inverted; no necessarily so if the inversion is between the top-2 models

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how to estimate LLMs' performance ranking without human preference annotations. In particular, it proposes to leverage three metrics (PEN, CIN, LIS) to evaluate the estimation quality, gives an estimation mechanism that first asks a list of LLMs (called ""reviewers"") to rank pairwise answers to user questions independently, and then aggregates their ranking via a weighted sum approach. A consistency optimization determines the weights of each reviewer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of LLM evaluation without human annotations is critical in resource-limited applications. The most important and interesting contribution of this paper, in my opinion, is proposing the problem of estimating the performance rank of LLMs instead of any metric of an individual LLM. The paper also reveals an interesting assumption that better reviewers are expected to be better answer generators, which leads to their consistency optimization approach. Overall, the paper is well-written and easy to follow.

Weaknesses:
While I find the proposed problem interesting, there are still a few limitations, unfortunately.

***Unclear implication of ground truth ranking***: The technical part of the paper starts by introducing a ground truth ranking (equation (1)) without giving its physical meaning. It simply assumes ""[...] alignment with human preferences"", but it is not clear what human preferences mean in this context. 

***Evaluation metric is strange***: One of my major concerns is on the choices of evaluation metric. All the three proposed metrics, PIN, CIN, LIS, in the authors' own words, seem originally used for time series comparison. However, the goal here is to compare rankings, not time series. Thus, it is unclear why we should not use the standard ranking comparison metrics, e.g., Spearman's rank correlation coefficient or Kendall rank correlation coefficient.

***Consistency optimization algorithm is not provided***: The core of the proposed ranking estimation method is the optimization problem (7). It does not seem to be a standard optimization problem, but I could not find (even a discussion on) any clue on how to solve it in this paper.

***An optimal solution to the consistency optimization formulation can be useless***: I find the following optimal solution to the problem (7): just set weight w to be 0 for all LLMs. It is an optimal solution as G and w are identical and thus the objective is always maximized.  However, this solution is undesired. I probably misunderstood something, but this seems to suggest the formulation is incorrect. 

***Consistency optimization formulation seems brittle to query distribution biases***: Another problem with the formulation is that it seems brittle to data distribution bias. E.g., suppose M1 is indeed better than M2 for some query q. And let us replicate many copies of q in the dataset D. Then the grade G1 can be arbitrarily large. In other words, the grade of an LLM is proportional to the number of battles involving it in the dataset D, which should not be the case.

***Choices of LLMs for evaluation***: In line 547, the authors write ""For our analysis, we meticulously selected 15 LLMs"". What is the principle of the meticulous selection? Other than open-source and close-source, the selection is quite arbitrary. For example, I am quite surprised to not see GPT-4 and Claude included in the reviewer LLMs.

***Comparison with a simple baseline***: One simple baseline is to ask a powerful LLM(e.g., GPT-4, Cluade-3) to give a preference for each answer question pair, and then take the vote to determine the ranking. I would suggest to compare the proposed method with this simple baseline.

Limitations:
No. The limitations are not well discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a more reliable evaluation system to rank the abilities of different large language models (LLMs). Previous evaluation methods typically suffer from two main drawbacks: (1) benchmark data leakage and (2) cost-intensive and potentially biased human evaluations. To address these issues, the authors introduce an unsupervised evaluation mechanism to measure the abilities of LLMs and derive their rankings. The core idea of this mechanism is to first collect the answers from each LLM, then treat each LLM as a 'reviewer' to rate the quality of the other LLMs' answers, and finally optimize the internal agreement of the reviews among all LLMs. They also conduct experiments on three datasets to validate the effectiveness of their proposed mechanism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Unsupervised Evaluation Method: The paper introduces PiCO (Peer Review in LLMs based on Consistency Optimization), a new unsupervised evaluation method that leverages peer-review mechanisms to measure the capabilities of LLMs automatically, particularly without human-annotated data. The unsupervised nature also makes it scalable and less subjectively biased.
2. Consistency Optimization Framework: The proposed approach includes a constrained optimization method based on the consistency assumption, which helps in re-ranking LLMs to align more closely with human preferences.
3. New Evaluation Metrics: The paper proposes three new metrics—Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS)—to evaluate the alignment of LLM rankings with human preferences. These metrics can further inspire future work.

Weaknesses:
1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, ""Does this assumption always hold true in practice?"" I suggest the authors further discuss the applicability of their method.
2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?
3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.
4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The highlight of the work is the proposed method of evaluating Large Language Models without relying on human feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed evaluation method is a novel attempt of automating the LLM improvement process. Such method worth further exploration. It could be adapted to many of the LLMs and potentially bring us more insights.
- By eliminating the involvement of human, the proposed evaluation method limits the bias brought by human labelers. The observations presented are also interesting as LLMs can sometimes surprise us.
- Great presentation and visualization.

Weaknesses:
Some of the equations and notations in the paper seems unnecessarily complicated, which can be reorganized when polishing.

Limitations:
The idea is straightforward and make sense to me. But LLMs can be trained to bypass such systems, which may lead to potential fairness or security problems.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
u1b1dJtyxc;"REVIEW 
Summary:
The paper studies the existing ""brain score"" approach of evaluating how similar LLM representations are to human brain activity. First, they show that when using shuffled train-test splits on the Pereira dataset, which some prior studies use, a trivial temporal auto-correlation model performs similarly to GPT2-XL. Second, they show that untrained GPT2-XL's brain score is simply due to encoding sentence length and position. Third, they show that a trained GPT2-XL's brain score is largely explained by sentence length, sentence position, and static word embeddings, which are all non-contextual features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper studies the important topic of understanding why recent research has shown similarities in LLM representations and human brain language activity. 
2. They highlight issues with existing neural datasets commonly used in the LLM-brain field, e.g., shuffled train-test splits on the Pereira dataset.

Weaknesses:
From most to least significant:
1. The paper writes: ""OASM out-performed GPT2-XL on both EXP1 and EXP2 (Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption of multiple previous studies [2, 11, 10] that performance on this benchmark is an indication of a model’s brain-likeness"" (Lines 173-177). 
- I agree this shows that a model that exploits temporal auto-correlation, OASM, can achieve similar neural predictivity on the Pereira dataset as GPT2-XL. However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to temporal auto-correlation, rather than linguistic similarity. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to each factor. Although GPT2-XL can theoretically exploit temporal auto-correlation artifacts, it may not be empirically doing so as it was optimized for language performance instead.
- Furthermore, OASM may be a much stronger method at exploiting temporal auto-correlation than GPT2-XL's architecture is capable of. The paper's results may highlight that the Pereira dataset is easy to ""cheat"" using temporal auto-correlation, but not that GPT2-XL or other LLMs are doing so.
2. The paper evaluates ""brain score"" using a metric they defined, out-of-sample R-squared, whereas the prior research they cite [2, 24] seemed to use Pearson correlation. Although they argue for the advantage of the metric they used, it is challenging to understand how their results relate to prior research. For example, they do not show the Pearson correlation that their OASM model obtains on Pereira, which would make it easier to compare to models in prior research. Furthermore, they only tested a single language model, GPT2-XL, whereas more recent research has used larger or different models.
- Additionally, the metric they defined seems to produce results close to 0 for GPT2-XL and less than 0.05 for all models too. In Figure 2b, the R-squared results cluster around zero, with many negative values. They obtain an average R-squared value that is positive (e.g., Figure 2a?) only because they clip negative values when averaging.
3. The paper provides a theoretical justification arguing that GPT2-XL can encode sentence length and sentence position (Lines 197-201). However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to sentence length/position, rather than contextual/semantic features. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to the two factors.
- They compared GPT2-XL to two ideal models of sentence position (SP, represented as a 4-dimensional one-hot vector) and sentence length (SL, represented as a scalar). However, these ideal models may be a much ""cleaner"" representation of sentence length/position than the perhaps noisy GPT2-XL representation of sentence length/position that may not be cleanly and linearly decodable.
4. The paper writes: ""GPT2-XL only explains an additional 28.57\% (EXP1) and 16.7\% (EXP2) neural variance over a model composed of features that are all non-contextual."" However, the paper does not provide a noise ceiling for the metric they defined, out-of-sample R-squared. Consequently, it is unclear whether the small improvements in neural predictivity is due to hitting the noise ceiling.

Limitations:
Limitations not mentioned in the paper:
1. Please see Weaknesses 1-4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors investigate the simplest set of features that can explain variance in neural recordings (fMRI, ECoG) during language processing. The authors focus on the surprisingly high alignment (""brain scores"") of untrained LLMs, but also investigate trained LLMs. The authors conclude that the predictivity performance of untrained LLMs can be explained by simple features such as sentence position and sentence length. The authors quantify the effect of autocorrelation on shuffled cross-validated train-test splits and find that predictors that account for the temporal structure in the neural data explain the data better than other (linguistic) features. Overall, the study highlights the importance of understanding why LLMs (or, any feature space for that sake) map onto the brain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is generally well-written, and the analyses are well-motivated. The topic is timely.
- The paper is very comprehensive, and provides in-depth analyses of one widely used dataset (from Pereira et al. 2018) for LLM-brain mapping studies. The paper also investigates two other datasets, but in less depth. The authors run analyses across several seeds for the untrained models, and in general, include a good amount of well-motivated control analyses.
- The analyses of trained GPT2-XL are interesting (Section 3.3), and provide a good contrast to the analyses of the untrained models.

Weaknesses:
- The authors motivate the paper with ""attempting to rigorously deconstruct the mapping between LLMs and brains"", but do not really acknowledge other work doing so. The paper lacks a short relevant work section on other studies that ask why artificial models map onto human brain data (from language, e.g., Merlin and Toneva, 2022; Kauf et al. 2023; Gauther and Levy, 2019, ...). 
- I find it very odd that the authors include ""brain scores"" in their title and also motivate the study through Schrimpf et al. 2021, but then do not replicate almost any of the analysis choices in Schrimpf et al. 2021: the feature space pooling is different, the ridge regression, the evaluation metric. For instance, sum feature pooling is motivated because ""it provides higher alignment scores"", but other studies motivate last token pooling because it is conceptually better motivated (Transformers integrate over the context). It does not feel quite right to make decisions based on ""what gives the highest alignment"", because, perhaps sum feature pooling does indeed artificially inflate scores. Either way, it is not very suitable to link the title and most of the motivation of the paper based on one instance of prior work, and then make completely different choices. That being said, the choices are definitely well-motivated in most cases, but it makes the comparison with prior work different -- which is fine, the motivation should just be changed in that case. 
- The authors should make it more clear which voxels are used in which analyses. The authors mention that unless otherwise noted, the language voxels are used (line 75), but it is not always very clear. For instance, Figure 2d clearly includes voxels across several networks.
- Regarding novelty: Kauf et al. 2023 also investigated contiguous splits on Pereira2018 as a supplementary analysis (not to the same extent as in the current paper), and also discusses the problem of temporal auto-correlation.

Limitations:
The authors discuss limitations of their study.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper paper studies the topic of neural - brain representation mappings. They focus on three neural datasetse commonly used in LLM-to-brain mapping studies: Pereira fMRI, EcoG and Blank fMRI. Specifically, the study investigates the assumptions underpinning previous positive reports about the existence of mappings between brain representations and LLM internal representations. The study focuses in particular on GPT2-XL, which was shown to perform well on the Pereira dataset in particular, with which a series of brain-activation prediction experiments are performed.

The first presented result is that when shuffled train-test splits are used, the result is very different than when contiguous train-test splits are used, with opposite patterns on which layer performs best. This is particularly true for fMRI datasets. The authors then train an orthogonal auto-correlated sequences model on the shuffled split, which out-performs GPT-2-XL despite having a completely non-linguistic feature space. The authors take this as a signal that previous results should be challenged on their conclusion that high performance on this benchmark should be taken as an indication of brain-likeness.
 
Next, the authors investigate what explains the neural predictivity of an untrained GPT2-XL model, and they fi
nd that it is fully accounted for by sequence length and position. Following-up on that, they find that these
features are also main drivers for much of the neural predictivity of a trained GPT2-XL model.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper presents a detailed study into why LLM activities may be predictive of neural activities, 'debunking' several previous claims. I think there is a lot of value in this
- The experiments seem sound (though I am not an expert in this field)
- The conclusions are interesting, and contain valuable lessons for future work on this topic

Weaknesses:
- The presentation could be improved, in my opinion. I don't always find everything completely clear. For instance, the notion of 'shuffled train-test splits' is quite important for the paper, but it is never really explained how they are specifically constructed, and how they differ from their 'contiguous' counterpart. (I can imagine multiple dimension in which one could shuffle)

Presentation suggestion: I think it may work better if the results of the different datasets are grouped together, result-by-result, rather than dataset by dataset.

Limitations:
The authors adequately address limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There is a large body of research focused on measuring the similarity between language processing in the brain and in language models. Recent studies have shown that representations from Transformer-based language models exhibit a higher degree of alignment with brain activity in language regions. However, the authors mention that this inference is valid only for the subset of neural activity predicted by large language models (LLMs).
The primary aim of this paper is to investigate this question by analyzing three popular neural datasets: Pereira, Blank, and Fedorenko. To achieve this, the authors build voxel-wise encoding models to compare encoding performance between representations from language models and brain recordings in three settings: (i) shuffled train-test splits during voxel-wise encoding, (ii) untrained LLM representations and their alignment with the brain, and (iii) trained LLM representations and their alignment with the brain. The experimental results demonstrate that untrained language models are explained by simple linguistic features such as sentence length and position, while trained language models are explained by non-contextual features (i.e., word embeddings).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This study primarily focuses on understanding the reasons behind the better alignment between language model representations and brain recordings. The exploration of various simple linguistic and non-contextual features, and their contribution to explaining the variance in brain predictivity over contextual embeddings, is valuable for the research community.
2. The authors tested different validation setups, including comparisons between untrained versus trained models and shuffled versus unshuffled data, to evaluate brain scores. 
3. Controlling features with different combinations provided valuable insights into the contribution of each feature to the performance of brain alignment.

Weaknesses:
1. While the main research question aims to investigate the simplest set of features that account for the greatest portion of the mapping between LLMs and brain activity, the insights remain unclear for specific language regions of the brain. For instance, considering language parcels based on the Fedorenko lab, do simple features explain all the variance in these language regions? Or do these features only account for early sensory processing regions?
2. It is a well-known fact that Transformer-based representations consist of both low-level and high-level abstract features. If embeddings from language models predict brain activity and this predictivity is only due to a simple set of features, it should be better interpreted using approaches like residual analysis (Toneva et al. 2022), variance partitioning (Deniz et al. 2019), or indirect methods as suggested by Schrimpf et al. (2021).
3. Shuffling train-test splits is not an ideal scenario for brain encoding, especially for continuous language. All prior studies follow unshuffled train-test splits, i.e., contiguous time points (TRs). Shuffling the train-test split can result in sentences from the same passage being present in both the train and test sets, which is not ideal for model validation.
4. What are the implications of this study for both the AI and Neuroscience communities? What are the final conclusions?

Limitations:
Yes, the authors have presented several limitations in the conclusion. However, these limitations do not have any societal impacts on this work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the similarity between large language models (LLMs) and human brain activity by analyzing brain scores, which measure how well a model predicts neural signals. The authors question the validity of using brain scores as a measure of similarity between LLMs and human cognitive processes. They analyze three neural datasets and find that simple features like sentence length and position explain much of the neural variance that LLMs account for. They caution against over-reliance on brain scores and emphasize the need for a detailed deconstruction of what LLMs are mapping to in neural signals.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study provides a thorough examination of how various features (simple and complex) contribute to the neural predictivity of LLMs, offering a detailed deconstruction of the relationship between LLMs and brain activity. The replication of key findings using RoBERTa-Large, in addition to GPT2-XL, strengthens the validity of the conclusions drawn regarding the generalizability of the results across different LLM architectures

Weaknesses:
1.  The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper.
2. While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
tRjgapiCpm;"REVIEW 
Summary:
The paper presents a heuristic approach for evaluating the privacy of DP-SGD when only the last model iteration is released. This method contrasts with traditional analyses that consider all intermediate updates, offering a more practical assessment for scenarios where adversaries only access the final model. The proposed heuristic is experimentally shown to provide reliable privacy leakage estimates, making it a valuable tool for pre-audit privacy assessments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Focus on a good and important question.
2. Good explanation and clear paper layout.
3. Propose a new analysis neither from the theoretical nor empirical point of view.

Weaknesses:
I think the proposed method is interesting and new but I still have some questions.

1. I know the linear loss function assumption is common in theoretical analysis but it seems that the proposed method wants to have contributions in the empirical case, so why still make the linear assumption? 

2. While I appreciate the effort to introduce a Heuristic analysis, I remain skeptical about its necessity and effectiveness. The primary benefit of theoretical analysis is its precision and rigor, which often include the flexibility to adjust bounds as needed. If the goal is to find a more relaxed lower bound on privacy risks, this can often be achieved by simply loosening the constraints within the existing theoretical framework. Introducing a separate heuristic analysis seems to complicate matters without providing clear advantages. 

3. I do not think you are using a correct baseline. When you make that only the last iteration model can be seen assumption, it is not fair to use normal DP-SGD analysis. I think it is better to use the theoretical analysis from those hidden state papers you cited. I am curious if you compare your proposed method with those methods, will you still get the same conclusion? 

4. I find Table 1 in the paper somewhat unclear and would appreciate further explanation from the authors regarding its purpose and implications. The table suggests that similar levels of heuristic ε are achieved across varying batch sizes, yet there is a noticeable increase in the standard privacy budget for smaller batches to maintain comparable performance. This observation seems to underscore the well-known impact of batch size rather than demonstrating an advantage of the proposed heuristic method.
Could the authors elaborate on how this data relates to the efficacy of the heuristic analysis?

Limitations:
Please check the weaknesses.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a heuristic privacy analysis of releasing only the final model iterate of differentailly private gradient descent (DP-SGD). The analysis is based off of the worst-case differential privacy guarantee of DP-SGD with linear losses, under the assumption that the heuristic can be applied to more general loss functions in order to approximate the privacy loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The premise of the paper (a heuristic privacy analysis of releasing only the final model iterate of DP-SGD) is very interesting, and Theorem 1 is a cool result.
* The paper thoroughly assesses the limitations of the heuristic (in Section 4).

Weaknesses:
* I don’t know how useful the heuristic analysis would be in practice — beyond a lightweight sanity check — since ultimately it is just a heuristic and not a rigorous upper or lower bound on the privacy loss.

* The empirical study of the heuristic looks to be very thorough, but sparse on interpretation. I would have appreciated more discussion on the figures, and didn’t really feel like there was a strong take-home message from the paper.

* Algorithm 1 is DP-SGD with a regularizer, but in practice it is somewhat rare to use explicit regularization with DP-SGD. So I’m not sure that the heuristic would be widely applicable to the more common implementation of DP-SGD without regularization.

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a heuristic privacy analysis for DP-SGD that focuses on releasing only the last iterate, as opposed to all intermediate iterates. The authors argue that this approach is more realistic and provides sharper privacy guarantees in practical scenarios. The heuristic is based on a linear structure assumption for the model and is validated experimentally through attacks/privacy auditing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written. The paper introduces a new heuristic analysis of DP-SGD for linear loss functions and also critically examines its limitations, and identifies areas for further research.

Weaknesses:
To my understanding, this paper offers a tighter privacy accounting analysis specifically for linear loss functions. However, I find its applicability limited since it cannot be extended to general ML tasks where the loss functions are not linear. Additionally, the fact that the privacy adversary has access to all intermediate iterates of the training process makes DP-SGD overly conservative is quite well-known. The main challenge remains in developing tight privacy accounting analyses for iterative algorithms like SGD.

Limitations:
The limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide exact DP guarantees for cases where only the last iterate of DP-SGD is shared with the malicious clients, and linear models with linear loss functions are used. They propose their DP bound to be used as a heuristic that approximates the true DP guarantees for cases where more complex models are used. They show that for normal DP-SGD training, the predictions of their heuristic fall between the standard DP bound computed under the assumption that all intermediate iterations of DP-SGD are shared with the attacker, which is a strict upper bound of the true DP guarantee when only the last iterate is shared and DP-SGD with full batches and only last iterate sharing. They also compare their method against SoTA DP attacks and show that under most circumstances, their heuristic value for the DP is higher. They suggest that this is the result of the attacks not being good enough at precisely estimating the true DP guarantees. Finally, the authors demonstrate that their heuristic under unrealistic circumstances can underestimate the true DP guarantee but argue this only happens under hand-crafted losses and gradient updates, which do not happen in practical circumstances.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The last iterate setting is important
- The linear function DP bound is exact 
- The linear function DP bound has interesting properties 
- The counter-examples for the DP heuristic themselves seem interesting and probably can be adapted to other settings

Weaknesses:
- I am confused by L234. The authors propose to maximize their heuristic over all $t\leq T$, while beforehand (e.g. in Figure 1/Section 2) they advocated to computing the heuristic for a single $T$. Which one is the exact proposed heuristic by the paper?
- In Figure 2, I am not sure how we adapt existing techniques to the last-iterate-only setting? Can the authors explain in more details?
- Can the authors explain in Figure 1, what network and dataset were used?
- The authors do not provide code. I am not sure about the reason, but I will give them the benefit of the doubt that the reason is indeed related to anonymity 

**Nits:**  
- Eq. 8. I assume you do indexing from i = 1. In that case, $A_{T-i}$ should be $A_{T-i+1}$ instead. If you do 0-based indexing, even more fixes to the equation are needed.
- I believe Eq. 7 should be multiplied by $\eta$ on the right-hand side
- Equation at L442, left-hand side should be $m_T$ not $m_t$
- I believe the last equation at L459 should have $(1-q)^{n-k}$ instead of $(1-q)^{k}$. I also believe $n$ is $T$ in this equation
- The definition of $l(m)$ in L109 is confusing as $m$ is considered input to the function, while in the rest of the paper $m$ is used as a parameter. Consider putting $x$ instead.
- Consider defining the hockey-stick divergence in terms of both its pdf and cdf in the appendix to ease unfamiliar readers. I had to read quite a bit on my own to understand it. 
- Consider adding some information in the appendix as to how to deal with the mixed discrete-continuous probability for $P$. I assume many readers will be unfamiliar. 
- Consider deriving the formulas for $P$ and $Q$ in Section 4.2 in the appendix. They are not obvious. 
- Consider having an appendix section that quickly recaps how [NSTPC21] and [NHSBTJCT23] work. Their operation is critical for understanding Section 3. I ended up reading them to get an idea of what was going on there.

Limitations:
The authors acknowledge the limitations of using the heuristic to compute the DP bounds

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
tVO3b68Oyp;"REVIEW 
Summary:
This paper proposes a two-stage speech language model with semantic tokens and acoustic tokens similar to AudioLM ([Borsos et al., 2022]).
-   The semantic tokens come from a speech tokenizer that can group a variable number of frames into a single token. To train such a speech tokenizer,
    1.  This paper first takes inspirations from syllable-like structures uncovered from HuBERT, and produces an initial segmentation (Section 3.1).
    1.  An iterative process is then applied to improve the segmentation (Section 3.2).
    1.  Finally the tokens are obtained by clustering of the mean-pooled frame features (Section 4.1).
-    The acoustic tokens are identical to the HuBERT-based tokens in ([Hassid et al., 2023]), referred to as ""mHuBERT"" in this paper.

Experiments with the proposed model demonstrate the following when compared to previous work,
-   Better unsupervised syllable segmentation
-   Lower speech reconstruction WER
-   Better or competitive accuracy in speech language modelling tasks (sWUGGY, sBLIMP, tStoryCloze) with lower compute
-   Better speech continuation quality

[Borsos et al., 2022]: https://arxiv.org/pdf/2209.03143 ""AudioLM: a Language Modeling Approach to Audio Generation""
[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
-   Originality: This paper proposes an original method for producing syllable-like segmentation of speech in an unsupervised manner.
    -   It uses conditional probabilities from a masked language model instead of feature similarity ([Peng et al., 2023]) to detect initial syllable boundaries.
    -   The use of an iterative process to further improve the segmentation quality is also original.
-   Quality: This paper is well-motivated. The experiment design is sound. Ablation studies included in the experiments provide valuable insight to various modelling choices.
-   Clarity: The experiment results are reported in an easy-to-interpret manner.
-   Significance: The proposed model is an competitive speech language model with a lower inference computational cost.

[Peng et al., 2023]: https://arxiv.org/pdf/2305.11435 ""Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model""

Weaknesses:
I think the method and results in this paper would make a good paper for NeurIPS, however I cannot make a recommendation for acceptance because this paper needs substantial revision to improve its readability. A non-exhaustive list of issues making the paper hard to follow includes the following,
-   References to items not yet introduced
    -   Lines 153-154, the phrase ""our loss"" make it sound like a referrence to the masked language model loss discussed in the previous sub-section, whereas in fact it is referring to Equation (3), a yet-to-be-introduced loss for SylBoost.
    -   Lines 159-162 give a very vague description of the ""similarity matrix"" and the ""cut algorithm"" which can only be known if the reader has already seen the subsequent Section 3.3.
    -   Starting at line 188, Section 4.2 makes repeated references to ""mHuBERT"". ""mHuBERT"" appears to be name given to the acoustic tokens in ([Hassid et al., 2023]) by this paper (line 241). ([Hassid et al., 2023]) itself does not use this name, so an ordinary reader would not be able to tell what an ""mHuBERT"" model is when they work through Section 4.2.
-   Confusing terminology
    -   ""pretraining"": This paper makes a liberal use of the term ""pretraining"" to the point it's very difficult to tell which is the model being ""pretrained"". For example,
        -   Line 113 mentions a ""pretrained HuBERT teacher model"", then line 119 says ""during pretraining, the **student** model ..."". The teacher and the student are presumably not trained at the same time, yet the use of ""pretraining"" in this context make it appear that the contrary is happening.
        -   Line 225 says ""for all pretraining experiments"". A reader will have to look really closely to see this means ""training of the speech LM"", not ""pretraining HuBERT, etc"".
    -   ""Agglomeration"" vs ""SylBoost"": This paper appears to use these two terms interchangeably. Agglomerative clustering is apparently also used  (line 183). This makes it difficult for the reader to tell when ""agglomeration"" is mentioned, whether the authors intend to refer to SylBoost or just the clustering.
-   Confusing equation
    -   The unnumbered equation between line 126 and line 127 defines the similarity matrix from MLM probabilities. It makes reference to
$Y_t$ without specifying which $t \in M$ is used to define $C_{r,c}$. As a result, after having read the paper 6 times over, I still do not know how to compute $C_{r,c}$.
-   Writing style
    -   Overall the writing style of this paper is very wordy, inconcise and disorganized. Often the same message can get through with far shorter sentences. Most of the paragraphs read like a dump of the stream of consciousness of the author instead of a technical document intended for actual readers. For example,
        -   Lines 102-112 would be a lot easier to understand with formal notations and a concrete example.
        -   Lines 127-131 appear to be a mere repetition of the equation above, without any new information.
        -   Lines 242-255 contain a large amount of disorganized modelling details.

[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Limitations:
The authors adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first introduces an algorithm named LossPred that generates syllable-level speech segmentation without any training or supervision. The algorithm works by analyzing the prediction loss of speech tokens under different mask positions.

With the initial boundaries proposed by LossPred, the paper proposes further training a pretrained HuBERT / data2vec2 model by minimizing the sum of squared distances between feature vectors of each token and the average of feature vectors within the corresponding segment. This process is called SylBoost, and it further improves syllabic segmentation performance and efficiency.

Finally, the paper proposes training a Generative Spoken Language Model (GSLM) with the speech tokens obtained from quantized SylBoost units. Compared to existing GSLMs trained on other discrete representations, SylBoost encodes speech into much shorter sequences, significantly boosting training and inference efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The proposed speech representation learning and unit discovery algorithms, LossPred and SylBoost, are novel. While the idea of improving computational efficiency through dynamic or fixed-rate downsampling of speech representation is not new, this paper appears to be the first to successfully apply dynamic-rate downsampled representations with a very low sampling rate of 5Hz to Generative Spoken Language Models (GSLMs).
2. The presentation of the paper is of high quality and clarity. The authors report extensive experimental results, which effectively demonstrate that the proposed method outperforms various state-of-the-art (SotA) methods.
3. The topic addressed in this paper is significant, as very low sampling rate speech representations can benefit various tasks, including speech understanding and generation.

Weaknesses:
1. As pointed out by the authors, the proposed LossPred and SylBoost methods seem to be restricted to speech representation learning. It might be difficult to apply these methods to music, singing voice, speech with noisy backgrounds.
2. LossPred is slow in evaluating the loss prediction matrix. Each sentence requires about 200 Transformer network evaluations.
3. LossPred is highly heuristic. There seems to be no theoretical guarantee that the HuBERT model combined with LossPred reveals syllabic boundaries instead of revealing only phoneme or word boundaries.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies learning low bitrate speech units that preserves semantic information. As presented in the paper, the proposed approaches achieve SoTA performance on tasks like ASR and ZeroSpeech. The proposed approach also shows benefits in terms of compute resources — as claimed by the authors, 30x faster to train, and also benefits in terms of inference and transmission due to low bitrate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the proposed multistage approach — first using the HuBERT like model to extract syllable-like noisy segmentation, then bootstrapping pseudo-syllabic units iteratively makes sense to me. The proposed approach also shows clear benefits in terms of performance and efficiency. 

Good performance: Compared to baseline approaches like SD-HuBERT, the proposed method achieved higher accuracy on syllable boundary detection and clustering, ASR, and also shows better continuation metrics as shown in Table 7 for generative spoken language modeling experiments. All those evaluations all positively demonstrate the strong associations with syllables of the generated speech units, while it does show lier-bitrate compared to the baselines compared in the paper.
The authors also conducted ablation studies to further demonstrate a couple design choices.



Efficiency: As claimed in the paper, the proposed technique is capable of achieving extremely low-bitrate compared to the counterpart speech units, while still being able to achieve good performance in a wide range of tasks, with the efficiency in both training and inference phases.

Weaknesses:
Demonstrating efficiency: As efficiency is also one selling point of the paper, it would be great if the authors can demonstrate the training efficiency and low-bitrate benefits in a more comprehensive way, like visualizing the GPU training time vs Performance, and also bitrate vs unit quality for certain tasks.



Limited use cases: The proposed approach focuses on learning semantic units for speech applications. It’s unclear if the proposed methods can be applied to other important non-speech use cases like understanding acoustic environment, and understanding speaker’s identity and emotion. 



Understanding Unit Quality: To demonstrate the unit quality for synthesizing the audio and for generation, should the author also compare with other related works (like [1] and [2]) in terms of reconstructing the original signal? Like in [1] (see Table 1), the authors compare the different approaches in terms of reconstruction performance using a couple of metrics like MEL, STFT and ViSQOL score, and also semantic task performance.

[1]: https://arxiv.org/abs/2405.00233
[2]: https://arxiv.org/abs/2306.06546

Limitations:
Not aware of potential negative societal impacts

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an approach for extracting syllable-like units from speech SSL models for use in a transformer-based language model. The motivation is that, compared to baseline acoustic units, which tend to mimic phonetic units in their time resolution, syllable-like units have lower time resolution, which makes them easier to model using techniques from the language domain. The authors propose an adaptation of the SD-HUBERT approach to extract units that can be used in Generative Spoken Language Modeling.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors identify an important limitation of why using language modeling techniques is a challenge in the speech domain, and their proposed approach seeks to address the limitation.

Weaknesses:
Overall, I found the submission difficult to follow. Please see my additional comments below.

line 2 -> Transformers do not require the inputs to be tokenized. The tokenization step is performed so that we can use language modeling techniques in speech.

line 18 -> Generally speaking, there is no requirement for the SSL representations to be powerful or abstract. 

Line 20 -> I don't see how the example of young children motivates your SSL description from the previous sentence; the transition is incoherent.

line 22 -> What does performant mean in this case? What is the connection between composing highly realistic text and the ability of a model to provide features for a downstream task? You seem to conflate the two goals, even though they are not necessarily the same.

line 23 -> The statement on this line is not clear. Several successful speech language model methods were introduced in the literature, what about previous approaches that make them fail? Please consider clarifying.

line 31 -> The temporal resolution impacts the LM part of the problem. Why is it important if we want to extract features for a downstream task?

line 37 -> What does ""syllable-like"" mean in this case? Can you elaborate on the time resolution it represents? Why is it important to start with a ""syllable-like"" unit? What makes it suitable for GSLM? What challenges from prior work are you addressing when using ""syllable-like"" units?

Line 38 -> I would refrain from using words like ""breakthrough"" and instead let the reader decide if the improvement is indeed a ""breakthrough.""

line 48 -> I disagree with labeling your method as ""train-free"" since it relies on a pre-trained HuBERT model. 

line 51 -> The distinction between the first and second contributions needs to be clarified. If the boundaries from the first contributions are not good on their own, then why mention them as a contribution? 

Line 102 -> It is not clear how/where you do the masking. Do you do it on the raw input, mel-spectrogram, or the extracted features? 

line 113 -> Shouldn't the approach be ""train-free""? Why do we have a student/teacher model that we are training?

line 147 -> The authors must refine the motivation for why syllabic units are useful for this application. Why not use word units instead? 

line 189 -> Superior compared to what?

line 198 -> I suggest leaving any experimental details to the experiments sections.

Table 1 -> Can you try any non-neural baselines for boundary detection? What would the performance be if we used heuristics based on energy, zero-crossing rate, or changes in Prosody to get rough boundaries?

Table 1 -> What makes Data2Vec2 better than HuBERT for extracting boundaries?

Table 1 -> What happens if you apply SylBoost to Feat-Sim?

Table 1 -> Please describe the metrics and abbreviations in the captions.

Table 2 -> What does the underline represent?

line 221 -> Implement what exactly? Please re-write the sentence.

line 237 -> typo

Table 3 -> What does the underline represent?

*Estimated.-> What does estimated mean? If prior work does not explicitly give this information, then it is better to leave it out.

line 263 -> What is the R score?

line 281 -> Please present the tables in the order they are referenced in the text; you currently jump from Table 1 to Table 4 and then go back to Tables 2 and 3.

line 340 -> Communication is not the last name of the first author from [16]

Limitations:
Can the authors comment on the trade-off between resolution and ease of modeling (and quality)? What do we lose/gain using syllable-like speech units in a language modeling paradigm?

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
t8ch1OCvHh;"REVIEW 
Summary:
This paper pays attention to extremely large outliers in LLMs and further investigates the reasons behind these ""attention spikes."" Consequently, the authors propose two methods to enhance the performance of quantized models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The analysis of attention spikes is thorough and comprehensive.

2. The exploration of the relationship between attention spikes and Gated Linear Units (GLU) variants is both interesting and insightful.

Weaknesses:
1. The proposed QFeM method is not hardware-friendly, as it maintains some modules at high precision and cannot directly utilize low-bit INT General Matrix Multiply (GEMM) for activations and weights.

2. The proposed QFeP method bears a strong resemblance to a previously researched method, IntactKV[1], yet lacks a detailed comparative discussion.

3. The experimental settings are limited to W8A8 configurations, which previous research, such as SmoothQuant[2], has shown can nearly achieve lossless quantization for W8A8 models.

4. The authors have not included comparisons with state-of-the-art baselines, such as OmniQuant[3], AffineQuant[4], QLLM[5], and QuaRot[6].


[1]. Liu, Ruikang, et al. ""IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact."" arXiv preprint arXiv:2403.01241 (2024).

[2]. Xiao, Guangxuan, et al. ""Smoothquant: Accurate and efficient post-training quantization for large language models."" International Conference on Machine Learning. PMLR, 2023.

[3]. Shao, Wenqi, et al. ""Omniquant: Omnidirectionally calibrated quantization for large language models."" arXiv preprint arXiv:2308.13137 (2023).

[4]. Ma, Yuexiao, et al. ""Affinequant: Affine transformation quantization for large language models."" arXiv preprint arXiv:2403.12544 (2024).

[5]. Liu, Jing, et al. ""Qllm: Accurate and efficient low-bitwidth quantization for large language models."" arXiv preprint arXiv:2310.08041 (2023).

[6]. Ashkboos, Saleh, et al. ""Quarot: Outlier-free 4-bit inference in rotated llms."" arXiv preprint arXiv:2404.00456 (2024).

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies some of the underlying causes for why activation quantization (PTQ) could lead to low performance and suggests some methods to address these issues.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see the “Questions” section.

Weaknesses:
Please see the “Questions” section.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the precision challenges posed by the large language models (LLMs) quantization during inference, specifically focusing on the quantization errors in GLU-based feedforward networks. The authors identify that GLU variants in LLMs cause significant local quantization errors due to excessive activation magnitudes, referred to as activation spikes. They observe that GLU-implemented models have larger spikes than non-GLU-implemented models.  They propose two methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate and mitigate these spikes during quantization. QFeM leave some linear layers unquantized (usually those layers that cause large activation spikes in the first several layers), and QFeP introduce an additional prefix before the inference process. Their extensive experiments show that these methods improve quantization performance and are compatible with existing techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The identification of activation spikes in GLU-based LLMs is novel.
2. The paper is well-structured and clear.
3. The QFeP method is novel,  and is somehow similar to the finding of ""sink token"" in StreamLLM [1].

[1] Xiao, Guangxuan, et al. ""Efficient streaming language models with attention sinks."" arXiv preprint arXiv:2309.17453 (2023).

Weaknesses:
1. My major concern is about the baseline of SmoothQuant reported in Table 4. For example, In Table 7 of SmoothQuant's original paper, they report that W8A8 SQ's PPL of Llama-7B on WikiText-2 dataset is 5.515, while the authors report a PPL of 9.907 on the same dataset. Is there a specific reason about this large gap?

2. In Table 3, the improvement brought by the QFeP method does not seem significant, especially when combining with the QFeM method.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces activation quantization methods for GLU-based LLMs, which often face challenges due to activation spikes. To effectively manage these spikes and enable activation quantization using a PTQ-based approach, the paper proposes a Quantization-free Module (QFeM) and a Quantization-free Prefix (QFeP). Specifically, QFeM aims to partially bypass quantization for linear layers where large quantization errors occur. QFeP identifies the prefix that triggers activation spikes and preserves its context as a key-value (KV) cache, preventing the recurrence of activation spikes in subsequent tokens. The paper presents extensive experimental results to compare the accuracy of the quantized models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) This paper is well organized and easy to understand.
2) The proposed QFeM and QFeP effectively mitigate the impact of activation spikes on activation quantization, preserving the accuracy of LLMs even when activation quantization is applied.
3) The ablation study thoroughly examines the effects of QFeM and QFeP, providing valuable insights.

Weaknesses:
1) The perplexity/accuracy results of the baseline methods deviate from the results reported in previous papers.

2) The paper does not compare its method with the state-of-the-art LLM quantization method [1], which enables W4A4 quantization (partially using 8-bit operations) with a PTQ approach.

[1] Zhao, Yilong, et al. ""Atom: Low-bit quantization for efficient and accurate llm serving."" Proceedings of Machine Learning and Systems 6 (2024): 196-209.

Limitations:
The proposed method is limited to GLU-based LLMs.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
sxZlp9ZoHD;"REVIEW 
Summary:
The paper proposes the Retentive Network (RetNet) as a foundation architecture for large language models. RetNet has a multi-scale retention mechanism with three computation paradigms: parallel, recurrent, and chunkwise recurrent. 
The retention mechanism starts with a recurrent modeling formulation and derives a parallel formulation. It maps input vectors to state vectors recurrently and implements a linear transform to encode sequence information. Then, it makes the projection content-aware by using learnable matrices. The retention layer is defined using these matrices and a complex position embedding, combining causal masking and exponential decay along relative distance. 
It achieves low-cost inference, efficient long-sequence modeling, comparable performance to Transformers, and parallel training. Experimental results show its superiority in language modeling, inference cost, and training throughput.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The RetNet also shows competitive performance in language modeling and knowledge-intensive tasks compared to other Transformer variants and has the potential to replace Transformers for large language models.
2. Achieves significantly better inference efficiency in terms of memory, speed, and latency.

Weaknesses:
1. The paper presents the scaling curves of RetNet and Transformer with model sizes ranging from 1.3B to 6.7B, concluding that RetNet is favorable in terms of size scaling and starts to outperform Transformer when the model size is larger than 2B. However, it does not provide a detailed explanation for this trend. Understanding the underlying reasons for this performance difference with increasing model size could provide more insights into the effectiveness of RetNet and its potential advantages over Transformer.
2. The use of $\gamma$ in the RetNet may appear somewhat heuristic. The paper assigns different $\gamma$ for each head in the multi-scale retention (MSR) module and keeps them fixed among different layers. While this approach is used to achieve certain effects, such as enhancing the non-linearity of the retention layers and improving the model's performance, the specific rationale for choosing these values and the potential impact on the model's behavior could be further explained.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a linear attention model called RetNet for language modeling, which has a linear training complexity and constant inference complexity.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. RetNet has both linear time complexity and constant inference memory complexity. 
2. RetNet has a chunk recurrent form which can be beneficial for speculative decoding.

Weaknesses:
1. The authors introduce a new term called ""Retention,"" but this is essentially the same as Linear Attention without the denominator, which has already been proposed in [1].
2. Lack of comparison with the baselines on open source pretraining data. All the training experiments are conducted on in-house data mixtures, which harms the reproducibility.
3. The paper doesn't compare RetNet with other linear attention model (such as GLA, RWKV, Mamba) on downstream tasks with standard metrics instead of perplexity. Table 2 only include RetNet and Transformer. The efficiency measurment of RetNet+ is absent.
4. The evaluation on MMLU/Qasper is using perplexity but not the widely-used accuracy/F1 metric. The perplexity results don't necessarily mean that the model can make correct choices for the samples in MMLU, and has less guidance for the model's downstream performance.
5. Missing citations: The authors should also cite [1] for the normalization after retention, and discuss the details of the triton implementation of RetNet and its difference from the implementation in the Flash Linear Attention [2] library.

[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.

[2] Yang, Songlin and Zhang, Yu. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism. https://github.com/sustcsonglin/flash-linear-attention

Limitations:
No, the authors should have a limitation section to point out the strong assumptions of their approximation of self-attention and relative position embedding.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Retentive Network (RetNet), a family of efficient models that incorporate exponential decay within a linear attention-like structure. RetNet shares similarities with state-space models and linearized attention, enabling both training parallelism and O(1) inference cost. Additionally, RetNet supports chunk-wise parallel computation for efficient long-sequence training. Experimental results demonstrate RetNet achieves performance comparable to Transformers and outperforms other efficient variants on language modeling and vision tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The structure of RetNet is easy to understand and follow
- RetNet exhibits promising training and inference efficiency, and is able to scale up to 6B.
- Comprehensive evaluation on both language and vision tasks, highlighting its generalizability.

Weaknesses:
- Some experiments could be improved
- Some claims may be misleading
- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.

Limitations:
I didn't see serious problems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
s5Y3M5l1qg;"REVIEW 
Summary:
To better defend against adversarial attacks, the paper proposes a novel adversarial defense mechanism for image classification – CARSO – blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes a novel defense mechanism.

The proposed method is validated on multiple datasets.

Weaknesses:
1. The presentation of the paper is poor.

    a) In the first half of the paper, the author merely describes some background. There is a lack of analysis of existing methods, such as the shortcomings of the current methods, what problems the proposed method can solve, and why it can solve these problems.

    b) Some descriptions are unclear, such as 'Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.' I think 'may' should be removed here.

2. The current experiments are insufficient to prove the effectiveness of the proposed method.

    a) Table 2 simplifies a lot of information, which reduces clarity; for example, it only records the mean or best results of multiple methods and lacks the clean accuracy of the purification method. I suggest listing all methods according to both clean accuracy and adversarial accuracy. The existing content in Table 2 can be added as additional row information.

    b) Since the paper does not give specific problems, only a general goal, which is to better defend against adversarial attacks, the experiments become relatively limited. I believe the author should re-summarize the shortcomings of existing methods and the advantages of the proposed method and conduct more experimental comparisons.

Limitations:
The authors have discussed limitations of the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a novel adversarial defense method called CARSO. CARSO consists of two models: a classifier and a purifier. The classifier is (pre)trained to correctly classify possibly perturbed data. The encoder of the purifier is trained to generate a latent space from the internal representation of the classifier and the original (possibly perturbed) input. The decoder of the purifier is trained to reconstruct a sample from the latent representation and the internal representation of the classifier. The final prediction is determined by aggregating the outputs of the classifier for reconstructed data.

Detailed procedures are summarized as follows:

- The classifier is always kept frozen. Other parts, including the VAE and small CNNs for compression, are trained on a VAE loss consisting of a reconstruction loss based on a pixel-wise channel-wise binary cross-entropy loss and KL-div.
- The internal representation and input are compressed by small CNNs before being inputted into the encoder of the purifier.
- The classifier is pretrained according to [18] or [62].
- When training the purifier, each batch contains both clean and adversarial samples.
- The aggregation is represented by a double exponential function.
- Evaluations are conducted under $L_\infty$ attacks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The concept of blending adversarial training and purification is novel and interesting. The proposed method, CARSO, achieves robust accuracy that surpasses the SOTA adversarially trained models and purification methods, including diffusion-based models, despite its relatively simple mechanism.
- The evaluation was carefully conducted. The authors explicitly address common pitfalls in evaluating robustness. For example, they conducted end-to-end validation (full whitebox setting), addressed concerns about gradient obfuscation, and used PGD+EOT to address the stochasticity of CARSO.
- CARSO can utilize existing pretrained models, which have already achieved high robust accuracy.
- A wide variety of datasets (CIFAR-10, CIFAR-100, and TinyImageNet-200) were used for evaluation.

Weaknesses:
**1**. In my opinion, the claim that CARSO surpasses the used adversarially trained model seems questionable. If my understanding is correct, during inference, the decoder takes class information only from the internal representation of the classifier. Thus, I believe the decoder can correctly reconstruct the sample only if the classifier, outputting the internal representation, can correctly extract class information from the original perturbed sample. Could the authors clarify this?

Note: Initially, I doubted whether some experimental or evaluation settings were appropriate. However, as far as I can tell, there are no issues. Just in case, I recommend the authors review their source code again.

**2**. CARSO sacrifices clean accuracy more significantly than existing SOTA methods. Additionally, to compare CARSO and the best AT/purification models in terms of clean accuracy, Table 2 should include the clean accuracy of the best AT/purification models (i.e., the contents in Table 15). The scenario or dataset columns in Table 2 might not be necessary.

**3**. Few ablation studies. The authors should include the case of $L_2$ perturbations and use internal representations from different layers. Particularly, the relationship between the layers used for extracting representation and robust accuracy is of interest.

Limitations:
The authors explicitly addressed the limitations in Section 5.3.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper integrates adversarial training and adversarial purification to enhance robustness. It specifically maps the internal representation of potentially perturbed inputs onto a distribution of tentative reconstructions. These reconstructions are then aggregated by the adversarially-trained classifier to improve overall performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of combining adversarial training and adversarial purification is interesting.

Weaknesses:
1, The experiments are too weak. I hope the authors can refer to at least [1][2][3], which are relevant to adversarial purification, to conduct experiments from more dimensions and consider more baselines and fundamental experiments.

2, Could we just combine [1] with an adversarially-trained model to achieve similar performance?

3, Why should the classifier be adversarially trained for better accuracy?

4, Why can't we directly purify the image? Could we use an image-to-image method to purify the input image?





[1] DISCO: Adversarial Defense with Local Implicit Functions.
[2] Diffusion Models for Adversarial Purification
[3] IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks

Limitations:
The method heavily relies on training a VAE as the generative purification model.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
rog0J435OO;"REVIEW 
Summary:
This paper proposes a novel method to address the high computational and memory complexity of current large-scale transformers. By adopting a simple yet effective column-wise sparse representation of attention masks, the algorithm achieves reduced memory and computational complexity while maintaining the accuracy of attention computation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper investigates a topic of interest, given the current trend toward increasing context lengths in LLMs.
2. The method proposed in this paper is straightforward and easy to implement.
3. The paper is well-written and clearly presented.

Weaknesses:
1. It is crucial to highlight the advantages of this method over related work to help readers fully understand its significance. However, in the subsection ""Attention Optimization Techniques,"" the authors only mention the drawback of FlashAttention and discuss its relationship to their work. The introduction of other related works is confusing and makes it difficult to comprehend their relevance to this paper. The overall conclusion, ""*Both of the previously discussed solutions either compromise precision or yield only marginal enhancements in efficiency. Conversely, our proposed FlashMask is capable of delivering exact computations.*"" is general and non-specific. It is unclear which methods compromise precision and which ones only offer marginal improvements.

2. In the experiments, the baseline algorithms are limited to Vanilla Attention and FlashAttention. Are there more efficient Transformer algorithms that could be used for comparison? If not, the authors should explain the rationale behind the selection of these specific baselines.

3. As a non-expert in this field, I found the writing of this paper confusing. For instance, the initialism ""HBN"" is introduced without any explanation or context.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes FlashMask, which accelerates the masked attention mechanism that can reduce the original attention from O(N^2) to O(N) and simultaneously reduces the memory cost. Experimental results show that the proposed FlashMask significantly reduces training time without accuracy degradation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper provides a comprehensive study and analysis about the sparse attention, in terms of their efficiency. Also, this paper includes existing attention optimization like FlashAttention, explaining the motivation of the proposed FlashMask, which lies in the lack of optimization for sparse attention.

+ This paper proposes an optimization for column-based sparse attention, which significantly improves memory efficiency and reduces computational costs.

+ This paper provides a comprehensive complexity analysis, evaluation, and comparison with existing methods. It seems the authors make a lot of efforts on the proposed approach.

Weaknesses:
- Even though FlashMask achieves significant improvement in the memory efficiency of sparse attention, the key idea is similar to FlashAttention, but it is just for sparse attention mechanisms. Based on this fact, the novelty of this paper is not strong. I recommend the authors explain why the red part in the algorithm is designed and why it is unique for sparse attention.

- The authors only present optimization for column-based sparse attention. The performance for other types of sparse attention is unknown. If the proposed approach can be applied to all sparse attention, the contribution of this paper is extremely great. However, the existing version is not comprehensive.

- Based on the experiments, the practical latency is not significantly reduced as compared to other methods, even though the theoretical complexity is from N^2 to N. Besides, the authors do not provide results for accuracy.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces FlashMask, an innovative algorithm designed to address the computational and memory challenges associated with conventional attention mechanisms in large-scale Transformers. FlashMask employs a column-wise sparse representation for attention masks, significantly reducing the computational complexity from quadratic to linear with respect to sequence length. The authors demonstrate FlashMask's effectiveness across various masking scenarios and training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper presents a novel solution to a well-known problem in the field of natural language processing, offering a practical method to reduce the computational burden of attention mechanisms in Transformers.

The paper provides extensive empirical evidence to support its claims, including comparisons with state-of-the-art techniques like FlashAttention, demonstrating FlashMask's superiority in terms of speed and efficiency.

FlashMask's performance across different masking scenarios and training modalities shows its versatility and robustness, indicating its potential applicability to a wide range of models and tasks.

Practical Impact: The paper not only presents theoretical advancements but also demonstrates practical benefits, such as enabling the

Weaknesses:
The scaling ability of the proposed method deserves further verified on large scale datasets.

While the paper demonstrates FlashMask's effectiveness in specific scenarios, it may lack broader evidence on how it performs across different types of NLP tasks or diverse datasets.

The paper could provide more detailed insights into how FlashMask handles different sparsity levels and the impact on various model sizes and complexities.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes FlashMask, a modification of FlashAttention with fixed masks. The paper shows speedup of FlashAttention when using sparse masks in the attention matrix.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
FlashAttention is an important algorithm, and sparsity in the attention matrix is an important feature. Further study of these aspects is helpful.

Weaknesses:
The paper seems to make claims that are unsubstantiated by experiments. In the abstract and introduction, the paper claims speedup without sacrificing model quality. However, there is no experiment evaluating model quality in the experiments. This is a critical flaw.

Further, the contribution of the paper is unclear. Block-sparsity is already supported in FlashAttention (see section 3.3 of FlashAttention). It is unclear how this paper is different. There are also more recent works such as ""Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention"" (NeurIPS 2023), which seem to be strictly more expressive in features than this paper.

Limitations:
The paper discusses superlinear scaling in sequence length as a limitation, but is lacking in discussion of model quality.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
rcch4UsMBi;"REVIEW 
Summary:
## Overall summary
- This paper introduces GLAN, a method for enhancing LLMs by generating synthetic instruction data using a taxonomy of human knowledge and capabilities. GLAN constructs this taxonomy by decomposing knowledge into fields and disciplines, leveraging LLMs for generating a comprehensive syllabus for each subject. 
- GLAN’s scalable and customizable framework allows for easy integration of new fields of skills, highlighting its potential for ongoing improvement and adaptation.

## My opinion of the paper
- I think this is a really interesting approach to generate data that can allow LLMs to be potentially smarter. However, I am wondering if there are newer topics, for example (within the medical area, we have the new topic called ""Covid-19"".) Since GLAN is very dependent on LLMs, the main area of concern would be ensuring that the LLMs that GLAN depends on remains updated.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
## Originality
- The approach is quite interesting. The authors made use of real life scenarios, which is to use the structure of human education systems to build the taxonomy. This approach mimics the systematic acquisition of knowledge and skills in education, providing a framework for generating instruction data.
## Clarity
- Pseudo Algorithm provided and figures are easy to understand.
## Significance
- By creating a general and scalable method for instruction tuning, GLAN has the potential to improve the performance of LLMs across a wide range of tasks and domains.

Weaknesses:
## Quality
- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments.

Limitations:
Indicated in the appendix (do consider placing it in main paper), but did not mention about computation cost like what was mentioned in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces generalized isntruction tuning (GLAN), an approach for synthesizing instruction tuning data using a taxonomy-based approach. GLAN generates synthetic instruction data from pre-curated taxonomy of human knowledge and capabilities and aims to create diverse and broad-ranging instruction dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Comprehensive Coverage of Evaluation: The paper presents extensive experiments demonstrating that GLAN outperforms various popular instruction-tuned LLMs across multiple dimensions, including mathematical reasoning, coding, logical reasoning, and general instruction following.
2. Minimization of Human Involvement: The generation process significantly reduces human involvement, requiring human verification only at the taxonomy construction stage. This makes the approach scalable and less labor-intensive.
3. Customizability and Extensibility: The taxonomy-based approach allows for easy customization and extension. New fields or skills can be incorporated by simply adding new nodes to the taxonomy.

Weaknesses:
1. While the paper addresses generalization, there is a risk that the generated synthetic data might overfit to the taxonomy's structure, potentially missing out on more nuanced, real-world instructions.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GLAN, a general and scalable method for instruction tuning of Large Language Models (LLMs). GLAN employs a top-down approach to generate high-quality instruction tuning datasets. Experiments across various benchmarks demonstrate that GLAN performs comparably to other existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper focuses on the alignment of Large Language Models, which is a trendy and important topic. If the dataset is released, it will be beneficial for the community.
2. This method is easy to follow. The process is highly scalable, leveraging LLMs like GPT-4 for generating instructions on a massive scale.
GLAN allows for easy customization. New fields can be added by incorporating new nodes into the taxonomy.

Weaknesses:
The novelty is limited as similar top-down designs have been utilized in many previous works. Besides, the main experimental results in Table 1 appear mediocre compared to other methods.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a generalized way of creating instruction data. The high-level motivation is to take inspiration from how curriculum is designed for human learning into a taxonomy of subjects and use the same to prompt an off-the-shelf LLM to create data. GLAN does not need seed examples, or pre built-taxonomy like prior work. Human verification is also performed post the building of taxonomy to weed out unimportant or inaccurate divisions. The overall process is High level taxonomy -> subjects -> syllabus -> instructions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Overall strong Performance: Extensive experiments show GLAN's effectiveness in various tasks, outperforming or matching state-of-the-art models in several benchmarks (Table 1)
2. Figure 2 on scaling properties of GLAN: I found this figure quite interesting. It suggests a log linear scaling trend in performance as GLAN data is scaled up. This is quite promising.
3. Section 3.5 on Task-specific overfitting: Another great analysis section that discusses how GLAN does not particularly overfit to the training data. This ensures that the synthetic data remains generalizable across different domains.
4. Modularity of the pipeline: The modular nature of the GLAN pipeline allows for easy customization and extension by incorporating new nodes into the taxonomy without re-generating the entire dataset.

Weaknesses:
1. No use of actual human curriculum: The paper set the expectation right in the abstract of using/getting strongly inspired from human curriculum. I was disappointed that the method does not utilize existing human curriculum structures, potentially missing out on years of insights in developing the same. Generating synthetic data, and in this case entire taxonomies from pre-existsing models can lead to extremely large amounts of bias. I would have much rather seen the authors delegate only lower level questions to LLMs than high level abstractions, which would lead to a trickle down effect on every single node in the taxonomy. This study, in my opinion, is incomplete without using either human generated taxonomies, and/or a comparison between how different the taxonomies are.
2. Computation cost not compared: The paper does not provide a comparison of computational costs with similar methods, such as WizardLM. For instance, GLAN training required approximately 8 days using 32 A100 GPUs to generate 10 million instructions, but no direct comparisons are made to illustrate the efficiency or cost-effectiveness relative to other approaches.
3. The method is limited by the performance of GPT-3.5/4: The quality of the generated taxonomy and syllabus heavily depends on the capabilities of the underlying LLMs used in the process, namely GPT-3.5 and GPT-4. In general, GLAN does not inform how we can improve capabilities of models beyond GPT4. But also, does not consider the cost of generating 10 million instructions.
4. High variability in results (Table 2): There is significant variability in GLAN's performance across different categories, with particularly weaker results in humanities and social sciences compared to STEM fields. The authors should address this, also discuss the document proportion of each taxonomy, and potentially see if there is a correlation between the data size and performance.

Limitations:
Please see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
rSSpEmrN0C;"REVIEW 
Summary:
The paper introduces a novel approach, named LayTextLLM, for document understanding tasks, which efficiently integrates spatial layouts and textual data within LLM. It employs a Spatial Layout Projector and introduces two innovative training tasks: Layout-aware Next Token Prediction and Shuffled-OCR Supervised Fine-tuning. Extensive experiments demonstrate significant improvements over previous state-of-the-art models in KIE and VQA. This paper demonstrates the importance of layout information in document understanding tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces a novel approach by integrating SLP and P-LoRA to effectively encode and process layout information. This method significantly improves the interaction between spatial layouts and textual data within LLM, providing a new direction for future research.
2. The paper proposes the LNTP task and SSFT task to enable the LLM to layout information, thereby enhancing its document understanding capabilities and improving performance on document-related tasks.

Weaknesses:
1. Due to miss the crucial visual information necessary for document understanding, this LayoutTextLLM heavily relies on OCR-derived text and spatial layouts. Other works such as LayoutLLM, layoutLMv3, introduces visual information to enhance the document understanding performance.
2. The exploration of the shuffling ratio was conducted only on Key Information Extraction (KIE) tasks. It should also be validated on Visual Question Answering (VQA) datasets to determine if the 20% shuffling ratio is optimal across different types of tasks.
3. The effectiveness of LNTP and SSFT methods should be substantiated with more ablation studies. It is recommended to fine-tune Llama2-7B directly using the existing data for a more comparisons.
4、Although LayTextLLM shows higher performance on DocVQA compared to LayoutLLM, this comparison is not entirely fair as LayoutLLM was evaluated in a zero-shot setting. Moreover, the zero-shot performance of LayoutLMv3 on DocVQA surpasses that of LayTextLLM.

Limitations:
The author has already mentioned in the limitation section of the paper that the proposed model is difficult to handle scenarios where inference relies on visual cues.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the LayTextLLM method for document understanding, which encodes text positional information in the embedding space of an LLM and trains for effective understanding of document data as interleaved OCR-detected text and bounding box information. The results show improved performance compared to prior works on the KIE tasks, as well as on VQA in many cases.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The treatment of layout information as a modality interleaved with text is logical, and the use of a projection into the LLM’s embedding space to represent bounding boxes is clever and appears to be novel. The tasks approached are important and overall the proposed method does appear to improve document understanding (though this will be more convincing if the caveats listed below are addressed).

I also appreciate the focus on open-source models and data for the method and its evaluation, making the results reproducible.

Weaknesses:
There are some issues regarding the comparisons to existing models, making it unclear how much of the observed improvement is really due to the novel method proposed.

LayTexLLM is implemented with Llama-2-7b, but it seems that many models compared to (e.g. the strong-performing LayoutLLM) may use other LLM backbones, making it unclear whether the superior performance of LayTexLLM in many settings is due to the proposed novel method or the LLM backbone. The results will be more convincing with a comparison of different methods with the same LLM backbone (or at least an analysis of the number of parameters in each model).

It is not clear what OCR engine is used, raising the concern that different OCR engines could explain some of the gaps in performance between models being compared.

There are also issues with how the training is presented that make it difficult to interpret results. Some places (L131, L179, etc.) mention pre-training and SSFT, implying that pre-training means the LNTP training task. However, Sec 4.1 mentions “pre-training” and “SFT”, implying that pre-training refers to SSFT+LNTP and that it is followed by SFT (Supervised Fine Tuning) for particular tasks (VQA and KIE). The results also mention zero-shot and supervised results (e.g. L297), but it is unclear from the text and results tables which results are obtained zero-shot or from SFT, making it hard to understand if the comparisons are fair.

The statements about large improvements over SOTA MLMMs (L13-14, L83-84) seem slightly misleading since LayTextLLM uses OCR detections and thus is more comparable to other OCR-based methods.

LNTP (Sec. 3.2.1) is presented as a novelty but seems to just be the regular language modeling objective. If I understand correctly, this could be toned down to simply say that the added SLP and P-LoRA parameters are updated with a language modeling loss.

Limitations:
Limitations are clearly discussed in Section 5 (which should have the title “Limitations” in plural). Additionally, does the limitation of lacking visual cues apply to text formatting such as bolding or italics? This would connect well to the examples in Figure 6 where bold text is prominent.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents an innovative method for integrating layout information into LLMs to enhance document understanding tasks. Instead of treating bounding box coordinates as input text tokens, the bounding box information is embedded into a single token and interleaved with text tokens. This approach addresses the challenge of long sequences while leveraging the autoregressive nature of LLMs. Experimental results demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance and resulting in shorter input sequence lengths.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.Interleaving layout information and text is novel.

2.The proposed Shuffled-OCR Supervised Fine-tuning is interesting and may benefit other OCR-based approaches.

3.The approach achieves state-of-the-art performance on most text-rich VQA and KIE tasks, validating the effectiveness of interleaving layout and text and significantly reducing input length.

4.The paper is well-written, providing sufficient experimental details, ablations and discussions to comprehend each component of the model.

Weaknesses:
1.In layout-aware pretraining tasks, whether it is beneficial to predict both the bounding box and the text, rather than just the text. 

2.LaytextLLM achieves satisfying performance in various tasks, but it will be better to incorporate the visual modality for more application scenarios.

Limitations:
Please refer to weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents LayTextLLM, a novel approach to document understanding that effectively integrates spatial layout information and text into a large language model. Existing methods that integrate spatial layout with text often produce excessively long text sequences. LayTextLLM addresses these problems by projecting each bounding box into a single embedding and interleaving it with text.  The method is evaluated on Key Information Extraction (KIE) and Visual Question Answering (VQA) tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Effective sequence reduction: The proposed method reduces the length of text sequences, addressing a common problem in document understanding.
- Performance improvement: LayTextLLM demonstrates improvements in KIE and VQA tasks, showing performance gains over alll state-of-the-art models.
- Evaluation: The paper provides detailed benchmark evaluations on 2 tasks and 7 datasets

Weaknesses:
- Incomplete related work: The paper omits several relevant OCR-based models, such as UDOC, LayoutMask, BROS, LAMBERT, DocFormer and LiLt.
- Insufficient explanation: The repeated claim that DocLLM cannot fully exploit autoregressive features is not adequately explained.
- Limited comparisons: There is no comparison with alternative methods that embed coordinates, such as co-as-token approaches (Lmdx, Shikra, ICL-D3IE).
- Marginal token reduction: The reduction in the number of tokens appears to be limited, and the paper does not clarify whether words or lines are encoded, which could have a significant impact on token reduction.

Limitations:
- Limited comparisons: The paper primarily compares LayTextLLM to DocLLM, which may not provide a comprehensive assessment of its performance.
- Impact of token reduction: The reduction in the number of tokens, while beneficial, appears to be limited and may not provide significant practical benefits in all scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
rC0OM4Jm4v;"REVIEW 
Summary:
The paper proposes to employ text-to-image latent diffusion models to augment images through a controlled modification such that the resultant class is different from the source class. Such augmented images are referred to as hard negative images. Building upon SDEdit style image modification, the paper controls the extent of modification by adaptively determining the appropriate noise-scale for each image separately. The benefits of this type of augmentation have been demonstrated on few-shot and long-tailed imagenet classification tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper is very well written presenting the core idea of generating hard-negative images by modifying an image with a caption of another class. This idea is simple, intuitive and interesting. 
- Furthermore, the algorithm to determine the optimal noise-level for each image adaptively is not only simple and intuitive but also effective in eliminating dependence on hyperparameters. I feel that a connection can be made to the recent work [1] on phase-transition in diffusion models since this algorithm is attempting to find the diffusion-time when phase-transition occurs. 
- The evaluation is comprehensive considering a variety of diffusion-augmentation baselines as well as traditional augmentations.
- The paper illustrates the effectiveness of the adaptive search procedure through separate experiments with DINO-v2 and visualisations.
- In many cases, synthetic data generation with a diffusion model may be replaced by a simpler retrieval baseline [2]. However, the goal of this work is to use a diffusion model to search for and generate hard negatives, which is an interesting deviation from some of the previous synthetic data augmentation approaches. 

[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. ""A phase transition in diffusion models reveals the hierarchical nature of data."" arXiv preprint arXiv:2402.16991 (2024).

Weaknesses:
- From the various results in the paper, it seems that the Text2Image, GeNIe, and GeNIe-Ada achieve comparable performance with respect to each other on average. This seems to suggest that the majority of the gains can be attributed to the increased number of _distinct_ examples --- as compared to regular augmentations which simply apply different transformations to the same image --- for each class rather than the hard-negatives in GeNIe/GeNIe-Ada. 
- Additionally, it seems that beyond some threshold, any value of $r$ that changes the source-image to the target image yields comparable performance indicating that it may be sufficient to generate an augmentation that is similar to source-image and it need not specifically be a _hard-negative_.  It may be useful to consider some other applications where images lying in the boundary of the classifier may be informative: for example, see recent work on generating outliers [1] for OOD detection. 
- GeNIe-Ada algorithm is compute-intensive as compared to a simple Text2Image augmentation since it requires generating several augmentations for each source image before selecting one optimal augmentation that lies on the decision boundary. Given how close the text2image and genie-ada performances are in some cases, it may be possible that we could generate more augmentations using text2image in the same compute budget and improve over GeNIe. 
- (minor) GeNIe is applicable to the fine-tuning stage rather than the pretraining stage.

[1] Du, X., Sun, Y., Zhu, J. and Li, Y. Dream the impossible: Outlier imagination with diffusion models. NeurIPS 2024.

Limitations:
Yes, limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GeNIe, a data augmentation method for training vision models using synthetic images. GeNIe generates images by combining a source category image with a target category text prompt, selecting those that feature source characteristics but belong to the target category as negative samples. Experimental results show that GeNIe improves performance in both few-shot and long-tail distribution settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed GeNIe improves the performance in few-shot and long-tail distribution settings.
* The paper provides extensive experiments to support the claims,  including the selection of noise levels.
* The paper is well-written and easy to follow.

Weaknesses:
The key idea of GeNIe is to use image editing to combine features from two categories. Here are several questions:

* Regarding controllable image augmentation
  * Line 9 mentions that GeNIe ""retains low-level and background features from the source image."" How does GeNIe control which features are retained or changed?
  * To combine features from different categories, how about adding the attribute from the target category to the prompt? For example, a ""[dog] with [wings]"".  This method does not require carefully selection of denoise steps. 
  * Other image editing methods, such as those in [1] and [2], efficiently control image changes using prompts or user instructions.  For example, they can transform a car into a motorcycle in Figure 2, while keeping the background unchanged for more challenging negative samples. What advantages does GeNIe offer over these methods?



* GeNIe generates images ""using images from all other classes as the source image"" (line 227). Will all (source image, target prompt) pairs lead to effective image generation? Which types of pairs contribute the most to the final accuracy?

     [1] Prompt-to-Prompt Image Editing with Cross-Attention Control

     [2] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR 2023

Limitations:
The paper has discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the idea is to generate data for data augmentation by utilizing a pre-trained diffusion model. The method employs different text prompts and an adjusted noise scheduler to generate hard negative samples for the source distribution. ""GeNIe"" creates new augmentations using diffusion by leveraging source images and contradictory target prompts. ""GeNIe-Ada"" adjusts noise levels on a per-sample basis, using the classifier as the condition boundary to select the right threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method offers infinite possibilities to separate the source from the target.
- The idea is simple, original, and convincing.
- The ablation studies and experiments demonstrate strong performance.

Weaknesses:
- The method is slow, particularly GeNIe-Ada, as it requires generating an image through multiple forward passes of a diffusion model and using a classifier to select the appropriate threshold $r$.

- The number of steps required to retain low-level features is crucial for optimizing the method's performance.

- The method relies on access to a foundational text-to-image model trained on billions of images.

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel augmentation method based on diffusion models. A latent diffusion model conditioned on a text prompt generates hard negatives, by adjusting the noise level. The hard negatives can be used as challenging augmentations. The authors demonstrate the effectiveness of their approach on long-tail and few-shot settings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Well-written paper with clear contributions and presentation.
- Extensive experiments and evaluation.
- Interesting and useful idea.
- Code included in the supplementary.

Weaknesses:
I am generally happy with the paper, experiments, and presentation. A weakness seems to be the selection of the noise ratio r.  The authors propose an algorithm for this. However, I am concerned how sensitive it is for different datasets or classification settings. This might affect performance in other settings or in real-world scenarios. If this is true, it might degrade the overall method's usefulness.

Limitations:
The authors have added a section for limitations and a section for broader impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
r7mj17BKzw;"REVIEW 
Summary:
This paper introduces SuperEncoder, a novel approach to Quantum State Preparation (QSP) that aims to combine the scalability of Approximate Amplitude Encoding (AAE) with the speed of traditional Amplitude Encoding (AE). SuperEncoder uses a pre-trained neural network to directly estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state, eliminating the need for iterative parameter tuning during runtime. The authors explore different loss functions for training SuperEncoder, finding that state-oriented training using fidelity as a metric (L3) performs best. They evaluate SuperEncoder on synthetic datasets and downstream tasks like Quantum Machine Learning and the HHL algorithm, comparing it to AE and AAE. Results show that SuperEncoder achieves runtime similar to AE while maintaining the scalability of AAE, but with some degradation in fidelity. The impact of this fidelity loss varies across applications, being more tolerable in QML tasks than in precise algorithms like HHL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a novel approach to Quantum State Preparation with SuperEncoder, which innovatively combines the strengths of existing methods (AAE and AE). The idea of using a pre-trained neural network to directly estimate quantum circuit parameters is a nice solution to the QSP problem.

Quality: The research demonstrates high quality through its comprehensive experimental design. The authors explore different loss functions, provide detailed analysis of their landscapes, and evaluate the method on both synthetic datasets and real-world applications. The comparison with existing methods (AE and AAE) across multiple metrics (runtime, scalability, and fidelity) shows a rigorous approach to validation.

Clarity: The paper is well-structured and clearly written. Complex concepts are explained in an accessible manner, with helpful diagrams (like Figures 2 and 3) to illustrate key ideas.

Significance: SuperEncoder potentially represents a step towards more efficient QSP, which is crucial for many quantum algorithms.

Weaknesses:
1. The gradient evaluation of the loss function (e.g. Eq. 1) requires computing the derivative of the state $\rho$ with respect to model parameters. As the authors acknowledge, this could become complicated on real devices due to the enormous cost of quantum state tomography. The authors work around this by using the parameter-shift rule to compute the gradient. However, the parameter-shift rule does not scale as well as classical backpropagation with autodiff (see https://openreview.net/forum?id=HF6bnhfSqH -- I guess a citation to this work would be relevant here). This casts doubts on the whole scalability of this method.

2. Again related to scalability, the number of input neurons to the model has to be $2^n$. This again doesn't look too scalable past 20 qubits, which can already be realized experimentally.

Limitations:
Limitations have been discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a model, namely SuperEncoder, to solve the quantum state preparation problem. Instead of evolving the parameterized gates to generate the target quantum state, they train a model to predict the rotation parameters from the target states.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Solve the quantum state preparation problem from a new perspective.

Weaknesses:
1. Poor results. The results seem ok with four qubits but decrease way too fast when increasing the number of qubits. The proposed method is not comparable to previous methods.
2. It is actually impossible to use an ML model to predict the parameters. Since training the AAE ansatz is a non-convex optimization problem, finding the optimal parameter is indeed an NP-hard problem. There are infinitely many pairs of quantum states and parameters, and I wonder how the size of the training set would scale with the number of qubits. 
3. The training overhead is non-negligible. If we are preparing a quantum state that is beyond the simulation power of classical devices, the evaluation methods based on state fidelity would need an enormous number of quantum circuit executions, which I suspect would not be much less than training the AAE.

Limitations:
Naive ideas with poor experimental results.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the problem of Quantum State Preparation (QSP), which is critical for quantum computing but requires a circuit depth that scales exponentially with the number of qubits, making it impractical for large-scale problems. The authors propose SuperEncoder, a pre-trained classical neural network model designed to estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state. This approach eliminates the need for iterative parameter tuning, making it a significant advancement towards iteration-free approximate QSP. 

Contributions

1. Introduction of SuperEncoder, which pre-trains a classical neural network to estimate PQC parameters directly, bypassing the need for iterative updates.
2.  Provides empirical evidence that SuperEncoder significantly reduces the runtime for quantum state preparation compared to traditional methods, thus enhancing the efficiency of quantum algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See  Contributions.

Weaknesses:
1. [Scalability Issue]
The most significant drawback of this work is its poor scalability. Since the input to the SuperEncoder is $2^n$ dimensional, the number of qubits cannot be too high, such as exceeding 20 qubits. This limitation severely restricts the applicability of the SuperEncoder to larger quantum systems. Discussing potential strategies to overcome this drawback would greatly enhance the practical value of the SuperEncoder.

2. [Barren Plateau Problem]
Another major issue is that, even within a reasonable range of qubit numbers (e.g., 10-20), training the SuperEncoder is challenging due to the barren plateau problem. Consequently, the SuperEncoder is likely only suitable for situations involving fewer than 10 qubits. In these cases, the time difference between AAE and SuperEncoder is not as significant as one might expect, which greatly limits the potential impact of this work.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
qNYYb4nOKA;"REVIEW 
Summary:
* This paper studies the evaluation of information extraction, particularly LLM-based IE, in scenarios where human-annotated data is unavailable.
* The proposed evaluation framework relies on the `Needle in a haystack` evaluation. That is, an LLM is first used to generate a piece of information (needle) given the original text; then, the needle is infused into the document, and the quality of IE is assessed by whether the needle can be successfully extracted. 
* In addition to this evaluation framework, the authors also discussed several aspects to be considered when using LLM-based IE for processing long documents.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An interesting application of `Needle in a haystack` evaluation in information extraction.

Weaknesses:
* The writing quality is not great, and several areas require further clarification
	* The current paper structure is confusing; not sure what role Sections 3 and 4 play in this paper, e.g., whether the authors were proposing a new LLM-based IE approach
	* I suggest providing a formal definition of IE studied in this paper because it is very confusing to know what information is extracted. For example, in the abstract, `entity and its properties` is mentioned; in Section 3, `short paragraphs of text` seem to be the information extracted `from the continuous text`; also see Q2 
* The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)
* The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the quality evaluation of information extraction (IE) performed by large language models (LLMs). It discusses the methods to handle the input/output size limitations of the LLMs and their performance in IE. It also introduces additional scores to evaluate the extraction quality and discusses how to interpret them.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper analyses the technical limitations of LLMs complicating the extraction of information from a long context.
2. This paper presents to insert a needle into the data to evaluate the performance of IE without labeled data.

Weaknesses:
1. The analysis of the performance of LLMs in IE is not new and has various analysis, such as in the following papers:

> [1] Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness (Li et al., 2023)

> [2] Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors (Han et al., 2023)

> [3] When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks (Peng et al., 2023)

Among the papers, the authors in [3] also analysed LLMs' limitations in long context understanding, which is similar to the conclusion of this paper. 

2. This paper lacks a thorough literature review in LLM for IE as well as new evaluation formats, such as [1, 2, 3] and the following paper:

> [4] Evaluating Generative Language Models in Information Extraction as Subjective Question Correction (Fan et al., LREC-COLING 2024)

3. This paper only focuses on the NER task but lacks the other IE tasks, e.g. relation extraction and event extraction. Additional experiments are required to test the generalisability of the method. The number of samples tested is also limited (see ""# entities used for evaluation"" in Table 3).

Limitations:
See ""Weaknesses"".

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a framework to capture information extraction quality in the absence of humanly labelled and curated datasets. It explains how an approach on how to include the schema, and the role and limitations of LLM's (specifically gpt-4-1106-preview).

Experiments are done (I guess), by ""extracting information"" from long business documents originating from the healthcare sector. Several scores are presented according to the SUSWIR metrics. It delves into the ""lost in the middle"" phenomenon. It introduces the MINEA score, a newly proposed metric.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It tries to address a relevant problem in the field (curated benchmark data is hard to come by).

Weaknesses:
- The paper is from the start extremely vague and misses concrete statements and explanations about the work done. The contributions are unclear, the data is essentially undefined, for most of the work what exactly is being done is simply unclear.
	
- Even the task of ""Information Extraction"" is not concretely described in a way that is reproducible.
	
- Line 7-8: ""The framework focuses on information extraction in the form of entity and its 
	properties"". 
	
- Table 1: it is completely lost upon me what is being presented here.

-  ""We extract information from several long documents from our business case"". What are these documents? What are they originating from?

-  The scores mentioned are ""redundancy"". How is this measured? What do these scores represent? Is lower or higher better? Even these basic questions are not answered. All of this in the appendix (where it shouldnt be), and the further tables are not better.

- The work is very dry. There are no figures that explain or examplify what the problem is, or how this framework is supposed to fit.
	
- The related work section is short and doesn't address the original point (evaluation in absense of benchmark data).

- It is unclear to me how this work should contribute in any form to evaluation in the absence of benchmark data.
	
- The introduced MINEA score is ""explained"", but not examplified or mathematically defined.
	
- All examples are screenshots of data in JSON format rather than helpful explanations.

Limitations:
No. The paper does not concretely address the limitations of this metric. There are no good, bad examples provided.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an automated framework for evaluating the quality of IE tasks using LLMs. The framework introduces a scoring method called MINEA, which creates evaluation criteria by injecting artificial data (""needles"") into documents. The paper also discusses how to deal with the limitations of LLMs when processing large amounts of data, and introduces an iterative extraction process to improve the completeness of the extraction and reduce repetition.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
s1. The introduction of the MINEA score is somewhat innovative.

s2. The paper is clear explanations of the proposed framework.

Weaknesses:
w1. Lack of Originality: The originality of the paper is insufficient. Related work has already mentioned using the ""needle"" method to evaluate the information extraction capabilities of LLMs. While this paper adds the use of large models to help create the needles, the contribution is still lacking.

w2. Insufficient Experimental Description: The description of the experimental setup is missing, including the experimental environment, data sources, and dataset sizes. However, the paper spends too much space on toy examples.

w3. Unreliable Conclusions on Length Limitations: For the experiments on the input and output length limitations of models, the paper only tested one model, making the conclusions unreliable.

Limitations:
L1. The paper should provide a comparison to existing work to highlight the improvements.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
qLtLQ4KUCq;"REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
qL4nN6Ew7U;"REVIEW 
Summary:
The paper proposes Fantasy, a T2I model based fully on transformers (except for the VQGAN for the latents encoding and decoder):
* A __fine-tuned LLM__ (based on Phi-2) for the text encoding
* A image generator based on the MIM (Masked Image Modelling) approach

The training happens in two stages, a generic stage for aligning the generator the the frozen Phi-2 features, followed by a fine-tuning stage where the Phi-2 encoder is fine-tuned alongside the MIM transformer.

The results on human evaluations are convincing, putting Fantasy alongside models that require larger computational resources, while the FID results are less convincing (due to the image being smooth according to the authors).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty:
* The LLM is __fine-tuned__ but only in the second stage of training, this approach is new and makes sense

Accessiblity:
* The 2 stage pre-training is already standard practice
* The Phi-2 model is available, it is likely that this approach works for other available models (Phi-3? It could be interesting to test)
* The model size allows the model to be trained in a reasonable time

Weaknesses:
Performance:
* The FID scores are not competitive and the authors describe why: the image are smooth => it seems that the human evaluations still rank Fantasy at the top on visual appeal, but it might be that if the question was ""visual realism"" they might prefer a different model
* Results are available for 256px, and a 600M parameters MIM generator, there is no proof that this method scales (we know that diffusion models based on UNet have trouble scaling for instance)

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an efficient text-to-image generation model that integrates LLM and MIM. It demonstrates that MIM can achieve comparable performance. Unlike commonly used text encoders like CLIP and T5, this study introduces an efficient decoder-only LLM, phi-3, achieving better semantic understanding. The effectiveness of the method is validated through a newly proposed two-stage training approach and sufficient experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written with clear logic.
2. The use of MIM and LLM for image generation introduces a novel approach.
3. The two-stage training method improves the generation results.

Weaknesses:
1. The quality of the generated images does not yet match that of existing methods (e.g., pixart-alpha, SDXL), with some loss of detail. This is noticeable from the comparison in column B of Figure 5.
2. Some aspects of the methodology could be clearer, and the overall coherence of the approach could be strengthened.
3. While the proposed method demonstrates efficiency advantages, particularly in faster training convergence, this can be influenced by various factors. However, the related experiments in the paper could be more comprehensive.
4. The semantic accuracy of the generated images, a potential strength of Fantasy, is not fully demonstrated in the paper. For instance, the model's ability to handle prompts with multiple entities, color attribute descriptions, or retaining key elements in long text inputs is not adequately showcased.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a technique for training transformer based masked image modeling in an efficient way. Two main contributions include (1) use of a LLM decoder as text embeddings, and (2) Two-stage training strategy for MIM models. Experimental results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of LLMs as text encoders seem interesting.
- Two-stage training approach makes sense. First, the use of pretraining data helps the model learn a general text-image model, and the high quality alignment data can improve the quality of generations.
- Training models on low resources seem appealing.

Weaknesses:
- I don't see anything new proposed in this paper. The authors simply use Phi-2 model as text encoder with MIM models, and use two-stage training. 
- Even two-stage training is not something new to image synthesis. People have been doing aesthetic finetuning to improve image quality in diffusion models (eg. stable diffusion). The authors extend this to instruction-image data.
- The quality of generated images are not very impressive. When zoomed in, we notice a lot of visible artifacts. The generated images are also flat and doesn't have a lot of details.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To develop a resource-efficient, high-quality image generator for long instructions, the authors presented Fantasy, an efficient T2I generation model that integrates a lightweight decoder-only LLM and a transformer-based masked image modeling (MIM). 

They demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance.

By incorporating pre-trained decoder-only LLMs as the text encoder, they observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text image alignment. 

Their training includes two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. 

They conduct evaluation on FID, HPSv2 benchmarks, and human feedback, which demonstrate the competitive performance of Fantasy against other diffusion and autoregressive models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The author proposed a T2I framework that combines several more recent components and performed a series of comparisons, including both quantitative and human evaluations.

Weaknesses:
- the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.
- Efficient T2I network: there is no justification about why the network is “efficient”. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. 
- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.
- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.
- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?
- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN’s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.
- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.

Limitations:
I would encourage the authors to emphasize about the core contributions rather than combining everything together, which can hardly show significant performance improvement over existing public models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
pNQB78UDp4;"REVIEW 
Summary:
In this paper, in order to break the dominance of adapter-based methods, the authors first analyze the weakness of the previously widely-used prompt-based method, Visual Prompt Tuning (VPT). Firstly, the prompt mechanism is inherited from NLP where each token/prompt represents an actual word with rich semantic information. However, in visual tasks, tokens represent image patches and contain sparse semantic information. Therefore, simply concatenating the prompt tokens with embedded tokens in visual tasks may not provide enough information to guide the model for downstream tasks. In addition, it is difficult to get a deep understanding of spatial relationships and structural features of an image with prompt tokens, which leads to another two weaknesses of VPT. 1. The computational complexity of self-attention becomes higher when more prompts are used, which introduces computational inefficiency and redundancy. 2. extra prompts will influence the results of softmax operation in the self-attention. Most of the weight falls on the prompts and causes the destruction of self-attention between embedded tokens. 

The authors thus proposed Cross Visual Prompt Tuning (CVPT). CVPT inserts a cross-attention module to calculate the cross-attention between prompt tokens and the embedded tokens after self-attention. This module decouples the prompt and the embedded tokens to avoid the quadratically increasing computational complexity of self-attention modules and the destruction of self-attention between embedded tokens. This module allows the model to focus on the relationship between embedded tokens and the prompt tokens to adapt to downstream tasks more efficiently. In addition, the weights used in cross-attention are shared with the self-attention module and kept frozen to reduce the trainable parameters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1.	Good performance on image classification and semantic segmentation tasks.
2.	Analysis of the weaknesses of prompt-based methods and VPT.
3.	Cross-attention module to decouple the prompt tokens and embedded tokens to solve the problems of prompt-based methods.
4.	Comparison with VPT to show the weakness of VPT and strength of CVPT when more prompts are used.

Weaknesses:
1. No experiment or previous work (at least not cited) demonstrates that the prompts in visual tasks lack representation information. In fact, this is somehow counter-intuitive to your 3rd observation: Destruction of self-attention between embedded tokens. The phenomenon the authors observed in this part clearly states that there is an over-emphasized on prompts with significantly higher value. Also, in [ref1-2], a clear activation/focus shift can be observed after prompt integration, does that mean prompt actual benefits from such the over-emphasized during transfer learning? To sum up, the idea/motivation becomes ambiguous with such observations.

2. Although the author shows clearly that the sum of the prompt’s weight values exceeds 0.8. However, no experiment proves the relationship between the distribution of the weights and the model performance. The prompts are learned and updated during training to fit the downstream tasks and the weights are calculated based on those prompts and embedded tokens. Can we say that in some situations, the prompts learned a more suitable and efficient representation than the embedded tokens, and more weights are applied to them? The distribution of the weights in self-attention is a good point for analyzing the prompt-based methods. But more discussions are needed. 

3. Cross-attention should be assigned to the preliminary, not the contribution of the paper in Sec 3.2.

4. More discussions with E2VPT are acquired since the cross-attention prompt tuning is strongly associated (without additional prompts after the cls token).

5. Also there is an inconsistency in the experiment setup, in Figure 2, the authors in detail discuss the self-attention weight obtained by prompt tokens and embedded tokens, where no comparison studies are included to the new proposed Cross Visual Prompt Tuning to show different observations in order to support this claim.

6. To show the robustness of Cross Visual Prompt Tuning, it is better to demonstrate other hierarchical transformer architectures' performance (e.g., Swin). However, I noticed that CVPT might be insufficient to do so with the introduction of shifted window. More details should be included on how CVPT adapts to these structures.

[ref1] Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?

[ref2] SA²VP: Spatially Aligned-and-Adapted Visual Prompt

Limitations:
The discussion on limitations is listed in Sec. 5. No potential negative societal impact is discussed (which is applicable).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on prompt learning of pre-trained ViT in downstream tasks, and improves the widely used visual prompt tuning (VPT) by employing cross-attention techniques and weight-sharing mechanisms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper's research topic on vision model prompting technology is highly significant in the era of fundation models. The experiments are detailed, the structure of the writing is complete, and the methods are straightforward.

Weaknesses:
W1: While using VPT as a baseline, the paper sets up a scenario (e.g., Figure 2) with an unnecessarily large number of prompts, whereas the number of required prompts generally varies depending on the downstream task. In many cases (e.g., VTAB-Natural), using fewer than 10 prompts yields better results [1]. In such scenarios, considering the Flops comparison between CVPT and VPT as shown in Figure 1, does CVPT still maintain an advantage in terms of both runtime and accuracy?

W2: The paper mainly integrates the method of CrossViT from [2] into prompt learning of VPT, but does not explain the motivation behind applying CrossViT's method to prompt learning in downstream tasks. Specifically, how does CrossViT relate to addressing the three issues of VPT mentioned in Section 3.1 (i.e., why CrossViT method is effective in prompt learning, and why it is superior to other derived methods like EEVPT)? It is recommended to attempt a theoretical explanation of the necessity of applying cross-attention, or to supplement the section with experiments and analyses explaining how CVPT addresses the three issues of VPT proposed in 3.1.

W3: The paper does not provide code for reproducible results, nor does it present evidence of statistical significance (e.g., std) in tables. The authors claim in the 5th question of the checklist that they need time to organize this part. It is suggested that the authors organize the paper comprehensively before submitting it to conferences.

References:

[1] Jia, Menglin, et al. ""Visual prompt tuning."" ECCV, 2022.

[2] Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. ""Crossvit: Cross-attention multi-scale vision transformer for image classification."" ICCV, 2021.

Limitations:
The authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a variant of visual prompt tuning (VPT) where the authors suggest applying cross-attention instead of self-attention in the Transformer layers to reduce training complexity. The authors analyze several drawbacks of existing VPT approaches and claim to address them using cross-attention.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- **Identified Drawbacks**: The authors reasonably point out some drawbacks of current VPT methods, such as a “lack of adaptation to visual tasks” and “computational inefficiency.”

- **Complexity Reduction**: The proposed use of cross-attention indeed reduces computational complexity compared to the original self-attention mechanism.

Weaknesses:
- **Limited Novelty**: The proposed idea is straightforward, merely replacing self-attention with a combination of self and cross-attention. Similar concepts have been explored in previous works, such as prefix tuning (Li et al., 2021; Yu et al., 2022).

- **Limited Impact and Efficiency**: The improvement in complexity is minimal because the number of prompts is typically much smaller (fewer than 20) compared to image embeddings (196).

- **Limited Performance**: The overall performance is limited compared to some recent works by Wang et al. (2023) and Wang et al. (2024). These works, which show significantly better performance, are not compared in the paper. Therefore, the claim that CVPT “reaches SOTA” (L272) is factually incorrect.


----

Li et al. Uav-human: A large benchmark for human behavior understanding with unmanned aerial vehicles. CVPR 2021

Yu et al. Towards a unified view on visual parameter-efficient transfer learning (V-PETL). 2022

Wang et al. Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR 2023

Wang et al. Revisiting the Power of Prompt for Visual Tuning, ICML 2024

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper furthers the research on Parameter Efficient Fine Tuning on the visual tasks. PEFT optimizes a large scale model by selecting a small set of parameters. This work refines the Visual Prompt Tuning by leveraging the cross attention between the prompt and embedded tokens. Further the model uses weight sharing mechanism for better representation capacity of the cross attention. This work performs evaluation on 25 datasets for number of downstream tasks. PEFT fine-tuning can be adapter or prompt based. The adapter based methods generally outperforms the prompt based fine-tuning methods.  This paper also achieves results comparable to adapter based fine-tuning methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper well explores the shortcomings of Visual Prompt Tuning (VPT) to amend it in this work for visual tasks. 
2.  This work shows the validity on the image classification and segmentation tasks by benchmarking on VTAB-1K, FGVC and ADE20K.
3.  The ablation study in the cross-attention location is helpful.

Weaknesses:
1. The conclusion seems to more of an abstract. 
2. The implementation details can be described with more details.
3. Although the authors performed a great ablation on the cross-attention, an ablation for the self attention would have been interesting.
4. One of the base cases with null text can provide a better understanding for the effectiveness of this method.

Limitations:
In this paper, the authors discuss the limitations on the Section:5 Conclusion, where they mention about taking the same initialization strategy as VPT. VPT discusses different strategies on initialization for better optimization.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oW6s6zFYj9;"REVIEW 
Summary:
This work belongs to ANN2SNN and proposes a novel coding scheme and neuron model to enhance the efficiency and accuracy of Spiking Neural Networks (SNNs) while reducing energy consumption. The Stepwise Weighted Spike (SWS) coding scheme improves information encoding by stepwise weighting input signals and introducing negative pulses, reducing the number of coding spikes needed. The Ternary Self-Amplifying (TSA) neuron model further enhances accuracy by progressively weighting the input through residual membrane potential adjustments and incorporating negative residuals and thresholds. Introducing silent periods allows the neuron to receive more input information before firing, significantly improving accuracy with minimal latency. Experimental results on datasets like MNIST, CIFAR10, and ImageNet demonstrate that the SWS coding scheme achieves better performance with fewer coding and computing steps, performing well even in very deep SNNs and achieving accuracy comparable to Artificial Neural Networks (ANNs) with the same structure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: This work introduces the Stepwise Weighted Spike (SWS) coding scheme, which is a novel approach in the field of Spiking Neural Networks (SNNs). The proposed method compresses spikes by weighting their significance in each step of neural computation, which enhances the performance and reduces the energy consumption of SNNs. Ternary pulses are a relatively new method in SNN, so the improvement of the ternary SNN encoding method has a relatively high degree of originality.

Quality: The paper provides a comprehensive set of experiments to validate the proposed SWS coding scheme. These experiments demonstrate that the SWS coding scheme significantly reduces operations and latency compared to existing neural coding schemes. The paper outlines the parameters used during training and provides justifications for the chosen experimental settings. 

Clarity: The introduction of the paper effectively motivates the work by discussing the limitations of current SNN coding schemes and proposing SWS as a solution. The methodology is clearly presented, with detailed descriptions of the new coding scheme and the Ternary Self-Amplifying (TSA) neuron model. Important symbols and their meanings are well-explained, contributing to the overall clarity of the paper.

Significance: The paper makes a significant contribution by proposing the SWS coding scheme, which enhances the efficiency and performance of SNNs. This new method addresses critical issues such as high latency and energy consumption in existing coding schemes, making it a valuable addition to the field. By improving the encoding of information in spikes, the SWS scheme has the potential to advance the development of more efficient and lower-power computing systems, thereby providing new options for the choice of coding schemes in SNNs.

Weaknesses:
In the ImageNet experiments in Table 2, SWS and other comparative ANN-SNN methods used different baselines, which is why the '$SNN\  Acc$' results are much higher than those of the comparative methods. However, the ‘$\Delta ACC$’ does not seem to show a significant difference (except for Hybrid training and Spiking ResNet). Using the same network architecture and pre-trained weights would be more credible.

Limitations:
The authors have not explicitly addressed the limitations or potential negative societal impacts of their work. To improve the transparency and completeness of their research, the authors could consider the following constructive suggestions:

1)  Limitations:

Create a dedicated ""Limitations"" section in the paper to discuss any constraints, assumptions, or potential weaknesses of the proposed SWS coding scheme.

Reflect on the robustness of the results to violations of assumptions, such as noiseless settings, model specifications, or dataset dependencies.

Discuss the scope of the claims made in the paper, including the generalizability of the approach across different datasets and scenarios.
Address factors that may influence the performance of the SWS coding scheme, such as computational efficiency and scalability with varying dataset sizes.

Consider possible limitations related to privacy and fairness concerns in the implementation of the SWS coding scheme.

2)  Negative Societal Impact:

Explicitly acknowledge the potential negative societal impacts of the SWS coding scheme, such as privacy risks, fairness considerations, or unintended consequences.

Discuss how the technology could be misused or lead to harmful outcomes, even if not intended by the authors.

Consider mitigation strategies to address any identified negative societal impacts, such as controlled release of models, monitoring mechanisms, or additional safeguards.

Emphasize the importance of ethical considerations and responsible deployment of the SWS coding scheme in real-world applications.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel Stepwise Weighted Spike (SWS) coding scheme designed to improve the efficiency of Spiking Neural Networks (SNNs) by compressing spikes and weighting their significance in each step of neural computation. This method addresses the issues of high delays and energy consumption associated with existing SNN coding schemes, as well as the complexity of neuron models and training techniques. The authors also introduce a Ternary Self-Amplifying (TSA) neuron model, incorporating a silent period to support SWS-based computing. This model is designed to minimize the residual error resulting from the stepwise weighting process. The experimental results provided in the manuscript demonstrate that the proposed SWS coding scheme significantly outperforms existing neural coding schemes, particularly in very deep SNNs. Key improvements include reduced operations and latency, enhanced overall performance, and lower energy consumption.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	Innovative Approach: Introducing the SWS coding scheme and TSA neuron model is innovative.
2.	Performance Improvement: This paper provides experiments showing that the proposed methods outperform existing coding schemes regarding both performance and energy efficiency.

Weaknesses:
1.	Clarity and Detail: Some sections of this paper could benefit from more detailed explanations, particularly in the description of the SWS coding scheme and TSA neuron model. This would help in understanding the underlying mechanisms and their advantages.
2.	Comparative Analysis: While the experimental results are promising, there is no proof from the experimental results that the encoding method proposed is more advantageous.
3.	There are some grammatical errors in the paper. Such as the second paragraph of Section 3.3, ""The neurons only integrates input and performs stepwise weighting"". It is recommended that a uniform representation be used for ""spike"" and ""pulse"".
4.	Symbol design problem, ""t"" in Eq. (3) becomes ""n"" in Eq. (5).
5.	There are many long paragraphs and sentences in the paper, making it difficult for readers to accurately understand the meaning of the paper.
6.	The description of the problem in the third paragraph of Section 1 and the end of Section 2 is not clear, making it difficult for readers to understand the problem that the article really wants to solve.
7.	The description of the encoding method in Eq. (7) is difficult to understand. According to Eq.  (7), the encoded value $A_j$ should have no time step. However, in the experimental part, the method of this paper has 8 time steps.

Limitations:
The paper mentioned that due to the setting of the neuron's silent period, the delay increases. It can be seen from the experiments that the overall latency of the method is lower, which can be regarded as solving this limitation. At the same time, this article does not have potential negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new coding scheme called Stepwise Weighted Spike (SWS) coding scheme for spiking neural networks to enhance the efficiency and reduce the number of operations and thus energy consumption. The SWS coding scheme tackles challenges associated with temporal and rate coding, such as heightened latency and energy usage. It achieves this by compressing spikes and assigning them varying weights at each computational step. Additionally, the paper introduces the Ternary Self-Amplifying (TSA) neuron model, which incorporates a silent phase to mitigate residual errors arising from the weighting procedure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The SWS coding scheme enhances information capacity and reduces the number of spikes, leading to lower energy consumption and higher accuracy as compared to other coding schemes. The effectiveness of this approach is demonstrated using different datasets.

Weaknesses:
1. Which model of a spiking neuron is being employed in equation 3 (line 120)? What is the reset mechanism here after the neuron fires? Are the weights allowed to have negative values? The description of the model is unclear. 

2. The notion of residual error intuitively makes sense but it is confusing. Please define the residual error mathematically (line 139) for better understanding. 

3. Why ANN-(sws)SNN conversion is opted instead of directly training the SWS based SNN?

4. There are some recent works [1,2,3] with TTFS encoding which claims better results in regard to energy-efficiency and low-latency. First, these works need to be cited in the related work section. In my opinion, a detailed comparative analysis with other models and encoding schemes (for instance with [1,2,3]) needs to be carried out. 

[1] Göltz, J., Kriener, L., Baumbach, A. et al. Fast and energy-efficient neuromorphic deep learning with first-spike times. Nat Mach Intell 3, 823–835 (2021).

[2] I. M. Comsa, K. Potempa, L. Versari, T. Fischbacher, A. Gesmundo and J. Alakuijala, ""Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 8529-8533, doi: 10.1109/ICASSP40776.2020.9053856.

[3] Stanojević, Ana et al. “An Exact Mapping From ReLU Networks to Spiking Neural Networks.” Neural networks : the official journal of the International Neural Network Society 168 (2022): 74-88.

Limitations:
There is no potential negative societal impact and and one limitation related to the inclusion of silent period is noted in the main text.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a novel encoding method called Stepwise Weighted Spike (SWS) and a corresponding new neuron model named Ternary Self-Amplifying (TSA) for classification tasks utilizing the ANN2SNN training method. The proposed SWS encoding method assigns weights to the importance of spikes at each time step. The TSA neuron, which employs the SWS encoding method, features a lower threshold and includes a silent period.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Comprehensive method analysis: the authors conduct a thorough analysis of the Stepwise Weighted Spike (SWS) process, proposing a lower threshold and a silent period method to address residual error issues.

2. Superior Performance: the proposed method demonstrates superior performance in the field of ANN2SNN classification tasks.

Weaknesses:
1. Effectiveness of SWS: Various encoding methods, such as rate encoding and Time-to-First-Spike (TTFS) encoding, can be applied to different neurons and models. However, as illustrated in Figure 5, the SWS encoding method alone is ineffective without incorporating a lower threshold and a silent period. It only functions effectively when a neuron employs SWS encoding along with these additional components. Therefore, the paper should emphasize the neuron model rather than the encoding method, as it is not a universally applicable approach.
2. Lack of Experiments: The ablation study shows that the introduction of a silent period is the primary contributor to the improved performance. This raises doubts about the effectiveness of the SWS encoding method itself. Can the authors provide performance metrics for rate encoding combined with a lower threshold and silent period (if applicable) to ensure a fair comparison?

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a new spike coding scheme, which allows them to directly convert quantized ANN to their coding scheme. They demonstrate the effectiveness of their conversion on several pre-trained ANN with minimal loss in performance at the cost of an increase in latency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- strong experimental results
- coding scheme appears to be novel

Weaknesses:
- limited connection to spiking neurons, a more straightforward motivation would be a temporal encoding of quantized ANN

Limitations:
- method only applicable to conversion from pre-trained ANN
- no demonstration of training of a model using this coding scheme.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
p32gjG4yqw;"REVIEW 
Summary:
This work generalizes the ridgelet transform to equivariant neural networks, providing constructive proofs of universality in the general case as integrations over parameter distributions. Although such a direction had been taken up in prior work [33], they generalize it from scalar activations to vector activations, therefore encompassing more practical equivariant networks. The authors consider the form of the ridgelet transform for deep networks, and groups including the affine group and orthogonal group.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors provide a constructive universal approximation result, which is in contrast to many non-constructive universality results. They strictly improve on the past work of Sonoda et al [33] by extending from scalars to vectors, which is more realistic. They consider the implications of their framework on depth separations for equivariant networks.

Weaknesses:
Significance/novelty: The novelty relative to Sonoda et al [33] is limited, and the significance of this work to the universality and equivariance literatures is unclear. For example, many universality results already exist in equivariance (see e.g. work by Yarotsky [3], by Dym et al [2], etc.) — it is not clear how much value this extension of the ridgelet transform adds. 


Clarity: I found the writing of the paper extremely hard to follow. It did not provide sufficient background on the ridgelet transform, universality results for equivariant networks (whether constructive or non-constructive), or perhaps most importantly, motivation for why one should value constructive approximation theorems for equivariant networks. It felt that one had to have read the previous works by Sonoda et al, in order to grasp why this work was important or where its novelty was, such as how vector-valued equivariant feature maps are superior to scalar-valued feature maps, what exactly formal networks are, what the practical use or theoretical value of the ridgelet transform is, etc. The work would also benefit from an outline of the sections earlier in the paper, and a more concise and early statement of what the authors consider their main theorem/s. It was not clear what the central result about the ridgelet transform was, as the transform seemed to still involve an integral in all equivariant cases, without simplification. 

As a demonstration of the power of their theoretical formulation, the authors claim to show a depth separation, in which some class of networks is exponentially wide when shallow (constant number of layers), and only linearly wide when deep (linear number of layers). However, it is not clear whether they show that any shallow network is exponentially wide when representing a given function, or just the one constructed by the ridgelet transform — is this a strict depth separation?

Mathematical rigor: Although I did not check all of the math, some glaring errors stood out to me. First, the proof of Lemma 5 begins with, “Recall that a tensor product of irreducible representations is irreducible.” This is incorrect — for example, the tensor product of the irreps of the group of 3D rotations, SO(3), are reducible, and the irreps that appear in the decomposition of their tensor products are famously given by the Clebsch-Gordan coefficients (see e.g. [1]). Moreover, in the limitations section (6.1), the authors discuss the assumption that the group is locally compact, but say that this “excludes infinite-dimensional groups”. Yet, this is also false: for example, the infinite group SO(3) is compact (and therefore locally compact). In fact, several of the authors’ examples pertain to infinite groups, such as the affine group. These errors are surprising. 

Also, the mathematical techniques themselves do not appear to be novel (for instance, Schur’s lemma is quite standard, and the proofs included in the main body are rather simple — Lemmas 1 and 2 are in fact widely known), and there are no experiments or practical implications, so the merit of the paper must rest on the significance of the results themselves. Unfortunately, the the broader significance of the results are not clearly demonstrated. The authors claim to reveal “the close relationship between machine learning theory and modern algebra,” but the mathematical tools they use seem like the standard ones used already throughout the equivariance literature. I am not sure what the “major upgrade to machine learning theory from the perspective of modern algebra” will therefore be. 

[1] Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network by Kondor, Lin, and Trivedi 2018

[2] On the Universality of Rotation Equivariant Point Cloud Networks by Nadav Dam and Haggai Maron 2020

[3] Universal approximations of invariant maps by neural networks by Dmitry Yarotsky 2018

Limitations:
Yes, the authors discussed limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a unified approach to universal approximation theorems for neural networks using group representation theory. It extends to vector-valued joint-group-equivariant feature maps, providing a systematic method for both shallow and deep neural networks with nonlinear activation functions. By leveraging Schur's lemma, the paper shows that these networks can universally approximate any function within a certain class. It main contribution is the closed-form ridgelet transform, which offers a constructive proof and explicit parameter distribution for these networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The paper introduces a unified constructive universal approximation theorem that applies to both shallow and deep neural networks using group representation theory. This is an innovative approach. It also extends previous work by incorporating vector-valued joint-group-equivariant feature maps.

2.  The paper is theoretically sounding, leveraging concepts from group representation theory and Schur's lemma. They perform the thorough and systematic development of the ridgelet transform, providing a closed-form solution for parameter distributions and ensuring the findings are theoretically well justified.

3. The paper is well-structured and clearly written. Definitions, theorems, and proofs are presented in a coherent manner, making it easier for readers to follow the details of the argument and understand the implications of the results.

4.  This work is significant since it provides a relationship between deep learning theory and modern algebra. By providing a unified framework that applies to a wide range of network architectures, the paper incentivize further research and development in the field of machine learning.

Weaknesses:
1.  While the paper is strong in its theoretical contributions, it lacks empirical validation through experiments or simulations. Demonstrating the practical applicability and effectiveness of the proposed ridgelet transform and the unified framework on real-world datasets or benchmark problems would strengthen the paper. Including even a small set of experiments could provide evidence of the practical relevance and performance of the theoretical results.

2. This work makes several assumptions, such as the local compactness of the group \( G \) and the boundedness of the composite operator \( \text{NN} \circ R \). While these assumptions are standard in group representation theory, the paper could benefit from a more detailed discussion on their implications and limitations. Exploring scenarios where these assumptions might not hold or providing guidance on how to relax these assumptions.

3. Some of the technical details, particularly those related to advanced concepts in group representation theory and the ridgelet transform, might be challenging for readers who are not experts in these areas. Providing additional intuitive explanations, diagrams, or examples to illustrate these concepts could enhance the clarity of the paper.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization of the work by Sonoda et al. by extending their formulation of universal approximation theorems applicable to a specific class of neural networks namely scalar-valued joint-group-invariant feature maps for ""formal deep network"" to a much larger class of learning machines. Their theory using tools from group representation theory allows them to uniformly treat both shallow and deep neural networks with a larger class of activation functions. They provide an explicit construction for parameter assignment (aka Ridgelet Transform) and apply it to vector valued joint group-equivariant feature maps.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is well motivated and the writing is clear and understandable. The interspersed explanations in plain english are quite helpful in understanding a paper that leans quite heavily on sophisticated mathematical formalisms. (eg line 93-94). 
- The proofs and the notation are clear and succinct.
- The authors extend an earlier work to a much more practical and real world class of NNs by introducing *vector-valued joint group-equivariant* feature maps, which yields universal approximation theorems as corollaries. They also unify the treatment of both shallow and deep networks by leveraging Schur's Lemma.
- They provide explicit examples for depth 2 and depth $n$ fully connected network with an arbitrary activation in Section 4.2 which helps ground their method and significantly helps the reader understand how to leverage the tooling introduced by the authors.
- The paper provides formal support for the popular interpretation for the efficacy of DNNs compared to shallow networks, namely that they construct hierarchical representations which would take an exponential number of neurons to represent using a single layer.
- The limitations section is well written and is explicit about the assumptions made so that the reader is aware of the regime in which the proofs are applicable.

Weaknesses:
**Major**
- The biggest weakness of the work seems to be that it shares a vast amount of technical analysis, machinery and the fundamental proofs are shared with the earlier work by Sonoda et al. While the extension to a larger class of networks and the introduced vector values feature maps is certainly valuable, I am not fully convinced of the differential novelty of the work. Most of the (valuable) effort has been spent in a mostly natural extension of the previous work on the topic.


**Minor**
-  The authors mention that assumption (5) (that the network is given by the integral representation) in limitations is potentially an ""advantage"". If that is so, a discretized version would be the preferred model since it is also closer to real world NNs
- Typo on line 77  - mathmatical -> mathematical
- Typo on line 310 - cc-universaity -> cc-universality
- lines 135 - 137 would be significantly easier to read when broken into multiple lines

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
opdiIAHfBr;"REVIEW 
Summary:
The authors proposes a method to create a universal feature space using brain fMRI response prediction as a training objective. The key idea is that deep networks trained with different objectives share common feature channels that can be clustered into sets corresponding to distinct brain regions, revealing visual concepts. By tracing these clusters onto images, semantically meaningful object segments emerge without a supervised decoder. The paper employs spectral clustering on the universal feature space to produce hierarchical visual concepts, offering insights into how visual information is processed through different network layers. The two main insight being the localization of emerge of foreground/background features, as well as interesting visualization of class-specific concepts using the top spectral-tsne egeinvectors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
First, congratulations to the authors, I liked reading this paper, and I think the experiments and the core is great:

1. Using brain voxel response prediction to find the common space between model is interesting and novel
2. The author propose visualizations of what we could interpret at brain-activations subspace
3. I like the idea for the visualization of how visual concepts emerge and transition through different layers of various models. However, i have concern on it's validity (see weakness)
3. Nystrom-like approximation show the authors thought about scaling their methods

Weaknesses:
Nevertheless, this paper has problems, some more important than others. 

So I will separate them into major problems (**M**) and minor problems (_m_). I want to make it clear that for me, all these problems are solvable and do not detract from the quality of the paper.

Let's start with what I think are the Major problems (**M**):

**M1**. Related Work Quality (Page 9, Section 4):
- The related work section is critically weak, with **only 24 references** and lacks depth in discussing relevant literature. The paper misses an entire set of works on (1) concepts xai, (2) alignment of brain and activations (3) study of representations and (4) attributions methods, which are either crucial or should be mentioned for this study. **A significant rewrite is necessary** to properly position the paper within the existing body of work. 

**M2**. Validity of t-SNE for Distance Measures (Figure 9):
- The paper uses t-SNE for analyzing bifurcation in feature space, but t-SNE is known to distort distances. This raises concerns about the validity of conclusions drawn from t-SNE plots regarding feature bifurcation.


Now for the minors problems:

_m1_. Redundancy of Discovery Claim (Page 2, Line 25):
- The claim that channel feature correspondence exists across networks is not new, as it has been extensively studied in major works (eg using CKA, RSA...), please update and compare your work to this litterature.

_m2_. Reliance on Channels (General):
- Channels are not necessarily the best basis for analysis, as recent researchs suggests there are better ways to represent features, directions (neuron is not a great basis). The paper should address why it continues to rely on channels, considering the limitations.

_m3_. Orthogonality Assumption (General):
- The paper's assumption of orthogonality in feature decomposition does not align with current understanding, especially regarding the neural collapse phenomenon in late layers. Say it otherwise, all point for the class tench are nearly collapse in the latest layer, relaxing othogonality (e.g dict learning) may be a good idea (althought if i am correct, the nystrom approx should not yield perfectly orthogonal vectors). This should be discussed.

_m4_. Parameter Sensitivity (Appendix):
- As always when we have hyperparameter, i expect a small discussion discussing  the effect of changing the parameters (λ eigen, λ zero, and λ cov). This would help understand the robustness of the method to these hyperparameters.

_m5_. Direct t-SNE Application (General):
- The paper uses eigenvectors for t-SNE. It would be more straightforward to apply t-SNE directly to the data, and the paper should justify the chosen approach.

Limitations:
Yes, the limitations identified by the authors are accurate and well-documented. Regarding the weakness I mentioned, I reserve the right to increase the score if the authors adequately address my major concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a method to align different vision models' features to a common space, and to discover interpretable features as clusters in this space. 
The alignment is done by learning linear mappings from features to fMRI activations, the intuition being that the human visual cortex provides a meaningfully structured space, in which locations have known properties (e.g. different regions are known to respond to specific concepts) and are thus readily interpretable. Once the features have been linearly aligned to this common space, they are treated as a weighted, fully connected graph, wherein each image patch is a node, and edge weights (affinities) are computed based on the cosine similarity between the features in each node. A standard spectral clustering method (Normalized Cut) is used to compute a soft partition of this graph into sub-graphs (clusters). As performing this clustering on the full graph would be computationally infeasible, the authors propose to cluster a sub-sample of the graph, and then propagate the resulting clusters to the K nearest neighbors of each subsampled node. Finally, in order to learn linear mappings that preserve the quality of the clusters, a regularization term is added to the reconstruction loss, which ensures that spectral clustering eigenvectors are preserved across the mapping, based on a subsample of nodes.
This method is used to visualize, for each layer of three different models (MAE, DINO and CLIP), the concept that each image patch is assigned to, coded as a color. The 20 top eigenvectors are reduced to 3 dimensions using t-SNE, and these 3D vectors are shown as RGB colors. This visualization reveals that CLIP and DINO produce maps that are close to uniforms in the first 4 layers, suggesting that figure-ground segmentation only emerges in later layers. MAE, on the other hand, shows signs of segmentation from earlier layers. The segmentations extracted from the models in this way are evaluated on the ImageNet-segmentation benchmark, confirming that in CLIP (the model that showed the strongest segmentation) the segmentation emerges at layer 4, and plateaus afterwards. Using the PASCAL VOC benchmark, which also includes category labels, CLIP is also found to encode categorical information, which peaks at layers 9 and 10. In another analysis, a discovered ""figure/ground"" concept is visualized by averaging its activation within the ""figure"" and ""ground"" regions (based on the ImageNet-segmentation ground truth labels) and plotting it on the surface of the brain, showing that areas known to encode objects, faces or bodies tend to respond more to the foreground, while scene-selective areas more to the background. This figure/ground concept is found to be agnostic to object category, and to an extent, consistent across models. In the next section, concepts corresponding to different object categories are visualized on images, and on the surface of the brain. Finally, a 2D t-SNE visualization of the evolution of the features across layers shows a bifurcation between figure and background as the layer depth increases, in both CLIP and DINO.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper proposes to use the human brain as a shared space in which to evaluate different models: this is a clever intuition, which might prove useful for interpreting differences between models.
- It shows that spectral clustering can be a well-suited method for grouping the features of vision models, and in particular ViTs, into meaningful clusters.
- It proposes a clever modification of an existing subsampling-based method for graph clustering, by using K nearest neighbors.

Weaknesses:
- The overall concept is not made clear in the Introduction. The method is relatively simple conceptually, consisting of a step in which multiple models' features are aligned to the brain to provide a common reference frame, followed by clustering of the features within this common space. The Introduction does not make this pipeline clear. Specifically, while the alignment into a common space is clear, the clustering procedure is not explained: first, the authors evoke neuroscientific ideas on lines 39-41, without discussing how these relate to the problem of clustering features, nor even that the goal of the current work is to find clusters of features. Subsequently, on lines 42-47, they discuss the problem in terms of ""graph edges incidents on each pixel"", and ""channel grouping hypothes[e]s"". While this is partly a matter of subjective taste, I believe that discussing the problem at hand in terms of reducing the dimensionality of features at each location (image patch), thus finding a small number of dimensions (combinations of features) which can explain the affinity structure between patches, would be more easily understood by most readers.
- Several key details about the methods are left out: for example, almost no information about training (learning rate, optimizer used, batch size, number of epochs) is provided.
- Related to the previous point, as the Appendix contains several important methodological details (such as the use of additional regularization losses) but is never referred to in the main text, the authors should add references to it where relevant.
- A key component of the proposed method is the learning of a mapping of different models' features into a common space, and as shown in Figure 3, this does indeed result in the cosine similarities of different channels' activations becoming more similar across models. At the same time, the goal of the method is to uncover differences between models. The authors should include an explicit discussion of what kind of model differences are likely to be preserved, and which are likely to be destroyed in the alignment process. As a possible suggestion in this direction, the paper makes several references to the models' features before alignment (for example comparing their segmentations' evaluations with the aligned features in Figure 5), but these features are never visualized. A direct comparison of each model's (clustered) features before and after alignment would be very informative.
- The feature clusters are visualized by reducing their dimensionality to 3D using t-SNE, and visualizing the resulting 3D features as RGB colors. This visualization, however, is not easily interpretable, as different channels are conflated together by the additional dimensionality reduction. Visualizing single channels separately might be more useful to understand the nature of the discovered clusters. Was there a specific reason for choosing the 3D t-SNE visualization rather than showing single channels?
- Overall, it is not clear what the discovered channels can tell us about the models. The single interpretable channel that is discussed in depth in the paper is figure/ground. While this provides a good sanity check on the meaningfulness of some of the discovered features, the ability of vision transformers to segment objects has been observed in several papers (e.g. Melas-Kyriazi et al. 2022, Xu et al. 2023), and the different responsiveness of different regions in the visual cortex to figure and background is well established. Other concepts revealed by the method (the category concepts in Figure 8) are shown on the surface of the brain, but it is hard to interpret what these brain maps mean. The authors should make the questions that can be answered using the proposed method clear and explicit.
- The paper fails to cite closely related work in the Related Works section. Particularly, it cites mechanistic interpretability work in other fields, such as language, but not the recent rich line of work that has specifically looked at vision transformers' ability to perform specific visual tasks. The two papers cited above (Melas-Kyriazi et al. 2022, Xu et al. 2023) are a good example, the former in particular as it proposes a segmentation method based on spectral clustering which is very close to the present one. Another relevant paper is El Banani et al. (2024), which looks at 3D-related tasks. As this is not my field of expertise, I am not aware of papers that look for meaningful directions in the space of vision transformers' channels, but I would be surprised if this didn't exist. I would recommend the authors to do a more exhaustive literature search to find papers that more closely relate to the method proposed here.

- In Figure 7, the figure-ground visual concepts discovered by different models are plotted on brain maps. In the text, the authors write that ""the foreground or background pixels activates similar brain ROIs across the three models"". However, a glance at the brain maps reveals similarities, but also differences. A statistical evaluation of the similarity between different models' brain maps would be recommended.

**References**

El Banani, M., Raj, A., Maninis, K. K., Kar, A., Li, Y., Rubinstein, M., ... & Jampani, V. (2024). Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21795-21806).

Melas-Kyriazi, L., Rupprecht, C., Laina, I., & Vedaldi, A. (2022). Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8364-8375).

Xu, J., Liu, S., Vahdat, A., Byeon, W., Wang, X., & De Mello, S. (2023). Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2955-2966).

Limitations:
As I wrote in the ""weaknesses"" section, I believe the precise scope of the method (what kinds of questions can and cannot be answered with it) has not been properly acknowledged and discussed by the authors.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In the domain of interpretability research, this paper aims to make a mark by proposing AlignedCut, a method to discover shared and expressive visual feature spaces across networks by aligning those spaces with neural responses in human brains. The method is quite interesting - channel-wise responses to images are aggregated and ""feature clusters"" are formed on the basis of the functional connectivity between the pixels and linear combinations of channels. These linear combinations are acquired by predicting neural responses to the same images. The feature space spanned by the neural responses is considered the universal feature space. The eigenvectors corresponding to those feature clusters help us visualize what parts of the image the networks rely on to encode which concept, thus providing an interesting interpretability lens. The most striking example presented is how figure-ground segmentation can be interpreted as a mapping between the input and specific channels in various networks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality
- The AlignedCut method is new to me - and is super interesting - however, I am not an expert in that specific sub-field so I am not sure of its novelty.
- The interpretability lens on figure-ground segmentation is very informative, however, again I cannot judge its novelty.

Quality
- The authors present plenty of analysis to demonstrate the power of their method, which helps in inspiring some confidence in the claims.

Clarity
- The methods and results are relatively clear and the authors provide useful context at the start of each section.

Significance
- Linking pixels to visual features, parameterized through network activations and neural activations opens doors in interpretability research.

Weaknesses:
I see three major weaknesses:

1. The necessity of the brain is unclear to me. Instead of aligning features to the brain, you could've aligned the features of the different networks to each other - creating an ""emergent"" universal feature space. Would your results, e.g. w.r.t. the figure-gound segmentation, change much if you do so? If not, what does bringing the brain into play buy us here in terms of network response interpretability? This is unclear to me.

2. Most of the results need robustness checks. For e.g., in Fig. 6 you show foreground vs background difference in neural response associations. Presumably, that's an average across a lot (all?) of images. Could you indicate some sign of robustness, for e.g, running a permutation test to assess how likely the differences you see would've been expected given the data statistics alone? Same holds for Figs. 7 and 8. We need to know if these differences are flukes or not.

3. Reliance solely on ViTs. To make your point more general, showing that a high-performing CNN shows the same results would be very informative. ViTs have more expressivity in terms of patches interacting with each other - perhaps figure-ground segmentation isn't as strong in CNNs (although if previous research is to be trusted, CNNs should have some notion of figure-ground segmentation; see Hong et al. NatNeuro 2016 and Thorat et al. SVRHM 2021). 

Refs:
- Hong, Ha, et al. ""Explicit information for category-orthogonal object properties increases along the ventral stream."" Nature Neuroscience 19.4 (2016): 613-622.
- Thorat, Sushrut, Giacomo Aldegheri, and Tim C. Kietzmann. ""Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization."" SVRHM 2021 Workshop @ NeurIPS.

Limitations:
The authors mentioned methodological limitations. It is sufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method to interpret deep learning models using brain data. The two apparent contributions are that (1) this new model is able to align channels activations from different layers of different models into a universal feature space, and (2) a Nystrom-like approximation is introduced to speed up spectral clustering analysis.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
I have to be very transparent in this review. Unfortunately, I found extremely difficult to follow this paper, and I was not able to understand its different components; as a consequence, I don't feel capable to evaluate what the possible strengths of this work are.

Weaknesses:
As I said in the previous section, I was not able to understand the methodological details of this paper. A lot of terms and concepts are used throughout the paper without a proper explanation, and as a consequence I cannot honestly understand what is going on with the methods of this paper to properly evaluate it. This paper seems to have a lot of work in it, so I want to believe that what's happening here is that (1) this is one of the first - if not the first - paper from the authors, and thus they lack the experience to explain what they did in a way that their peers can understand, (2) a lot of the concepts used are seen by the authors as very obvious jargon from the subfield, and thus the problem is that I'm not familiar with this subfield, or (3) both. I'm leaving my doubts in the next section, in the hope that they will allow us to understand better whether my understanding difficulties are related to points (1), (2), or (3). As a consequence, I'm rating this paper as a borderline reject, hoping that during the rebuttal period the authors will have time to tackle this readability issues, and as a result I'll be able to properly reassess this work.

Limitations:
Limitations of this work are mentioned in the Conclusion section, but no discussion about the potential negative impact of this work is presented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
ojFEP11Cqj;"REVIEW 
Summary:
The authors propose a energy-based generative boosting method. They try to maximize the log-likelihood functional delta with a second-order expansion. This lead to a boosting algorithm with deltas as steps in the log-likelihood. Instead of scaling the steps with a fixed predefined value (as a anti-overfitting hyperparameter), they rely on linear search to get better performance. They initialize f0 as uniform and use trees as weak learner to learn the deltas. They derive the objective with trees, it is well explained and looks similar to other second-order method objectives like XGBoost. They use MCMC to sample from Q(x) as is typical from energy-based approaches. They use Gibbs so they only need to sample from one dimension while keeping the rest constant. However since there are t trees, this is quadratic in t. They use some form of accept/reject to sometimes accept previous iteration samples in order to not have to re-samples new samples given the quadratic cost. They include a probability of refresh in order to not just always accept old samples. They propose interesting ways of regularizing the approach. They provide good literature review of related methods. It seems like Section 4 could be integrated with the related work section.

Figure may appear unimpressive to a generative expert unfamiliar with trees. But, being able to generate good samples from MNIST using only decision is an impressive feat (even if MNIST is downsampled). This paves the way for trees being used on more complex data.

Table 2 shows nice prediction results, and they do extensive hyperparameter tuning. However, ML Efficiency is not the best metric, it only focus on prediction. A generative model could produce low-diversity fake data that lead to good classifiers. I would recommending adding some distribution metrics such as the Wasserstein distance (which is computable in low-dimension and is used for high-dim data such as images/videos in the form of the widely popular FID/FVD). There are also other useful metrics for quality and diversity, I recommend looking the extensive choice of tabular-data metrics described in https://proceedings.mlr.press/v238/jolicoeur-martineau24a.html. However, the only essential one in my opinion is having a distance in distribution. It can be the Wasserstein distance or something like the MMD distance.

Except for the missing distribution metric, everything else in the results is good and the methodology is sound with good hyperparameter tuning. I would encourage the authors to add a distribution metrics.

The method is sound, novel and quite interesting. The presentation is very well made. Being the first tree method achieving good image data samples is impressive in my opinion. This is a very high quality paper.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The method is sound, novel and quite interesting. 

The presentation is very well made. 

Being the first tree method achieving good image data samples is impressive in my opinion. 

This is a very high quality paper.

Weaknesses:
Missing a distribution metric, the ML efficiency is not a adequate metric on its own.

Limitations:
If the authors had to downsample MNIST from 28x28 to lower, than there are scaling limitations that should be added.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a boosted tree algorithm that performs distribution learning using an energy-based formulation. Inspired by methods like XGBoost, it is claimed to achieve high performance not only in generative ability but also in discriminative performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method incorporates techniques that can be leveraged because they inherit from tree boosting models, such as the approximate sampling algorithm. Moreover, as a model that possesses not only generative quality but also discriminative performance, it has a wide range of applications.

Weaknesses:
This study is not theoretical; therefore, the validity of the proposed method must be confirmed through experiments. However, there are some questions regarding the experimental settings. 

Also, from an algorithmic perspective, since methods unique to tree ensembles are incorporated, they could potentially offer advantages in terms of computational complexity compared to other methods. However, evaluations regarding efficiency have not been conducted. If there are advantages, it seems opportunity loss. 

Please refer to the Questions section.

Minor:
The evaluation of variance is presented separately in Appendix G, but it is difficult to compare. Therefore, I would like it to be summarized in one table.

Limitations:
Computational cost of sampling is larger than existing methods.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed an energy based generative boosting algorithm analogous to XGBoost, which can be used as generative model as well as be applied to discriminative tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The energy-based boosting is novel. The proposed method is capable of both generative sampling and discriminative tasks, enabling broad methodological applications.

Weaknesses:
My concerns regarding the proposed method and the experiments are detailed in the questions section.

Limitations:
I do not identify significant limitations other than the ones discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes to extend the success of tree-based methods in discriminative tasks to generative modelling, which is implemented via an energy-based generative boosting algorithm (NRGBoost). Specifically, NRGBoost directly extends the tree-based tabular models by replacing the discriminative objectives with a generative one, which seems novel to me.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper has well-founded rationales: (1) Tree-based models are performant in discriminative tasks, and thus they are highly likely to also be performant in generative tasks on tabular data, and (2) existing tree-based generative methods do not preserve the tree structures well.
2. The paper is well-written, especially the notations.

Weaknesses:
1. **[Important]** Some highly relevant benchmark methods are missing, including ARF (tree-based) [1], GOGGLE (diffusion) [2] and TabPFGen (energy-based) [3].

2. In Line 325, the authors claim that the proposed method “significantly” outperforms other methods, while the significance test seems missing.

3. I would suggest the authors add comparison results on the computation efficiency. Because NRGBoost basically employs the same architecture as traditional gradient boosting trees, the computation efficiency should be higher than most other network-based generative models.

4. There seem to be some typos throughout the main text: “I.e.” (Line 272)

5. **[Important]** Code is not provided. I remain conservative about the results claimed in the paper.

[1] Watson, David S., et al. ""Adversarial random forests for density estimation and generative modeling."" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.

[2] Liu, Tennison, et al. ""GOGGLE: Generative modelling for tabular data by learning relational structure."" The Eleventh International Conference on Learning Representations. 2023.

[3] Ma, Junwei, et al. ""TabPFGen–Tabular Data Generation with TabPFN."" NeurIPS 2023 Second Table Representation Learning Workshop.

Limitations:
The authors detail the limitations of NRGBoost in Section 7.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oRZN9a53ag;"REVIEW 
Summary:
This paper present novel theoretical results to identify causal effects in restricted ANMs even in case of unobserved confounders.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
**The paper provides novel contributions to the field of score-based causal discovery by extending previous works to confounded restricted ANMs. Based on these contributions, I strongly support this paper's acceptance.**

- the problem definition is very useful to orient readers
- the theoretical results are novel
- the experiments compare against a sufficient number of baselines, and though the proposed method is not SOTA, it compares well and has better theoretical guarantees


Based on the authors' response, I am eager to improve my score.

Weaknesses:
I have a few remarks on improving the flow of the paper; however, even the first four points are not considered major issues.

- Even though condition 1 is a well-known result in the causality literature, I suggest explaining why that admits linear models and including some description of **restricted ANMs** in the main text (at least for me, it is not evident, especially since the condition lacks intuition). To be clear, even this point does not diminish the main contribution, which I see regarding the results for confounders.
- As **inducing paths** are an important concept for the main contributions, please _include it in the main text_ if space permits (suggestion: you can reduce spacing in $\texttt{itemize}$ by setting $\texttt{\\\\begin\\{itemize\\}[nolistsep]}$ )
- I could **not find the definition of an active path** (not even in Def. 5, where it is said to be defined); I presume it is a path that is not blocked, but it would be better to state this explicitly. Maybe it would even be better to use ""a path that is not blocked"" instead of introducing new terminology (this is the first time I encountered ""active paths""; I could be wrong about this)
- The text is sometimes difficult to follow, due to heavy reliance on notation. I'd consider delegating the not crucial part to the appendix (potential candidates in 2.1) and using the remaining space to explain the main quantities better, especially the residuals (e.g., Eq. 12)


## Minor points
- please specify what you are calculating the expectation with respect to (using a bold E is also unconventional, though it's clear from the context it's an expectation)
- as the mathematical objects for d-/m-separation are distinguishable, you might consider dropping the superscript to simplify notation; also, I'd suggest adding whitespace after $\perp^d_\mathcal{G}$ and the like to make it easier for the reader to attribute the indices to $\perp$ and not the not on its right
- I could not find the definition for $\dot{\cup}$ 
- in the explanation of Prop 3., the wording makes it a bit hard to discern that you also provide intuition for the second part; it would help if you refer to _Part (ii)_ explicitly
- _Score matching through the roof_ in the title does not have added value for me, I'd consider rephrasing it to convey the message that ""we propose score-based causal discovery methods for confounded restricted ANMs""

Limitations:
The authors provide an **honest comparison** of their method in the experiments, clearly stating its limitations compared to other methods.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose AdaScore, a method for causal discovery that generalizes previous work based on score matching for SCMs with possibly latent nodes. They combine connections of the score to conditional independence as well as to additive noise SCMs and show that a NoGAM-type procedure works to recover the direction of non-confounded edges of the corresponding partial ancestral graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Adapting NoGAM [1] to the case allowing hidden variables is a practically meaningful contribution.
- Model assumptions are somewhat weakened compared to CAM-UV by allowing for general mechanisms within blocks of observed and latent parents.

Weaknesses:
The novelty of the paper lies primarily in the application of NoGAM to orient very specific edges in a partial ancestral graph (PAG) (the ancestral graph that represents the Markov equivalence class, in analogue to a CPDAG). This falls significantly short of the main contributions as described by the authors on l33--55. Specifically:

- The authors state that they show how constraints on the Jacobian of the score can be used as conditional independence testing. However, the extent to which this is done is only by noticing the equivalence between conditional independence and the corresponding zero in the Jacobian term (previously noted in [1,2]), without any formal analysis of the proposed t-test (Appendix C) as a statistical test of conditional independence (which happens to be a notoriously difficult test).

- The authors state that their identification results for additive noise models generalize the previous results obtained by previous works. In l193, the authors state ""we remove the nonlinearity assumption (of [3]) and make the weaker hypothesis of a restricted additive noise model"", but 1) this is a stronger assumption than additive noise, not a weaker one, and 2) the authors in [3] also consider the same restricted additive noise model. 

- The authors claim that AdaScore is able to handle a broad class of causal models (l54), but three out of four possible situations are direct applications of existing work. 1) Under no structural assumptions with or without latent confounders, AdaScore simply performs constraint-based causal discovery (FCI) using the conditional independence properties of the Jacobian of the score, a straightforward application of [1] also previously noticed in [2]. 2) Under an additive noise assumption, AdaScore is exactly equivalent to NoGAM. 3) Only under an additive noise assumption with hidden confounders, does AdaScore generalize NoGAM to orient unconfounded edges of the PAG returned by FCI, which may be very few of the discovered adjacencies.

### Other comments

- The experiments do not seem to suggest that AdaScore out performs other methods in any meaningful way---in fact, in Figure 1 a) AdaScore is completely equivalent to NoGAM, and is thus redundant. In Figure 1 b), where AdaScore should distinguish itself, it does not appear to be consistently better than CAM-UV. 

- Much of the paper (> 5 pages) is spent on directly describing previous works, NoGAM[3] and/or provide basic background on DAGs and MAGs. 

[1] Spantini et al., ""Inference via low-dimensional couplings."" JMLR 2018.
[2] Montagna et al., ""Scalable causal discovery with score matching."" CLeaR 2023.
[3] Montagna et al., ""Causal discovery with score matching on additive models with arbitrary noise."" CLeaR 2023.

Limitations:
The authors do not adequately discuss the limitations of their method---the limitations section in the appendix focuses purely on the empirical study. The authors claim that AdaScore is adaptive in the sense of being ""less reliant on prior assumptions which are often untestable"", but this is only in the sense that it performs different algorithms depending on user specification, which hardly constitutes one single unifying adaptive algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper extends theoretical results about causal discovery through score matching to encompass both linear and non-linear SCMs and lift the sufficiency assumption. The theoretical results relax the non-linearity assumption of Montagna et al 2023 by swapping it with the less restrictive one of restricted ANM from Peters et al 2009. As for the latent confounder detection a parallel with m-separation is drawn using results from Spantini et al 2018, to establish that the score will be non-zero in the presence of an active path. Following the theoretical results, an algorithm to estimate causal graphs from data is proposed and evaluated, generalizing the NoGAM algorithm of Montagna et al 2023, which only covers the non-linear case.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written and, if it wasn’t for some of the definitions relegated to the appendix, very easy to follow.  

The theoretical results, particularly Propositions 2 and 3 are important extensions of the score-matching methodology for causal discovery, dealing with both the non-linearity and sufficiency assumptions of the method proposed in Montagna et al. 2023.

Weaknesses:
The paper motivation is basically the weakening of current assumptions for causal discovery methods. However, assumptions and benefits of the proposed methodology are not clearly specified. In line 74, the authors state that faithfulness is assumed (I believe, it is kind of hidden in the background notions). If that is the case, the method adopts the same assumption as FCI, plus ANM. So the proposed method is relaxing assumptions compared to CAM-UV, RCD and NoGAM, but adding onto FCI. Regarding the benefits, the alleged flexibility of the method to output DAGs, MECs, MAGs, PAGs, which should make it preferable to FCI, is merely touched upon in the contributions and the experiment section.  

Proposition 1 is a rather trivial application of the more general lemma in Spantini et al. and it does not specify the required faithfulness assumption to obtain the result from Eq. 6 in the paper. 

The experimental results show limited added value according to the one metric chosen (SHD) in a synthetic setting. They are not comprehensive enough, with no application to common (pseudo-)real benchmarks (e.g. from bnlearn). More experiments and more metrics are needed as, as it stands, the proposed method seems to add no real value compared to the baselines. Additionally, it is not clear from the experiments if it is really able to identify confounders. Breaking down precision and recall by mark would show this. FCI and a random baseline should be also added for reference.  

Experiments are conducted on data with at most 9 variables, and the scalability of the method is not shown nor discussed. 

The model used to estimate residuals is not discussed, nor the assumption that the chosen model fits the data adequately to correctly estimate residuals, and what is needed to assess this.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oQc7TOCk5G;"REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oEmyoy5H5P;"REVIEW 
Summary:
The paper is a review of algorithmic recourse (AR) literature. The authors deploy a systematic framework to investigate research trends in algorithmic recourse and evaluate their incorporation of practical concerns like societal and institutional considerations of AR, or lack thereof. The review finds that current research is  focused on methods and technical considerations. The authors encourage researchers in AR to consider real-world implications of their work and conduct user studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Paper is well-organized and easy to follow
- Section 2 provides solid background information on algorithmic recourse
- The questions in Section 4 are pertinent

Weaknesses:
While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.

Spending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.

A more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.

The paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides a review of previous works that study ""algorithmic recourse"", i.e. conceptual and practical approaches for giving people actionable recommendations to change how they are impacted by algorithmic systems. This literature is deeply connected with counterfactual explanations and understanding models through small changes to test data, answering questions such as ""how would the model M produce a different output if changed attribute x about myself"". The authors review 127 archival publications and answer 9 questions about how these works frame and study algorithmic recourse.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
In terms of originality, quality, and clarity:
- While the primary novel contributions of this draft are to highlight themes in previous work, the overall level of novelty is reasonable. Some concerns here, see below.
- Quality: the ""Systematized review"" methods are described such that they are replicable and seem justified. I don't expect readers to have major issues with inclusion criteria of papers, or any of the analyses presented.
- Clarity: Writing is clear throughout.

In terms of significance, the paper could have impact on future work studying algorithmic recourse, and might motivate NeurIPS community members (including those in companies or working with governments) to support recourse methods. This would be a large positive impact.  

 This kind of review can certainly be useful to researchers trying to incorporate ideas or findings from recourse-related research. The calls to engage with HCI and systems-level thinking are reasonable (though, some of the broader discussion/motivation in the paper is more convincing on this front than any of the empirical results from the 127 recourse-related papers). If a version of this paper were able to unify definitions in the recourse space, this could be powerful (though further expansion of Section 2.2 might be necessary: the paper does note that reference [70] is highly similar -- the current draft was a bit vague in comparing these and clarifying the added contribution here.).

A few other notes: There are 9 overall sub-research questions answered. Overall, these results seem likely to be useful to researchers entering the algorithmic recourse field (though, see below, some of these felt very general and not domain-specific in the current draft). The paper does fit into the ""Social and economic aspects of machine learning"" category listed in the CFP this year.

Weaknesses:
Overall, I do think the current draft may not achieve the full impact that a future revision could provide.

The current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. ""Why hasn't this field engagement with any real world deployments"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.

One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 ""endorsed"" by the paper? Is the ""imagine a counterfactual input x*"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.

Ultimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse ""tasks"" could have an even larger impact.

Limitations:
- No major concerns regarding unmentioned social impacts.
- Regarding the limitations of systematized literature review, the current draft discusses these reasonably.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper provides a comprehensive review of the algorithmic recourse research literature, concentrating on understanding the recourse research ""in the wild"", by focusing on the practical application of these techniques in real-world scenarios. The authors then provide some suggestions to practitioners to push future research to better practical applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and well-structured. Considerable effort has been put into this work to provide a comprehensive review of the area, highlighting the need for a more down-to-earth approach when considering recourse. The data collection and analysis are well-motivated and described sufficiently (Section 3 and Section 4). The recommendations in Section 5.1 are on point and all true, and they highlight issues that everyone in the community is aware of but that are largely ignored.

Weaknesses:
I feel NeurIPS is not the right venue for this kind of contribution, since this paper does not provide the level of technical novelty required by the conference. Being a review, I think it does not fit the requirement of ""new and original research"" given by the Call of Papers. I suggest the authors not be discouraged, since I think the contribution is still valuable for the community. Potential other venues I believe are more in line with the scope of this work could be the following (the order is random):
- IJCAI Survey Track (https://ijcai24.org/call-for-papers-survey-track/)
- ACM FAccT (https://facctconference.org/)
- AAAI/ACM AIES (https://www.aies-conference.com/2024/)
- ICML Position Papers Track (https://icml.cc/Conferences/2024/CallForPositionPapers)
- ACM Computing Surveys (https://dl.acm.org/journal/csur) 
- TMLR (https://jmlr.org/tmlr/)

Lastly, I would like to point out some potential additional papers on algorithmic recourse which could complement some remarks made by the authors:
- Line 182 ""We did not identify any applications evaluated with humans in the loop"": there has been some development in providing human-in-the-loop algorithms to identify better recourse options:
  - [1] De Toni, Giovanni, et al. ""Personalized Algorithmic Recourse with Preference Elicitation."" Transactions on Machine Learning Research, https://openreview.net/forum?id=8sg2I9zXgO
- Recommendation 4, ""Accounting for emergent effects"": there has been some research regarding providing recourse to multiple individuals, where they are competing for a limited pool of resources, looking also at the fairness of these systems:
  - [2] Fonseca, João, et al. ""Setting the right expectations: Algorithmic recourse over time."" Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. https://dl.acm.org/doi/pdf/10.1145/3617694.3623251
  - [3] Bell, Andrew, et al. ""Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity."" arXiv preprint arXiv:2401.16088, https://arxiv.org/pdf/2401.16088

I also point the authors to some new papers considering human-in-the-loop interfaces for recourse (Recommendation 1, Section 5.1):
- [4] Esfahani, Seyedehdelaram, et al. ""Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration."" Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. https://dl.acm.org/doi/pdf/10.1145/3627043.3659556
- [5] Koh, Seunghun, Byung Hyung Kim, and Sungho Jo. ""Understanding the User Perception and Experience of Interactive Algorithmic Recourse Customization."" ACM Transactions on Computer-Human Interaction. https://dl.acm.org/doi/pdf/10.1145/3674503

Limitations:
The authors have highlighted the limitations of their work in Section 5.2.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a survey regarding algorithmic recourse scientific literature. In their work, the authors analyze what types of contributions do the authors choose to make to the AR research, what are the criteria covered in the authors’ definitions of AR, what are the criteria covered in the authors’ definitions of actionability, the roles of end users, what types of real-world considerations motivate existing research, what types of real-world considerations are seen as challenges for future work, what types of group-level dynamics are addressed in the existing research, what are the approaches to the realistic evaluation of proposed methods, and what are the open source and documentation practices in AR research. They conclude their paper by providing recommendations on how to make future algorithmic recourse solutions better suited for real-world needs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- the authors invested much effort into explaining the procedure followed to ensure a high-quality survey
- the authors very synthetically review scientific literature related to algorithmic recourse and provide a great insight into the field within a few pages
- the authors reviewed a vast amount of literature (165 references!)

Weaknesses:
We did not identify important weaknesses. While an extensive survey could be created following this one, providing in-depth details for each of the sections, we understand this cannot be done within the constraints established for this venue.

Limitations:
The authors have adequately acknowledged the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oAmHzy8btj;"REVIEW 
Summary:
This paper considers the graph matching problem, where the goal is to produce a mapping between vertices of multiple graphs which maximizes similarities among them. The authors study graph matching from a theoretical perspective, in which one observes multiple (appropriately correlated) Erdös-Rényi (ER) graphs that have ground-truth latent mappings between them. The authors' goal is to characterize the information-theoretic threshold for exactly recovering the latent mappings between all of the observed ER graphs. Prior work has settled the information-theoretic thresholds for 2 correlated ER graphs, and this paper settles it for more than 2 ER graphs. 

To determine the information-theoretic threshold for exact graph matching, the authors establish matching achievability and converse results. The converse is based on a simple reduction to a graph matching problem with two ER graphs, combined with known results on impossibility results for exactly matching two ER graphs. For the achievability results, two algorithms are discussed. The first is the MLE, which is optimal for exact graph matching. The authors show that it has a clean, easy-to-understand form: the MLE outputs vertex mappings which maximize the number of edges in the corresponding union graph. However, the authors do not directly analyze this algorithm due to technical complexities. Instead, they propose an algorithm which involves two phases. (1) For each pair of graphs, a partial, fully-correct mapping is computed via the $k$-core estimator, and (2) unmatched vertices are matched through a ""transitive closure"" procedure. This algorithm provably outputs the full, correct set of vertex mappings in the parameter region that complements the converse. 

Finally, a few numerical experiments are presented, showing that the transitive closure procedure can be combined with known computationally efficient algorithms for pairwise graph matching to derive algorithms for matching multiple graphs in a principled manner.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Almost all the existing theoretical work on graph matching concerns two graphs, except for some trivial results (to the best of my knowledge). The extension of the theoretical framework to multiple graphs is a natural and important follow-up, and may inspire several future works as well. 

While the algorithms and analysis are largely adapted from prior work (e.g., the $k$-core estimator), a key novelty is the transitive closure step, which provides a principled (and optimal!) bridge between pairwise graph matching and $m$-ary graph matching. As the authors highlight, this step can be used to extend practical algorithms for pairwise matching to the $m$-ary case in a black-box manner. I imagine that this technique could be useful in practice. 

Additionally, the paper is well-written.

Weaknesses:
To me, the main weakness of the paper is in the discussion of transitive closure's implications. The authors make a striking observation that one can use their transitive closure technique (at least heuristically) to generalize pairwise graph matching to $m$-ary graph matching. However, several details are lacking in the simulations section. For instance, what are the graph parameters ($n$ and $p$)? What is the error rate before and after the transitive closure boosting? How do the results shown compare to the accuracy of Algorithm 2? (Even though the $k$-core matching is not efficient to compute, the result of the matching procedure is a function of the ground-truth permutations, so I believe the algorithm's accuracy can be simulated efficiently). 

There are a couple other minor weaknesses. One is that Algorithm 2 is computationally inefficient. However, making such an algorithm efficient is likely a challenging research question itself, and is appropriate for future work. Another weakness is that there is no nice figure to visualize Algorithm 2. I feel that the reader's understanding could be greatly improved if one could create a representative figure for the transitive closure boosting.

Limitations:
Limitations have been largely discussed. The authors could expand upon implications of graph matching to protecting / breaking privacy in anonymized social networks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the information theoretic limits for matching
multiple correlated random graphs. Based on a correlated Erdos-Renyi
random graph model, the authors provide both lower bound and achievable
bound for the condition to correctly match all nodes with high
probability. These bounds match each other. A highly interesting insight
is that, even when exactly matching two graphs is not possible, the
proposed algorithm can leverage more than two graphs to produce exact
matching among all the graphs. The achievable algorithm exploits the
transitivity among partial matchings through $k$-cores, which is also
quite interesting.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The novelty of the paper is high in dealing with graph matching among
multiple correlated graphs. 

2. The necessary and sufficient conditions for exact matching meet each
other. 

3. The proposed algorithm can exploit transitivity to match all graphs,
even when any two graphs alone cannot be exactly matched. This is a very
insightful result.

Weaknesses:
1. The proposed algorithms do incur high complexity.

Limitations:
Limitations are discussed in Section 5.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This theoretical paper gives tight conditions for exact graph matching with multiple correlated random graphs. This problem has been extensively studied recently for the case of 2 graphs, and it is shown here that with more than 2 graphs, there is a regime where pairwise alignment is not possible, but with the information provided by all graphs, it is possible to align all of them. This is a nice theoretical result.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper studies a natural extension of a well-studied problem from 2 graphs to more graphs and shows a surprising effect: making partial pairwise matching is sufficient to get the exact recovery. The proof outlines give the main insights into the technical proof.

Weaknesses:
The resulting algorithm is not practical as it does not run in polynomial time (as it is mentioned by the authors).

Limitations:
The authors are very clear with the limitations of their work in section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to find out alignments between G_1 and G_2,....G_m, under the assumptions that they all are essentially sampled from ER graph distribution. The paper presents one impossibility result (or necessary condition to estimate such alignment)  and two sufficiency results to solve the underlying problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tackles an interesting problem and it is written clearly.

Weaknesses:
(1) I am not too confident that the paper is appropriate for neurips audience. I think the paper suits better to a conference like ISIT or such.  The paper has barely any learning component and the practical utility is not very clear.
Also, the primary area assigned by the authors ""Probabilistic methods (for example: variational inference, Gaussian processes)"" is probably not correct.

(2) The paper only tackles a very simple graph model (ER graph model). While I understand that theoretical analysis for complex graph model is difficult, I would recommend the authors should discuss that in comprehensive manner. To elaborate concretely,
suppose,  G_1, G' _2...,G' _m are *not* generated from an ER model. But G_2,...G_m are generated using an ER like model with constant edge deletion probability $s$. In such case, can one characterize the necessary and sufficient condition.

Note that, the area is not too new in the literature. There has been work already in this line of research [CK17,WXY22 in the paper]. Although I will not say this work is an extension but the theoretical contribution given the existing works is not very interesting (m=2 to an arbitrary m for example). 

(3) There is no experimental analysis. I would have increased my rating if the authors have done a thorough study on implications (including limitations) of their work on graphs from other models. For example, if we apply the same algorithm in other graph models, how would it perform. Since the line of work is not new, I would not say the theoretical results have strong enough impact to ignore the poor experiments.

Limitations:
Restrictive graph model; poor experiments and incremental contribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nzzxLPJENZ;"REVIEW 
Summary:
In this work, the authors conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The authors demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of branching LLM evaluation is interesting and novel.

Weaknesses:
1. The authors missed a lot of key related works, including close-ended benchmarks such as MMLU, MMLU-pro, MixEval, GSM8k, GSM1k, etc; open-ended benchmarks such as Arena-Hard, AlpacaEval, WildBench, Chatbot Arena, etc.
2. I think the writing needs improvement. Now it's not easy for a reader to get what you are focusing on. If you are doing evaluation, then try to use some pipeline figures and comprehensive captions to describe the core idea. Besides, all captions in this paper is misleading, not telling the reader about what is happening in the table or figure; also, there lacks some key sections such as conclusion.
3. How to measure the quality of the proposed evaluation? I think just evaluating 5-6 models is far from enough. Beyond that, how is the model rankings related with Chatbot Arena or some other popular benchmarks such as MMLU?

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel approach to efficiently evaluate LLMs using branching preference learning. The authors conceptualize the evaluation process as a decision tree, where each path represents an evaluation reasoning trajectory. They introduce a tree-based data sampling method and preference learning based on the DPO algorithm to improve evaluation capabilities. The method is tested in three settings: in-distribution, out-of-distribution, and transfer evaluation. The authors claim their model significantly reduces dependency on labeled data and demonstrates strong performance across different evaluation settings while reducing inference costs by 90% compared to searching the entire evaluation tree.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper's novel approach of framing LLM evaluation as a decision tree problem is a significant strength. This allows for a more nuanced and flexible evaluation process that can adapt to different scenarios and criteria. The use of branching preference learning enables the model to prioritize critical evaluation criteria.
- The authors test their model in multiple settings (in-distribution, out-of-distribution, and transfer evaluation), providing a thorough assessment of its performance. Applaud to that.

Weaknesses:
- The biggest concern I have is that in-distribution performance is not better than other baselines it compares to. This begs the question of where the improvement gain is from. If in-distribution evaluation performance is mediocre but out-of-distribution does better, then doesn't the most gain come from a better dataset? 
- Another concern is the unnaturalness of using evaluation criteria as individual nodes. How to ensure the coverage of those criteria across different nodes. Are they overlapping each other or completely different? The paper is quite vague on this.
- Why is each criteria subtree only a binary tree? If using a tree structure, it seem like it can be easily extend to multiple nodes rather than just 2 at each layer.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They present an approach to improving LM evaluation by having models first generate an evaluation criteria, then a scoring guideline, and then finally a final judgement. They then develop a procedure for collecting training data corresponding to these three steps by applying branching/pruning approach (sample multiple criteria, from each sample multiple guidelines, etc...). They then use the generated data to train a DPO and SFT model. They find that their method outperforms baseline evaluation approaches according to correlation with human judgement on dialogue evaluation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem of improving LM evaluation is important
* The idea of enabling language models to hierarchically sample evaluations (e.g. fist criteria, then guideline, then judgement) is very neat, and similarly the idea of applying a tree-based sampling procedure to automatically generate data is quite cool.
* I think they do fairly thorough experiments and compare to quite a few baselines.

Weaknesses:
* The paper is honestly pretty hard to follow. There's a lot of moving parts and it's not explained in an easy to digest way.
* The specific method presented seems a little bit ad-hoc, and could be justified better in the paper (e.g. why use criteria, then guideline, then judgement, why not some other sequence of steps?).
* Looking at Figure 1, it doesn't seem that their method improves all that much over the baseline

Nits:
* The related work seems pretty sparse. There's lots of work on improving LM evaluation in math reasoning settings that isn't discussed.
* Figure 4, the text is really small and hard to read.

Limitations:
They do a good job of discussing the limitations. I would also note that it is unclear how effective this is when applied to more challenging tasks like mathematical reasoning (e.g. MATH benchmark) as a limitation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how to improve the quality of automated evaluation through fine-tuning (SFT and DPO). The main algorithm proposed by the paper is to construct an search tree which consists of node of (criterion, scoring guide, and judgment). This tree is later pruned and modified and the different paths serve as fine-tuning data for SFT and DPO.

My current rating is tentative. If the authors can kindly clarify the details of the paper, I'm happy to raise the score.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is very clear and easy to read.
2. The investigation is very thorough. Experiment is comprehensive (the in-distribution, out-of-distribution evaluation setup is great).
3. The main claim of the paper is substantiated (I.e., improving efficiency through fine-tuning).

Weaknesses:
I don't think this paper has substantial weaknesses.

1. There are some imperfections of text -- mostly just need to be clarified. Missing notation definitions, etc. 
2. The performance improve over Auto-J on AGR is minor (55.13 -> 57.18). OOD evaluation, Zephyr-7B AGR is 56.75 and GPT-4 is 62.28 (which is close, but not quite close). CNS however is beating GPT-4. This would be very helpful for me to understand a bit more about what CNS is, and whether beating GPT-4 on this metric is meaningful or not (see Q6).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors propose a tree-based data sampling method to conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The proposed method involves generating supervised data and preference pairs derived from the evaluation tree for SFT and DPO training. This approach aims to reduce the dependency on labeled data and improve the performance of the evaluation model across in-distribution, out-of-distribution, and transfer evaluation settings. Experimental results demonstrate that the proposed model can enhance evaluation efficiency and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method reduces the dependency on human labeled data by generating supervised data and preference pairs from the evaluation tree.
2. The paper is well-written.

Weaknesses:
1. Potential Biases --- The initial multi-branch training data is generated using only GPT4, which could introduce bias to the training data. Moreover, the branch ensemble method could also introduce bias to the training data. If the training data is biased or unrepresentative, the model's evaluations may also be biased. The authors should consider labeling a small annotation set to validate the branch ensemble approach.

Limitations:
Yes, limitation discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nu2Sqrsnr7;"REVIEW 
Summary:
The paper attempts to train PINNs which solves acoustic wave equations. They do so by using hard-constrained PINNs which can enforce IC and BCs, and propose a collocation point sampling method (DAFS) based on the amplitude of the solution at different regions.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper considers an interesting problem in acoustics and attempt to apply the techniques from PINNs to solve them.

Weaknesses:
The paper itself feels less coherent, and seems like just an application of many existing PINN training techniques (e.g., hard constraint PINNs, collocation point sampling) into solving a certain problem, rather than providing a novel method or a coherent framework into solving a domain-specific problem.

The experimental section feels incomplete. Different point selection algorithms have not been extensively compared with, e.g., from that in Wu et. al. (2023). Furthermore, it would be interesting to see how the method can scale to more realistic acoustic problems (i.e., outside of 1D settings).

The paper itself also seems incomplete. The Appendix and the NeurIPS checklist are partially filled and have half-finished sentences.

The labels within the graphs can also be enlarged slightly to make them more readable.

Limitations:
The authors have provided limitations with selection of \tau.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript treats the one dimensional wave equation with a PINN approach and discusses the imposition of boundary and initial conditions directly into the network, as common practice in PINNs. The authors then propose a quadrature scheme based on a coarse finite difference discretization of the wave equation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The imposition of the time derivative seems to be a novel construction. Furthermore, the construction seems not to be limited to the wave equation.

Weaknesses:
The main weakness of the manuscript is the focus on the very special and simple toy problem of the one dimensional wave equation. Solving the one-dimensional wave equation with PINNs is only of academic interest and insights obtained from it for the training of PINNs might not generalize. More specifically:
- The exact imposition of the time derivative should also work for general time dependent equations. The authors should comment on this.
- The sampling strategy employing a finite difference simulation to determine regions of high sampling density is not a generalizable approach. If a finite difference solver for the equation at hand is available, a PINN solver is typically not required.
- The authors determine an optimal function $\tau$ via considering six concrete examples. There is no guarantee that this approach will generalize to different equation types and is therefore of limited practical use.
- The authors might want to discuss the theoretical literature that proves the theoretical advantage of exactly imposed boundary conditions [1, 2, 3] and more elaborate constructions of distance functions.

[1] https://proceedings.mlr.press/v190/muller22b/muller22b.pdf

[2] https://arxiv.org/abs/2311.00529

[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521006186

Limitations:
The scope of the paper is too narrow.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores to solve the acoustic wave equation in the context of PINNs. Hard boundary and initial conditions are enforced by employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. A Dynamic Amplitude-Focused Sampling (DAFS) method is introduced to improve the efficiency of hard-constraint PINNs under a fixed number of sampling points.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Propose a general hard constraint imposition formula which correctly imposes all boundary conditions and initial conditions as required.

Weaknesses:
1. Only the wave equation is discussed.
2. The proposed Dynamic Amplitude-Focused Sampling (DAFS) method is trivial.
3. There are no comparisons with other methods in the experiments.
4. In the experiments, the relative errors between exact solutions and predictions are not given.
5. In the context of PINNs, it is better to give explicitly the formulation of training loss. Training details are also lacking. 
6. Instead of tuning \tau (t) manually, it is better to train \tilde{u}(x,t) and \tau (t) simutanuously.
7. Many typos and grammar errors, such as ""both and \alpha"" in line 149, ""x \in {\partial \Omega}_i"" in line 125, ""computational"" in line 46.
8. The quality of Fig.7 should be improved.

Limitations:
Only the wave equation is discussed. There are no comparisons with other methods in the experiments.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper improves the training efficiency of original physics-informed neural networks to solve the 1D wave equation threefold: first by extending ansatz to also take the first derivative into account, second by a sampling method that focuses on high-amplitude regions, and third by a framework for domain decomposition.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The related work is well presented.
+ The evaluation of the six candidate functions for \tau in section 4.2 provides interesting insights. The authors explore an advanced selection method for \tau based on the task at hand which might be an interesting research direction.

Weaknesses:
[Originality] While considering the first derivative for the ansatz is a good addition, the contribution is only minor. 
Sampling more collocation points in the regions that might be more difficult to solve is a practical approach however the comparison and distinction to other sampling methods is missing.
Lastly if I understand the domain decomposition framework correctly, the contribution is to wrap the entire training into a loop and, based on the training process's results, increase or decrease the subdomain size. 

Evaluation results are only provided for the 1D wave equation. Further results for other differential equations are necessary to demonstrate the benefits of the proposed method.

[Clarity] 
The framework for domain decomposition is not presented clearly. While the flow chart in Figure 7 provides an overview of the method additional textual explanations in Section 4.4 are needed.
There were few to no remarks about the training regime (#training points, optimizer, learning rate…, etc.), making it more difficult to reproduce results.
Minor remarks:
-            N_pde is not introduced. It is probably the number of collocation points?
-            Most of the Figures (e.g. Fig. 1, Fig 6.) are hard to read.
-            Line 46: (…) optimal size of the computational [domain?] given (…)
-            Line 149: Both [N_pde?] and alpha (…) 
-            Line 178: (…) In general, (...) performs better in general

Limitations:
While the authors clearly state that they are interested in the 1D wave equation it would have been interesting to see their proposed methods applied to the 2D wave equation of any other differential equations what are typically used in PINN benchmarks.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nrcFNxF57E;"REVIEW 
Summary:
This paper introduces the an unbalance Gromov-Wasserstein distance, which adopted the formulation of unbalance optimal transport into the Gromov-Wasserstein with total variance (TV) penalty instead of KL. This new distance allows comparison of probability measures from different space with partial amount of mass, and so they name it Partial Gromov-Wasserstein (PGW). They further prove the metric properties of this distance and proposed two algorithms to solve PGW: Frank-Wolfe algorithms and Line search method. In experiment section, they test PGW with different tasks including: shape-matching, shape retrieval and barycenter problem.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper fills a gap in the topic of unbalance (GW) problems by introducing a TV-relaxed unbalanced GW, they call it Partial Gromov-Wasserstein (PGW). This work shows the theoretic metric properties of this distance with a solid proof, making a contribution within this topic. They proposed two solvers to compute this distance and show diverse experimental applications. The experiment results show that this new variance of unbalance GW performs effectively with data containing outliers, aligning with the anticipated performance of unbalanced Wasserstein or unbalanced Gromov-Wasserstein methods on noisy data.

Weaknesses:
- It's worth to note in literature review the similar works formulating partial Waserstein as a metric with TV constraints [1] [2].
- The proof of the metric properties is not well displayed in the main text, as this is the main highlight of this work.
- Further comparison with KL version was lack as regards to robustness against outliers. And also, further discussion on the scalability (very large dataset) will be appreciated.
- In experiment section, the selection of hyperparameter is not clear. The justification of choosing $\lambda$ value for both PGW and MPGW method was not presented. 

[1] Raghvendra, Sharath, Pouyan Shirzadian, and Kaiyi Zhang. ""A New Robust Partial $ p $-Wasserstein-Based Metric for Comparing Distributions."" arXiv preprint arXiv:2405.03664 (2024).

[2] Nietert, Sloan, Rachel Cummings, and Ziv Goldfeld. ""Robust estimation under the Wasserstein distance."" arXiv preprint arXiv:2302.01237 (2023).

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers an unbalanced version of the Gromov-Wasserstein distance, where the discrepancy terms correspond to the total variation between certain product measures for the given marginal and the marginal of the solution, respectively. Different (re)formulations of this distance is considered in both the discrete case and for general measures, existence of optimal solution and metric properties are shown for certain cost functions, as well as numerical methods for computing (local) optimal solutions. Finally a number of numerical experiments are considered.

The paper is well written and extensive. In the main paper the results are stated and with proofs in the  appendix.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is strong in terms of both content and the presentation. In particular the results about the metric properties of the PGW problem. It is also a quite complete paper in terms that is contains substantial results on theory, computational methods, and numerical simulations.

Weaknesses:
One weakness with the methods in the paper is that it only provides local optimal solutions. This is a problem with many non-convex problems, but in some cases global solutions can be guaranteed. In the abstract it is stated the methods solve the PGW problem. This is probably to strong statement since they are not guaranteed to converge to the global solution.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Partial Gromov-Wasserstein (PGW) metric as a means to handle unbalanced Gromov-Wasserstein problems between non-probability mm-spaces. The authors develop and demonstrate two computationally efficient variants of the Frank-Wolfe algorithm for solving the PGW problem. They establish that PGW is a well-defined metric, providing theoretical proofs and applications in shape matching, shape retrieval, and interpolation. The metric and algorithms are validated through numerical experiments against established baselines, showcasing improved results in handling outlier data with a robust performance.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper presents a novel approach to defining a metric in the context of unbalanced Gromov-Wasserstein, which has been a challenging issue in the field.

2. The quality of the research is high, evident from rigorous theoretical developments, proofs, and comprehensive experiments that validate the effectiveness of the PGW metric in practical applications.

3. The paper is well-organized, with clear explanations of complex concepts. The use of examples and experimental results helps in understanding the practical implications and advantages of the PGW metric.

Weaknesses:
1. While the paper provides a comparison with existing methods, it could benefit from a broader range of comparative baselines, especially newer techniques that might provide a stiffer benchmark.

2. The paper does not extensively discuss the sensitivity of the PGW algorithm to the choice of its parameter (e.g., the regularization coefficient - lambda), which is crucial for understanding its robustness and adaptability in diverse real-world scenarios.

3. There is limited discussion on the scalability (or the time complexity) of the proposed methods, particularly in high-dimensional or large-scale settings, which is vital for their applicability in big data applications.

Limitations:
Including limitations on the scalability and time complexity, such as the maximum solvable problem size within one hour, would be beneficial for its applications.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a partial Gromov -Wasserstein (PGW) formulation, which relaxes the original constraints present in Gromov-Wasserstein (GW) formulation. In PGW, the marginal equality constraints of GW are replaced by marginal inequality constraints. Following existing works in partial GW setting, the paper additional imposes TV-based marginal regularization in the objective. The paper showed that the proposed PGW approach defines a metric between metric measure spaces. The PGW problem is solved via Frank-Wolfe (FW) and empirical results are shown on shape interpolation  in the main paper.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper explores partial mass transportation setting in the GW problem. As  discussed around lines 149, an existing work [45] has also explored similar concepts in GW setting. [45], in particular, involves an additional hyperparameter (\rho) for overall mass of the learned transport plan. The proposed problem (16) as well as [45] employs FW algorithm. 
    - Proposition L.1 in the appendix shows that if \gamma is an optimal solution of proposed PGW problem, then \gamma is also an optimal solution of the mass constrained PGW problem (MPGW) of [45] with \rho=|\gamma|. 
    - It is unclear what the paper implies by ""mathematically this equivalence relation is not verified."" in line 955? How is the paper defining the term ""equivalence"" which is used multiple times in Appendix L. 
    - For a given \lambda=\lambda_0 in PGW, does there exist a \rho=\rho_0 for MPGW such that the set of first order critical points for PGW and MPGW are same?
    - The steps of the FW algorithm for proposed PGW and MPGW [45] seem quite similar.

Weaknesses:
- The paper is poorly written due to the following reasons:
    - The abstract and introduction states that the paper propose two variants of FW algorithm for solving the proposed PGW. However, the main paper does not describe two variants of FW algorithm. Only one variant is discussed in Sections 4-5. The other variant is described only in Appendix G. If the algorithm Typically, the main paper should be self contained, having the necessary details of the contributions claimed in the abstract and introduction. It should be noted that going through the supplementary material is a reviewer's discretion. 
    - The paper provides very less discussion on how its differences with MPGW [45] in the main paper (lines 148-149). While Appendix L contains this discussion, important parts of it should be discussed in the main paper. As mentioned above, going through the supplementary material is a reviewer's discretion. 
    - There are multiple grammatical and spelling errors throughout the paper. Eg. he -> the, con -> cost, etc. 

- The main paper contained experiments only on synthetic dataset. Tables 2 and 4 in appendix discuss experiments on real-world datasets. MPGW obtains same generalization performance as PGW in both the tables. This should be discussed in the main paper as well.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
nblJSwtdrJ;"REVIEW 
Summary:
The paper introduces Tina, a text-conditioned neural network diffusion model designed for train-once-for-all personalization. Tina utilizes a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. This innovative approach aims to generate personalized models for various end-users and tasks based on text prompts, demonstrating significant generalization capabilities even when trained on relatively small datasets (~1000 samples). The model is evaluated under zero-shot/few-shot image prompts, varying numbers of personalized classes, natural language descriptions, and predicting unseen entities to assess its understanding of world knowledge.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper provides a comprehensive explanation of the design and framework of Tina.
- It conducts a detailed ablation study and experiments across different datasets.
- The topic is interesting, and the presentation is clear and easy to understand.
- very detailed and robust comparison with previous works.

Weaknesses:
- The model parameter size in the experiments is too small; larger models are needed to evaluate effectiveness.
- In Table 1, the results of direct fine-tuning should be included.
- We might need an ablation study on the impact of text prompts.
- We might need an ablation study to determine if the model merely memorizes and reproduces parameters.
- Figure 2 requires polishing for better clarity.

Limitations:
The model size is too small in exp.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To generate personalized models for a variety of end-users and tasks via text prompts, this paper introduces Tina, a text-conditioned neural network diffusion model. Tina employs a diffusion transformer model, complemented by a CLIP model to embed task descriptions. Remarkably, Tina demonstrates superior generalization capabilities even on small-scale datasets, performing well both within and outside the distribution of the training data. Furthermore, Tina exhibits robust performance under zero-shot/few-shot image prompts, natural language instructions, and unseen categories.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method demonstrates excellent generalization, showcasing significant in-distribution and out-of-distribution performance even when trained on small datasets. It also exhibits robust behavior in predicting entities that have not been seen before.

2.Compared to existing text-to-image models (such as stable diffusion), text-to-video models (like Sora), and large language models (such as GPT-4), the concept of Tina, which generates personalized models suitable for specific tasks directly from text descriptions, is quite novel.

3.The experimental process is comprehensive and reliable. The paper conducts comparisons against baselines across multiple datasets, and it also undertakes experiments to validate generalization performance as well as performs ablation studies.

4.The experiments involves multiple datasets to verify the effectiveness of the proposed methods.

Weaknesses:
1.It is better to includes more comprehensive and competitive baselines to show the model’s effectiveness and advance. The two baselines come from one paper published in 2023. As for the experimental setting involves three widely-used datasets, I am wondering whether the experimental results excels or perform similarly to the SOTA performance on some of the three datasets. In other word, is it possible to apply the proposed strategy to some more advanced framework to make the performance similar to the SOTA, which ensure the proposed method have real applications in the real use.

2.The base model is CNN or ResNet in the experiments. Is the proposed method generalized to more advanced framework? Applying the proposed method on more advanced framework and obtain more advance performance indicates that the method has potential to be used in the real life.

3.We suggest providing necessary explanations in the captions of the model framework overview.

Limitations:
The limitations are fine with me.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces Tina, a text-conditioned neural network diffusion model designed for generating personalized models from text prompts. Tina aims to enable efficient personalization by training a generic model once and then customizing it for various end-user tasks using task descriptions. Leveraging a diffusion transformer model and CLIP-based text embeddings, Tina demonstrates the ability to generate models for a wide range of personalized tasks. The approach shows promising results in generalizing to both seen and unseen tasks, achieving state-of-the-art performance in several benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Tina's train-once-for-all approach effectively addresses the need for personalized models without requiring extensive retraining, making it a practical solution for diverse end-user scenarios.
2. The model achieves competitive performance across multiple datasets, demonstrating its robustness and effectiveness in both in-distribution and out-of-distribution tasks.
3. Tina can handle various types of input prompts (text, images) and generalize to unseen classes and tasks, highlighting its versatility and potential for broader applications.

Weaknesses:
1. Some methodological details are sparse, such as the specific configurations and hyperparameters used for training Tina. Providing more granular details could help readers replicate the experiments.
2. The reason for adopting DiT as the weight generation model is not well justified. It would be good to see some results of adopting different kinds of diffusion models.

Limitations:
While limitations are discussed, the manuscript could benefit from a discussion of the scalability of Tina to larger datasets and more complex tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nfC1OA6NeE;"REVIEW 
Summary:
This work derives SDEs for adaptive gradient methods and study the role of gradient noise. The analysis starts from theoretically driving the SDE for SignSGD and highlight its significant difference from SGD. The work further generalize the SDE analysis for AdamW and RMSpropW, two popular adaptive optimizers with decoupled weight decay and reveal key properties of weight decay. Finally, the work integrates the derived SDEs with Euler-Maruyama to confirm that the SDEs faithfully track their respective optimizers with various modern neural networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-The theoretical results are novel. To my knowledge, this is a first SDE analysis for SignSGD with quantitatively accurate descriptions.

-The theoretical analysis reports some novel properties in terms of gradient noise and convergence. These properties are interesting.

-The proofs seem complete and reasonable.

-A useful theory should be quantitatively verifiable. This work definitely make it. The experiments that SDEs fit the empirical results with various optimizers and models are informative and impressive.

Weaknesses:
-It seems that the reported theoretical results and insights cannot directly lead to some theory-inspired and improved methods. This raise a question on the significance of this work.

-While this work did literature review, some important references are still missing, such as [1] on analyzing Adam using SDEs. As weight decay plays a key role in the results, it may be helpful to review recent papers analyzing novel or overlooked properties of weight decay.


Reference:

[1] Xie, Z., Wang, X., Zhang, H., Sato, I., & Sugiyama, M. (2022, June). Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning (pp. 24430-24459). PMLR.

Limitations:
This work discussed the limitations in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors derive SDE for signSGD and Adam(W). The experiments show that the algorithm will converge toward the limit of the theorem indicates.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose ""accurate"" SDEs for algorithms Sign-SGD and Adam(W).

Weaknesses:
1. In Remark after Lemma 3.6, the authors claim that Sign-SGD is (almost) linear in $\sigma_{max}$. However, with $\Delta$ either in Phase 2 or Phase 3, there should be $\sigma_{max}^2$ in the final bound.

2. All the stationarity holds when Hessian is the same from $X_0$ to $X_t$ and convergence holds for strongly convex. However, the hessian changes a lot during network training.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper derives SDEs for SignSGD, RMSprop, and Adam.
The analysis offers insights into the convergence speed, stationary distribution, and robustness to heavy-tail noise of adaptive methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The derived SDE for SignSGD exhibits three different phases of the dynamics.

- The analysis reveals the difference between SignSGD and SGD in terms of the asymptotic expected loss, the robustness of noise variance, etc.

- The analysis of AdamW provides insights into the different roles of noise, curvature, and weight decay.

Weaknesses:
Refer to Questions and Limitations.

Limitations:
The SDE for AdamW is limited to quadratic functions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ndIum4ByTU;"REVIEW 
Summary:
This paper addresses the issue of the flow matching method's lack of dependence on the data population. It proposes incorporating the initial population density into the vector field through amortization—using a Graph Neural Network (GNN) to embed the populations and adding this embedding to the input of the vector field network. The paper argues that this dependence would better model the data due to sample interactions, demonstrating improved generalization on unseen initial distributions. The method's application is showcased in perturbation drug screening.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### Originality
The problem setting of adding population dependence to flow matching is novel. The model framework of adding input to the vector field network using a GNN as a population encoder is also novel.

### Clarity

The paper is clearly written, with rigorous mathematical notations. The related work and introductions are especially well-written.

### Quality

The writing is good, and the experiment involves many baselines.

### Significance

The proposed method excels at generalizing to unseen populations, which is a significant improvement over existing methods, particularly when the conditions for generation are complex. The application on drug screening addresses a significant scientific problem and holds promise for personalized healthcare.

Weaknesses:
1. The paper could explain more about the meta-learning aspect of this method.
2. It could include explanations and/or ablation studies on the role of meta-learning and the GNN, especially in the synthetic experiment.
3. More detail is needed on what properties of the Wasserstein manifold of probabilities are used in the model. It is unclear how the model proposed in section 3.2 depends on the properties of the Wasserstein manifold described in section 3.1.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed an extension of the Conditional Generative Modeling via Flow Matching (CGFM) framework. Taken inspiration from the theory of Wasserstein Gradient Flow, this new framework, Meta Flow Matching, proposed to learn the push-forward mapping of multiple measures in the same measures-space. This is motivated by the realistic problem of modeling single-cell perturbation data where we want to see the response of populations of cells of patients when receiving different treatments. A novelty of Meta Flow Matching is that by combining amortized optimization and CGFM, the trained MFM velocity network can model newly observed populations _without_ knowing their labels/conditions. Two empirical benchmarks were performed to showcase the effectiveness of MFM compared to the Flow Matching (FM) and CGFM.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method proposed is novel enough, and the problem is well-motivated. I also find the idea of integration of GNN to model the conditional variable quite neat. The method is based on the well-studied theory of Wasserstein gradient flow on measure spaces and amortized optimization framework.

- The empirical benchmark, especially on real biological data, seems to showcase the strength of MFM.

- Overall the paper is quite well written and easy to follow.

Weaknesses:
- The first part of the methodology section seems to be phrased as a new methodological contribution, but if I'm not mistaken this is just more or less restating the already established theory of W2 gradient flow and continuity equation (eq 14). I think the authors should put Section 3.1 into the background section (2nd Section).

- There is a lack of discussion on whether the 3 crucial assumptions (line 145-152) are satisfied in a realistic biological setting. For example, in theory, Assumption (iii) on the unique existence of the Cauchy problem stands when the velocity field satisfies some regularity conditions -- I'm not sure this can be extended to a parameterized neural network that takes input from another (graph) neural network as an embedding function, which is hardly Lipschitz smooth in most of the case.

- Algorithm boxes at the end of section 3 is highly welcome. Or if the authors cannot allocate the space, I highly recommend putting two (one for training and one for sampling) into the Appendix. It is quite hard to follow how the velocity is trained in reality. For example, what function $f_t(x_0, x_1)$ did the authors take for this work? Is it still linear interpolation as vanilla flow matching? Or does it involve adding some form of stochasticity as in stochastic interpolant or VP-SDE as in diffusion model? Is the coupling $(x_0, x_1)$ sampled to match randomly, or they are sampled to some form of alignment as in the multisample flow matching paper (Pooladian et al. 2023)?

- This might not be the original purpose of this work, but I would love to see how MFM perform on conditional image generation task. One can pick a simple small dataset such as CIFAR10 that already includes class labels, or better yet ImageNet dataset. The performance in this takse will be much more convincing than the synthetic experiment, where I would argue would target the same type of task.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discussed the novel problem setup of generative modeling of the dynamics of probability distributions. The paper proposed Meta Flow Matching (MFM), an extension of the flow matching framework for implicitly learning the vector fields on the Wasserstein manifold of probability distributions. The paper demonstrated better transferability of the proposed framework on unseen distributions on both synthetic datasets and real-world drug-screen datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem setup of learning a flow matching model for mappings between distributions (i.e., a probability path on the Wasserstein manifold), to the best of my knowledge, is novel and has not been explored in previous work.
- The idea of using distribution-specific embeddings (the population embeddings) is well explained and motivated in the paper.
- The proposed method demonstrates better transferability on both synthetic and real-world datasets compared to other baselines.

Weaknesses:
- The proposed method seems to be a special case of a conditionally trained flow matching model where the conditions are continuous learnable embeddings. Such an idea has already been applied in various diffusion or flow matching models including image generation (conditioned on text embedding in the latent space), protein co-design [1] (conditioned on sequence, generate protein structure, or vice versa), and peptide design [2] (conditioned on receptor proteins, generate peptides).

- The idea of population embedding in the paper is similar to task embedding, which has been well-explored in the meta learning (e.g. [3]). Although the authors claimed their framework to be *meta* flow matching, related work in meta learning seems to lack.


[1] Campbell, Andrew, et al. ""Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design."" arXiv preprint arXiv:2402.04997 (2024).

[2] Li, Jiahan, et al. ""Full-Atom Peptide Design based on Multi-modal Flow Matching."" arXiv preprint arXiv:2406.00735 (2024).

[3] Achille, Alessandro, et al. ""Task2vec: Task embedding for meta-learning."" Proceedings of the IEEE/CVF international conference on computer vision. 2019.

Limitations:
The authors have adequately and properly discussed the limitations and potential societal impact of the paper in the Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Meta Flow Matching (MFM), a flow matching framework modeling interacting samples evolving over time by integrating vector fields on the Wasserstein manifold. The authors leverage a Graph Neural Network to embed populations of samples and thus generalize the method over different initial distributions. The authors demonstrate the method on individual treatment responses predictions on a large multi-patient single-cell drug screen dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: The method uniquely considers population interactions, unlike previous flow matching methods that model samples individually.

Generalization: The authors extended conditioning on latent variables to conditioning on population index in section 3.2. The proposition in section 3.2 demonstrates that conditional flow matching can fit well within the MFM framework. The experiments show that MFM can generalize to unseen data, outperforming other methods in this regard.

Weaknesses:
In Table 1 of the synthetic experiment, the authors compared the performance of FM, CGFM and MFM of k=0,1,10,50. MFM doesn't seem to beat existing methods on the metrics and from the visualizations, it's hard to tell MFM is actually doing better than FM. Also, for the various values of k, some explanations on how performance correlates with values of k and why might be necessary for readers to understand this table.

In both experiments, the authors only compared FM, CGFM, and in Table 2 also ICNN. Probably more methods, like diffusion, should also be taken into comparison. Also, in experiment 2, only W1, W2 and MMD are computed as metrics. While these are useful when modeling distributions, more metrics, especially those specific to this application may be applied.

Limitations:
The authors have not addressed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nMFVdphOc9;"REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEvnCuviMO;"REVIEW 
Summary:
Metric distortion is a framework to evaluate the ""accuracy"" of social choice rules, by considering a worst-case candidate and voter embedding in a metric space, and by assuming that reported votes are derived from distances in the metric space. So far, votes were assumed to be a deterministic function of the distances. The paper investigates the case where they are probabilistic functions of the distances, in the asymptotic limit of a large number of voters. The key finding is that this inverts the evaluation of some voting rules, in particular Copeland and Random Dictatorship, for highly noisy voting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The model introduced by the authors is an insightful generalization of previous work which, remarkably, provides a markedly different view on social choice rules. Given the growing importance of social choice in machine learning, as well as accounting for noisy inputs while considering embedded vector spaces, I believe that this work scores high in significance.

Additionally, the analysis is quite thorough, with matching lower/upper asymptotic bounds for Plurality upper bound for Copeland and lower/upper bounds for Random Dictatorship.

The paper is also fairly well written.

Weaknesses:
My main concern is Lemma 3. The proof seems to argue that the constraints $\forall i, | b_i - w_i | \leq b_i + w_i$ (i.e. inequality constraint on $(i, W, B)$), but the optimization problem (7) has a constraint $\max_i | w_i - b_i | \leq \min_i b_i + w_i$. It is not clear to me why these constraints would be equivalent. Note that the latter implies the former set of constraints. Thus if $\mathcal E_{\alpha}'$ was defined with all voter-wise constraints, then it would be a minimum over a smaller set, and thus $opt(\mathcal E_\alpha') \geq opt(\mathcal E_\alpha)$. Since Lemma 3's proof actually says $\frac{SC(W, d)}{SC(B, d)} \leq 1 / opt(\mathcal E'_\alpha)$, using this inequality seems to imply the actual Lemma 3. Am I reading this correctly?

It is disappointing that the upper-bound for Copeland. It would be helpful if the authors can point out where the argument gets loose.

Limitations:
The paper stated its results very clearly and factually. I have no concerns about unaddressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends the framework of metric distortion, measuring how well voting rules minimize the social cost in a given metric space, to probabilistic voting scenarios where the preferences of voters are drawn from a probability distribution defined by the relative distances between candidates and each voter's ground truth position in the metric space.

The authors base their analysis on three different axioms that the induced marginal probabilities of relative preferences must verify, namely *scale-freeness*, *independence of other candidates* and *strict monotonicity.* They define a general class of marginal probabilities that verify the three axioms, and show that it encompasses the widely used *Plackett-Luce* model.

They then provide upper and lower bounds for the distortion of the *Plurality* rule, both linear in the number of candidates and matching asymptotically when the number of voters grows to infinity.

They then provide a upper-bound for the distortion of *Copeland* rule and show that it is independent of the number of candidates in the limit of a large number of voters.

Moreover, they give upper and (non-matching) lower-bounds for the distortion of *Random Dictator.*

They finally compare their results under both the *Plackett-Luce* and *Pairwise Quantal Voting* models (the latter being inspired form Quantal Response Theory), and show that the classical bounds of the metric distortion literature are recovered in the limit of vanishing randomness (although not for Copeland rule, hinting at a loose analysis).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper, in extending the metric distortion analysis to probabilistic voting models perhaps closer to reality, is rather original and proposes a more optimistic view of metric distortion where randomized dictator is beatable in a worst case distortion sense.
The paper is fairly well written and the proofs seem correct.

It is an interesting idea, with an interesting result.

Weaknesses:
The main problem of this paper is that it is not a good match to NeurIPS. This paper would work very well at an algorithms conference like SODA or ICALP, or a CS econ conference like EC (okay, probably one tier down like WINE), or possibly even at the those AI conferences that have a history in social choice theory like AAAI or IJCAI. And I also understand that NeurIPS has accepted such papers in the past. However, is it really a good fit for NeurIPS 2024? 

- There is no mention of the proof of Lemma 1 being in Appendix A.
- In the proof of Theorem 2, $\zeta$ is hardly introduced (also not in the Appendix).
- Formatting may be improved in place: e.g. Equations 6, 7, 10, or Theorem 3.
- l.312 ""converges to 9 instead of 5"". This part is not very clear, reminding the general bound in the deterministic case would improve readability.
- Typos:
	- l.220 ""and is by solving"".
	- Equation 24 showcases $(d)$ instead of $(a)$.
	- Equation 34: $\geq$ should be $=$.
	- l.618 ""LEt"".
	- l.641 should probably be deleted (equivalent to l.642).
	- l.660 weird grammar.
	- Footnote 6: missing index $\gamma_j$.
	- l.670: missing $(\hat{g}_{MID} +\hat{g}_{OUT}  )^2$ in inequalities $(a)$ and $(b).$ Furthermore, Equation 63 is used in $(a)$ rather than in $(b)$.

Limitations:
- The existence of distributions on rankings that generate pairwise order marginals of the form described in the paper is assumed and left for future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of metric distortion in single-winner elections. The key assumption is that the voters' preferences are not exactly compatible with the metric space, but they rather agree with it with a certain probability. The authors propose several axioms that formalize the requirements for the probability distribution for it to make sense in the context of distortion. Then they provide upper bounds of distortion in the probabilitic setting for Plurality, Random Dictator and Copeland (in case of the first two rules, they provide also lower bounds).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work is the first one to combine probabilistic voting with metric distortion, hence the novelty is clear. The paper is overall of good quality, the axioms proposed in their work make sense to me, and the results are sound. The research direction introduced in this paper can be continued in further follow-up papers.

Weaknesses:
The paper could have been more clearly written --- for example, the formal notation should be introduced at the beginning of Section 2   (together the model) rather than in the middle of the introduction.

Besides, I think that Axiom 2 (Independence of Other Candidates) could have been better motivated. I can imagine that it was crucial to obtain the authors' results, but it seems rather natural to me that  in the real-life scenarios that motivated the research, the presence of additional candidates can impact the voter's probability of ranking one candidate over another.

Another weakness is that the authors only consider three rules, and the analysis of only two of them is complete --- many important rules (like Borda or PluralityVeto) are not considered at all. This could raise a question whether this amount of technical contribution is enough for a top conference like NeurIPS.

Limitations:
The authors adequately addressed the limitations and there are no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers metric distortion in probabilistic models of voting. In the metric distortion framework the voters and alternatives are embedded in a metric space, and given the ranked preferences the goal is to find an alternative with low distortion. In this setting these rankings come from a probabilistic model.

In the first part of the paper, there axioms are introduced and authors show which axiom is satisfied by which probabilisitic model. In the second half of the paper the goal is to find the distortion of Plurality and Copeland rules for a specific class of probabilistic models. The results show matching upper and lower bound of $m$ for plurality and constant upper bound for Copeland.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
Defining a model for distortion in stochastic models is a useful idea in future analysis of voting systems.
The idea of the paper is novel and it uses novel techniques in the second half.
I like the definition of the three axioms. I find them natural and easy to understand.

Weaknesses:
My main concern is about the presentation. The preliminaries section is incomplete. The definition of distortion is hard to understand for a general audience and you made it harder by just putting the formula there. You have to add a description in words and give some intuition on why this definition makes sense. 

It's not clear how the probabilistic model works and how you define distortion on it until section 3 where you define it for a specific class. You have to formally define your probabilistic approach in Section 1.1 and also define distortion in this model. Not knowing the exact definition makes following the first paragraph of section 2 really hard. Before reading the rest of the paper I didn't understand why $P$ is a function of $d$ or why the preferences may not be consistent with the distances.

I think you have to add more intuition on the probabilisitic models. For instance you mention ground-truth in the definition of Mallows model but you have to explain in more details that how this model distributes the probabilities based on the distance to this ground-truth. The same explanations are needed for PQV.

My understanding is that the analysis that you provide works for any member of $G$, but currently the only members for which we have the final bound are PL and PQV. Is that true? If so is there any other interesting member of this class?

Limitations:
-

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nEnazjpwOx;"REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
mdK1vhgpa5;"REVIEW 
Summary:
The paper proposes a method to continually adapt a pre-trained classifier to an unlabeled stream of test data. They address the problem of continual test-time adaptation through the lens of Bayesian deep learning. Their method consists of three main components: (1) a variational warm-up strategy to turn any source model into a Bayesian Neural Network, (2) a mixing strategy between the source model and the last posterior to leverage the trade-off between adaptation and forgetting, and (3) a modified entropy term that is symmetric and incorporates data augmentations. The authors compare their method on standard CIFAR-C and Imagenet-C datasets to a set of TTA baselines. The paper also includes ablation studies on the components and an evaluation of uncertainty estimates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper tackles one of the most relevant problems in continual domain adaptation, namely the trade-off between agile adaptation while preventing the forgetting of the source model. It does so by constituting a mixture of the source and last adapted model in a VI framework, which is novel to my knowledge. (originality)
- Further, in a setting where robustness is crucial and therefore uncertainty quantification can be helpful, the combination of Bayesian deep learning and continual test-time adaptation is interesting and insightful. (originality)
- The methodological backbone is accompanied by insightful ablation studies that highlight the significance of the different parts of the paper’s contribution. (quality)
- The paper is, in most parts, pleasant to read. The notation is clear and consistent, and the reader is well guided through the different sections. (clarity)
- The paper presents clear experimental evidence in support of the method. The experiments show an improvement in adaptation accuracy on already quite saturated datasets (up to 1.8% points on CIFAR-10-C). VCoTTA also seems to be advantageous on most corruption types. (significance)

Weaknesses:
The paper presents strong evidence in support of the proposed method. However, it is left unclear to me why the method performs so much better than previous approaches.

- The method consists of a range of specific components. However, in some cases, the specific design of the components is not clearly motivated. In particular, equations 10 and 13 lack supporting citations or explanations. Why have exactly these formulations been chosen?
- I’d like to get more clarity on the difference between this paper and the original CoTTA work, as it seems to me there are certain components in common (e.g., student-teacher approach, EMA). More precisely, could you please highlight the difference in the update equations between the two papers? My understanding is that adding the VI framework notably changes (i) the optimization objective by adding the KL term (instead of solely minimizing entropy) and (ii) the predictions by marginalizing out the model parameters. Where else does the VI framework contribute to differences?

Limitations:
The authors have listed limitations including computational efficiency and the need for access to the source data at adaptation time

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces VCoTTA, a novel variational Bayesian approach to address the Continual Test-Time Adaptation (CTTA) task, which focuses on effective domain adaptation during continuous domain shifts at test time. The authors' main contributions include a method to measure uncertainties in CTTA, addressing the issue of error accumulation due to the use of unlabeled samples. They propose transforming a pretrained deterministic model into a Bayesian Neural Network (BNN) using variational warm-up at the source stage, and employ a mean-teacher update strategy during test time. The approach updates the student model by combining priors from both source and teacher models, with the evidence lower bound formulated as the cross-entropy between student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper demonstrates originality through the novel Variational Continual Test-Time Adaptation (VCoTTA) approach, which creatively utilizes Bayesian Inference for Continual Test-Time Adaptation, and employs strategies like the variational warm-up and prior mixture techniques. The quality of the work is evident in its solid theoretical foundation, comprehensive methodology, and empirical validation on multiple datasets. 

The paper's clarity is apparent in its well-structured presentation, use of visual aids, and explicit statement of contributions. The significance of the research is underscored by its practical relevance to risk-sensitive applications, potential for broad applicability, and the reported improvements in predictive accuracy and uncertainty estimation under distribution shifts. By addressing critical challenges in CTTA, such as error accumulation and uncertainty estimation, and bridging Bayesian methods with test-time adaptation, the paper not only advances the current state of the art but also opens up promising avenues for future research. Overall, this work represents a valuable contribution to the field, offering both theoretical insights and practical advancements in continual learning and test-time adaptation.

Weaknesses:
Regarding the computational overhead discussed in the paper, while it is noted that online Variational Inference is employed to make the approach computationally feasible, a detailed analysis of the computational costs associated with VCoTTA is absent. Table 13 presents a comparison of time and memory costs, but the source of these values is unclear. Could you specify which dataset was used for these measurements? Also, is it possible to clarify whether the time and memory comparisons pertain to training or testing phases?

The manuscript contains several typographical and grammatical errors that need to be addressed. Specifically, brackets are missing in Equation (5) and in the sentence following Equation (13). Could these omissions be corrected to prevent misinterpretation of the mathematical expressions and enhance the clarity of the paper?

There are multiple grammatical issues that require rectification. The sentence ""MT is initially proposed in semi-supervised and unsupervised learning"" is somewhat unclear. Could this be rephrased for better coherence? Additionally, the sentence ""We use the mean entropy derived from a given *serious* data augmentation to represent the confidence of the two prior models, and mix up the two priors with a modulating factor"" appears to contain a typographical error and could be better structured. Could these issues be addressed to improve the readability and accuracy of the text?

The heading for Section 5.7 seems to not accurately reflect the content discussed within. Could this heading be revised to more accurately convey the main topics or findings of the section, thereby ensuring clarity and relevance for the reader?

The explanation of how MT operates in semi-supervised and unsupervised learning settings appears incomplete and potentially misleading. The current statement, ""where the teacher model guides the unlabeled data, helping the model generalize and improve performance with the utilization of large-scale unlabeled data,"" lacks specificity. Could you specify which model (teacher or student) benefits from this guidance and in what manner? Additionally, the phrase ""where the teacher model guides the unlabeled data"" seems incorrect. Could this be clarified or corrected to accurately reflect the operational dynamics of the MT framework?

Limitations:
The authors have adequately addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a variational continual adaptation method. Where a sequence of test-time domain adaptation problems are shown to a model. More specifically a labeled dataset is given as an initial dataset to learn from, and then afterwards a sequence of unlabelled datasets with domain shifts are presented to the model.

Using traditional variational continual learning methods will result in error accumulations in the posterior over parameters. So the authors propose a scheme that regularizes against the posterior learned from an initial source dataset. 

The authors propose do away with using sequential Bayesian inference. Instead, the authors use a mix of a source prior (learned using labeled data) and a teacher prior. The teacher prior is an EMA of the previous task’s posterior learned with variational inference. The method the authors propose is named VCoTTA.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper is well written and the components of VCoTTA are clearly explained. 
* The authors provide an ablation to demonstrate which design choices worked well, for instance, to demonstrate that the mixture of priors worked well for test-time adaptation.

Weaknesses:
Novelty:
* In terms of novelty I’m not convinced that there are important applications of continual test-time adaptation in the form of classification tasks derived from CIFAR10 datasets. I could be wrong, but some justification in the paper is required and some more realistic benchmarks would be nice.
* Maybe I have misunderstood the variational warm-up procedure. But using the MLE estimates to initialize the BNN mean parameters was done in VCL https://arxiv.org/abs/1710.10628 . So this is not a novel idea. Furthermore, there are better ways to initialize a BNN such as using Bayesian linear regression: https://proceedings.mlr.press/v97/rossi19a/rossi19a.pdf.

Clarity
* Why does the teacher model use EMA updates instead of using the inference variational posterior?
* Why is data augmentation an important component in VCoTTA (Sec 4.2)? This is suddenly presented in the paper without justification.

Notation:
* In Section 4.2, the title is a “Mixture-of-Gaussian prior”, but Eq 11 is an addition of two priors which are Gaussians by design, so this is a “scale-mixture prior” https://arxiv.org/pdf/1505.05424, not a mixture of Gaussians (https://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/notes/w9b_mixture_models.pdf)?
* Confusing notation of the source prior: it is denoted as $p_0$ (Fig 2) and $p_1$ (Eq 11), this needs to be consistent.


Empirical weaknesses:
* No standard errors in the experimental results. So difficult to see which method outperforms another.
* Uncertainty estimation is not performed with standard methods like ECE or OOD detection like https://arxiv.org/abs/2102.06571. It is unclear to me whether the Brier Score estimates uncertainties.

Limitations:
There is a good discussion on the limitations of VCoTTA.

One limitation that is not discussed is in the effectiveness of (variational) Bayesian sequential inference methods. Weight space variational inference has been shown to be very difficult to do in practice, https://arxiv.org/abs/2301.01828. So weight space variational inference (without tricks like multi-head networks and coresets) might not be the best choice when wanting to remember a source distribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a variational Bayesian approach to handle uncertainties in continual test-time adaptation (CTTA). The source pretrained model is made Bayesian by variational warm and a mean-teacher update strategy is used at test time. To avoid drift due to uncertainty of priors using only unlabeled data at test time, the paper proposed to update the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method’s effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novelty: Bayesian approach in Continual Learning is a principled and elegant approach to the problem which this paper is relying on. In CTTA, there are additional issues due to the uncertainty of the prior distributions using only the unlabeled data from unknown domains. This paper presents a novel solution by using adaptive mixture prior models and student-teacher update on top of an existing framework.

- Relevance: CTTA is a topic that can interest a general audience, and the  Bayesian and variational framekwork can also be of interest to many.

Weaknesses:
- While Bayesian approach is nice in principle, it can be computationally demanding and offer little benefit in practice. Most of the existing CTTA methods are computationally and memory efficient, whereas this method present an opposite end of the spectrum. While the reported results are impressive, it is unclear to me why the proposed method is superior to other SOTA methods.
- The hyperparameter selection process is not addressed in the paper, which is critical in TTA where all hyperparameters should be predetermined before data access. How are they chosen?

Limitations:
Limitations are mentioned in the paper, albeit very brief. Overall, the proposed method is more complex and demanding (such as requiring a pretrained probabilistic model or source data) for TTA applications. Perhaps the proposed approach may work even better with UDA or other CL scenarios with (partially available) target labels?

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
